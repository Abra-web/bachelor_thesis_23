{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from spacypdfreader import pdf_reader\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from googletrans import Translator # don't forget to run \"!pip install googletrans==3.1.0a0\" before using this"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T17:43:32.946503Z",
     "end_time": "2023-05-04T17:43:37.354874Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# functions\n",
    "def find_folder_with_type(given_path, doc_type): # returns all documents found in path\n",
    "    doc_list = []\n",
    "    for doc in os.listdir(given_path):\n",
    "        if re.search (r'.*\\%s$' % doc_type, doc) is not None: # even though this shows as error in IDE it's fine\n",
    "            doc_list.append(doc)\n",
    "    return doc_list\n",
    "\n",
    "def folder_to_nlp_doc(given_path, list_of_doc_names, nlp):\n",
    "    temp_list = []\n",
    "    # the input should be a list of file contained in a folder\n",
    "    for file_name in list_of_doc_names:\n",
    "        file_path = given_path + \"\\\\\" + file_name\n",
    "        temp_list.append(pdf_reader(file_path, nlp))\n",
    "    return temp_list\n",
    "\n",
    "def get_entities(the_file):\n",
    "  \"\"\"\n",
    "  Here we extract the elements in an unsupervised manner, i.e., we will use the grammar of the sentences. The extraction of a single word entity from a sentence is not a tough task. We can easily do this with the help of parts of speech (POS) tags. However, when an entity spans across multiple words, then POS tags alone are not sufficient. To fix this we basically save our previous text's info.\n",
    "  \"\"\"\n",
    "\n",
    "  # init\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag (the relationship between any two words is marked by a dependency tag) of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  # for holding the text that is associated with the current subject/object (can be multiple words)\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  # going through each token\n",
    "  for tok in nlp(the_file):\n",
    "    if tok.dep_ != \"punct\": # if punctuation mark skip\n",
    "\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        if prv_tok_dep == \"compound\": # if the previous word was also a 'compound' then add to current text\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "      # check if modifier (a modifier gives information about another word in the same sentence e.g., blue house)\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        if prv_tok_dep == \"compound\": # if previous word was also a 'compound' then add to current text\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "      # extract first entity - subject\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        # reset info\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"\n",
    "\n",
    "      # extract second entity - object\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        # reset info\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"\n",
    "\n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]\n",
    "\n",
    "def get_relation(the_sentence):\n",
    "  temp_doc = nlp(the_sentence)\n",
    "\n",
    "  # creating the rule-based Matcher object\n",
    "  matcher = Matcher(nlp.vocab)\n",
    "\n",
    "  # defining the pattern - Each pattern should be a list of dicts and each pattern should be saved in another list\n",
    "  # ex: patterns = [[{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}], [{\"ORTH\": \"Google\"}, {\"ORTH\": \"Maps\"}]]\n",
    "\n",
    "  # This pattern tries to find the ROOT word in the sentence. Once the ROOT is identified, then the pattern checks whether it is followed by a preposition (‘prep’) or an agent word. If yes, then it is added to the ROOT word.\n",
    "  pattern = [{'DEP':'ROOT'}, # check for token with dependency label root\n",
    "            {'DEP':'prep','OP':\"?\"}, # other stuff\n",
    "            {'DEP':'agent','OP':\"?\"},\n",
    "            {'POS':'ADJ','OP':\"?\"}]\n",
    "\n",
    "  # matcher.add(\"match_id\", \"patterns\")\n",
    "  matcher.add(\"matching_1\", [pattern])\n",
    "\n",
    "  matches = matcher(temp_doc)\n",
    "\n",
    "  k = len(matches) - 1\n",
    "  if k == -1: # meaning no match was found so return null\n",
    "    return None\n",
    "\n",
    "  span = temp_doc[matches[k][1]:matches[k][2]]\n",
    "  return span.text\n",
    "\n",
    "def create_kg_csv_pandas(subjects, predicates, objects): # so source (subject) ----relation (predicate)----> target (object)\n",
    "    # field names\n",
    "    fields = ['Subject', 'Predicate', 'Object']\n",
    "    filename = os.getcwd() + \"\\\\head_relation_tail_data\\\\simple\\\\kg_of_simple_en.csv\"\n",
    "    # data rows of csv file\n",
    "    rows = [[subjects[i], predicates[i], objects[i]] for i in range(len(subjects))] # best to check if we have empty values\n",
    "\n",
    "    temp_df = pd.DataFrame(rows, columns = fields)\n",
    "    temp_df.to_csv(filename, header = fields)\n",
    "\n",
    "def create_kg_csv(subjects, predicates, objects): # so source(subject) -relation(predicate)-> target(object)\n",
    "    # field names\n",
    "    fields = ['Subject', 'Predicate', 'Object']\n",
    "    filename = os.getcwd() + \"\\\\head_relation_tail_data\\\\simple\\\\kg_of_simple_en.csv\"\n",
    "\n",
    "    rows = [[subjects[i], predicates[i], objects[i]] for i in range(len(subjects))]\n",
    "\n",
    "    # find out empty and None strings, replacing it with \"-\"\n",
    "    for i in range(len(rows)):\n",
    "        for j in range(len(rows[0])): # so len 3\n",
    "            if rows[i][j] == \"\" or rows[i][j] is None: rows[i][j] = \"-\"\n",
    "\n",
    "    # writing to csv file\n",
    "    with open(filename, 'w', newline = '') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(fields) # first writing fields\n",
    "        csv_writer.writerows(rows) # now the remaining record\n",
    "\n",
    "# German: de, French: fr, but can work for any other languages too\n",
    "def simple_translator(language): # assumes that en kg csv is already created\n",
    "    translator = Translator()\n",
    "\n",
    "    filename = os.getcwd() + \"\\\\head_relation_tail_data\\\\simple\"\n",
    "    filename_english = filename + \"\\\\kg_of_simple_en.csv\"\n",
    "    filename_assigned_language = filename + \"\\\\kg_of_simple_\" + language + \".csv\"\n",
    "\n",
    "    with open(filename_english, mode =\"r\") as file_original, open(filename_assigned_language, mode = \"w\", newline = '') as file_2:\n",
    "      csv_reader = csv.reader(file_original)\n",
    "      csv_writer = csv.writer(file_2)\n",
    "\n",
    "      for lines in csv_reader: # each line is a list of 3 elements (source - relation - target)\n",
    "        source_en, relation_en, target_en = lines[0], lines[1], lines[2]\n",
    "\n",
    "        # src(source) = english, dest(destination) = language to translate to\n",
    "        translated_source, translated_relation, translated_target = translator.translate(source_en, src = \"en\", dest = language), translator.translate(relation_en, src = \"en\", dest = language), translator.translate(target_en, src = \"en\", dest = language)\n",
    "\n",
    "        temp_row = [translated_source.text, translated_relation.text, translated_target.text]\n",
    "        csv_writer.writerow(temp_row)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T17:43:37.395604Z",
     "end_time": "2023-05-04T17:43:37.417088Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PIPELINE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Loading Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "### Loading Pipeline ###\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # A small English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, syntax and entities."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T17:43:37.420345Z",
     "end_time": "2023-05-04T17:43:38.092629Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Importing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "### Importing Dataset ###\n",
    "\n",
    "# automatically extracting path\n",
    "current_path = os.getcwd()\n",
    "data_path = current_path.replace(\"src\\\\main\", \"data\\\\en\\\\directives_pdf\")\n",
    "\n",
    "doc_list = find_folder_with_type(data_path, '.pdf') # detection of pdf files in the folder\n",
    "\n",
    "# importing the file, here doc is like a \"list\" of tokens (each tok is either a word, number, ...)\n",
    "file_list = folder_to_nlp_doc(data_path, doc_list, nlp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T17:43:38.096613Z",
     "end_time": "2023-05-04T17:43:59.472543Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Entities (Nodes) and Relations (Edges) Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 1\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:02<00:00, 66.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:02<00:00, 70.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 2\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [00:02<00:00, 70.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [00:02<00:00, 64.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 3\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [00:05<00:00, 71.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [00:05<00:00, 68.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 4\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:01<00:00, 73.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:02<00:00, 64.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 5\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:02<00:00, 73.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:02<00:00, 75.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 6\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:03<00:00, 67.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:03<00:00, 68.80it/s]\n"
     ]
    }
   ],
   "source": [
    "list_of_entity_pairs = [] # this is a list of lists (so each index contains the info of a data file)\n",
    "list_of_relations = []\n",
    "count = 1\n",
    "\n",
    "for file in file_list:\n",
    "    print(\"Document number \" + str(count))\n",
    "    count += 1\n",
    "\n",
    "    sents = list(file.sents) # extract sentences of the document (only checks for \".\")\n",
    "\n",
    "    print(\"Entities (Nodes - subject/object) Extraction\")\n",
    "    entity_pairs = []\n",
    "    for i in tqdm(sents): # here \"tqdm\" is just used for creating a progress bar\n",
    "      entity_pairs.append(get_entities(str(i))) # Extracting the entity pairs of each sentence.\n",
    "\n",
    "    print(\"Relations (Edges - predicate) Extraction\")\n",
    "    relations = []\n",
    "    relations = [get_relation(str(i)) for i in tqdm(sents)] # Here we assume predicate is the main verb in a sentence.\n",
    "\n",
    "    # pd.Series(relations).value_counts()[:10] # to visualize\n",
    "    list_of_entity_pairs.append(entity_pairs)\n",
    "    list_of_relations.append(relations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T17:43:59.474565Z",
     "end_time": "2023-05-04T17:44:37.180639Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Build Knowledge Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [i[0] for i in entity_pairs]\n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in entity_pairs]\n",
    "\n",
    "# kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})\n",
    "create_kg_csv(source, relations, target)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T17:44:37.184771Z",
     "end_time": "2023-05-04T17:44:37.224346Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. Translate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "simple_translator(\"de\") # CAUTION: since this uses API it can \"timeout\", so if happens just run it again"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T17:44:37.199534Z",
     "end_time": "2023-05-04T17:45:40.833129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "simple_translator(\"fr\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T17:45:40.830776Z",
     "end_time": "2023-05-04T17:46:40.122596Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRASH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kg_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"We will use the networkx library to create a network from this dataframe. Which is going to be a directed graph allowing us to draw a line from subject to object. However, we also need to use pyvis library because networkx's visualize method is currently not working.\"\"\"\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# create a directed-graph from a dataframe\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m directed_graph \u001B[38;5;241m=\u001B[39m nx\u001B[38;5;241m.\u001B[39mfrom_pandas_edgelist(\u001B[43mkg_df\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m, edge_attr\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, create_using\u001B[38;5;241m=\u001B[39mnx\u001B[38;5;241m.\u001B[39mMultiDiGraph())\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyvis\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnetwork\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Network\n\u001B[0;32m      8\u001B[0m net \u001B[38;5;241m=\u001B[39m Network(notebook\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, cdn_resources\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mremote\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'kg_df' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"We will use the networkx library to create a network from this dataframe. Which is going to be a directed graph allowing us to draw a line from subject to object. However, we also need to use pyvis library because networkx's visualize method is currently not working.\"\"\"\n",
    "\n",
    "# create a directed-graph from a dataframe\n",
    "# directed_graph = nx.from_pandas_edgelist(kg_df, \"source\", \"target\", edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "#\n",
    "# from pyvis.network import Network\n",
    "#\n",
    "# net = Network(notebook=True, cdn_resources='remote')\n",
    "#\n",
    "# net.from_nx(directed_graph)\n",
    "# net.show(\"example.html\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
