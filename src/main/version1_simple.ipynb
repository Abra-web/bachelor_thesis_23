{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from googletrans import Translator # don't forget to run \"!pip install googletrans==3.1.0a0\" before using this\n",
    "import spacy_transformers # might seem not being used but it is required to run the transformers\n",
    "from unidecode import unidecode\n",
    "from PyPDF2 import PdfReader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T13:36:47.185955200Z",
     "start_time": "2023-06-04T13:36:46.894839100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "# functions\n",
    "def find_folder_with_type(given_path, doc_type): # returns all documents found in path\n",
    "    doc_list = []\n",
    "    for doc in os.listdir(given_path):\n",
    "        if re.search (r'.*\\%s$' % doc_type, doc) is not None: # even though this shows as error in IDE it's fine\n",
    "            doc_list.append(doc)\n",
    "    return doc_list\n",
    "\n",
    "def folder_to_nlp_doc(given_path, list_of_doc_names, given_nlp):\n",
    "    temp_list = []\n",
    "    # the input should be a list of file contained in a folder\n",
    "    for file_name in list_of_doc_names:\n",
    "        file_path = given_path + \"\\\\\" + file_name\n",
    "        reader = PdfReader(file_path)\n",
    "\n",
    "        all_text = \"\"\n",
    "        for i in range(len(reader.pages)):\n",
    "            text = reader.pages[i].extract_text()\n",
    "            text = text.replace(\"\\n\", \"\")\n",
    "            all_text = all_text + text\n",
    "\n",
    "        temp_list.append(given_nlp(all_text))\n",
    "    return temp_list\n",
    "\n",
    "def get_entities_for_english(the_file):\n",
    "    \"\"\"\n",
    "    Here we extract the elements in an unsupervised manner, i.e., we will use the grammar of the sentences. The extraction of a single word entity from a sentence is not a tough task. We can easily do this with the help of parts of speech (POS) tags. However, when an entity spans across multiple words, then POS tags alone are not sufficient. To fix this we basically save our previous text's info.\n",
    "\n",
    "    requirements:\n",
    "    - punctuation: [\"punct\"]\n",
    "    - compound: [\"compound\", \"acomp\", \"ccomp\", \"pcomp\"]\n",
    "    - modifier: [\"acl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"meta\", \"neg\", \"nmod\", \"npadvmod\", \"nummod\", \"poss\", \"prep\", \"quantmod\", \"relcl\"]\n",
    "    - subject: [\"csubj\", \"csubjpass\", \"nsubj\", \"nsubjpass\"]\n",
    "    - object: [\"dobj\", \"oprd\", \"pobj\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # params\n",
    "    punctuation_parsers = [\"punct\"]\n",
    "    compound_parsers = [\"compound\", \"acomp\", \"ccomp\", \"pcomp\"]\n",
    "    modifier_parsers = [\"acl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"meta\", \"neg\", \"nmod\", \"npadvmod\", \"nummod\", \"poss\", \"prep\", \"quantmod\", \"relcl\"]\n",
    "    subject_parsers = [\"csubj\", \"csubjpass\", \"nsubj\", \"nsubjpass\"]\n",
    "    object_parsers = [\"dobj\", \"oprd\", \"pobj\"]\n",
    "\n",
    "    # init\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag (the relationship between any two words is marked by a dependency tag) of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    # for holding the text that is associated with the current subject/object (can be multiple words)\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    # going through each token\n",
    "    for tok in nlp(the_file):\n",
    "        if tok.dep_ not in punctuation_parsers: # if punctuation mark skip\n",
    "\n",
    "            if tok.dep_ in compound_parsers:\n",
    "                prefix = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if the previous word was also a 'compound' then add to current text\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # check if modifier (a modifier gives information about another word in the same sentence e.g., blue house)\n",
    "            if tok.dep_ in modifier_parsers:\n",
    "                modifier = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if previous word was also a 'compound' then add to current text\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # extract first entity - subject\n",
    "            if tok.dep_ in subject_parsers:\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # extract second entity - object\n",
    "            if tok.dep_ in object_parsers:\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "\n",
    "    return [ent1.strip().lower(), ent2.strip().lower()]\n",
    "\n",
    "def get_entities_for_french(the_file):\n",
    "    \"\"\"\n",
    "    requirements:\n",
    "    - punctuation = [\"punct\"]\n",
    "    - compound: [\"ccomp\", \"expl:comp\", \"xcomp\", \"fixed\", \"flat:foreign\", \"flat:name\"]\n",
    "    - modifier = [\"acl\", \"acl:relcl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nmod\", \"nummod\", \"obl:mod\"]\n",
    "    - subject = [\"expl:subj\", \"nsubj\", \"nsubj:pass\"]\n",
    "    - object = [\"iobj\", \"obj\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # params\n",
    "    punctuation_parsers = [\"punct\"]\n",
    "    compound_parsers = [\"ccomp\", \"expl:comp\", \"xcomp\", \"fixed\", \"flat:foreign\", \"flat:name\"]\n",
    "    modifier_parsers = [\"acl\", \"acl:relcl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nmod\", \"nummod\", \"obl:mod\"]\n",
    "    subject_parsers = [\"expl:subj\", \"nsubj\", \"nsubj:pass\"]\n",
    "    object_parsers = [\"iobj\", \"obj\"]\n",
    "\n",
    "    # init\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag (the relationship between any two words is marked by a dependency tag) of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    # for holding the text that is associated with the current subject/object (can be multiple words)\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    # going through each token\n",
    "    for tok in nlp(the_file):\n",
    "        if tok.dep_ not in punctuation_parsers: # if punctuation mark skip\n",
    "\n",
    "            if tok.dep_ in compound_parsers:\n",
    "                prefix = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if the previous word was also a 'compound' then add to current text\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # check if modifier (a modifier gives information about another word in the same sentence e.g., blue house)\n",
    "            if tok.dep_ in modifier_parsers:\n",
    "                modifier = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if previous word was also a 'compound' then add to current text\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # extract first entity - subject\n",
    "            if tok.dep_ in subject_parsers:\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # extract second entity - object\n",
    "            if tok.dep_ in object_parsers:\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "\n",
    "    return [ent1.strip().lower(), ent2.strip().lower()]\n",
    "\n",
    "def get_entities_for_german(the_file):\n",
    "    \"\"\"\n",
    "    requirements:\n",
    "    - punctuation = [\"punct\"]\n",
    "    - compound = [\"adc\", \"avc\", \"nmc\", \"pnc\", \"uc\", \"svp\", \"re\", \"pm\", \"par\", \"dep\", \"cvc\", \"app\", \"ag\", \"ac\"]\n",
    "    - modifier = [\"ams\", \"mnr\", \"mo\"]\n",
    "    - subject = [\"sb\", \"sbp\"]\n",
    "    - object = [\"oa\", \"oc\", \"og\", \"op\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # params\n",
    "    punctuation_parsers = [\"punct\"]\n",
    "    compound_parsers = [\"adc\", \"avc\", \"nmc\", \"pnc\", \"uc\", \"svp\", \"re\", \"pm\", \"par\", \"dep\", \"cvc\", \"app\", \"ag\", \"ac\"]\n",
    "    modifier_parsers = [\"ams\", \"mnr\", \"mo\"]\n",
    "    subject_parsers = [\"sb\", \"sbp\"]\n",
    "    object_parsers = [\"oa\", \"oc\", \"og\", \"op\"]\n",
    "\n",
    "    # init\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag (the relationship between any two words is marked by a dependency tag) of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    # for holding the text that is associated with the current subject/object (can be multiple words)\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    # going through each token\n",
    "    for tok in nlp(the_file):\n",
    "        if tok.dep_ not in punctuation_parsers: # if punctuation mark skip\n",
    "\n",
    "            if tok.dep_ in compound_parsers:\n",
    "                prefix = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if the previous word was also a 'compound' then add to current text\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # check if modifier (a modifier gives information about another word in the same sentence e.g., blue house)\n",
    "            if tok.dep_ in modifier_parsers:\n",
    "                modifier = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if previous word was also a 'compound' then add to current text\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # extract first entity - subject\n",
    "            if tok.dep_ in subject_parsers:\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # extract second entity - object\n",
    "            if tok.dep_ in object_parsers:\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "\n",
    "    return [ent1.strip().lower(), ent2.strip().lower()]\n",
    "\n",
    "def get_relation(the_sentence):\n",
    "    temp_doc = nlp(the_sentence)\n",
    "\n",
    "    # creating the rule-based Matcher object\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # defining the pattern - Each pattern should be a list of dicts and each pattern should be saved in another list\n",
    "    # ex: patterns = [[{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}], [{\"ORTH\": \"Google\"}, {\"ORTH\": \"Maps\"}]]\n",
    "\n",
    "    # This pattern tries to find the ROOT word in the sentence. Once the ROOT is identified, then the pattern checks whether it is followed by a preposition (‘prep’) or an agent word. If yes, then it is added to the ROOT word.\n",
    "    pattern = [{'DEP':'ROOT'}, # check for token with dependency label root\n",
    "            {'DEP':'prep','OP':\"?\"}, # other stuff\n",
    "            {'DEP':'agent','OP':\"?\"},\n",
    "            {'POS':'ADJ','OP':\"?\"}]\n",
    "\n",
    "    # matcher.add(\"match_id\", \"patterns\")\n",
    "    matcher.add(\"matching_1\", [pattern])\n",
    "\n",
    "    matches = matcher(temp_doc)\n",
    "\n",
    "    k = len(matches) - 1\n",
    "    if k == -1: # meaning no match was found so return null\n",
    "        return None\n",
    "\n",
    "    span = temp_doc[matches[k][1]:matches[k][2]]\n",
    "    return span.text\n",
    "\n",
    "def create_kg_csv_pandas(subjects, predicates, objects, re_type, language): # so source (subject) ----relation (predicate)----> target (object)\n",
    "    # field names\n",
    "    fields = [language + '_Subject', language + '_Predicate', language + '_Object']\n",
    "    filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\kg_of_\" + re_type + \"_\" + language + \".csv\"\n",
    "    # data rows of csv file\n",
    "    rows = [[subjects[i], predicates[i], objects[i]] for i in range(len(subjects))] # best to check if we have empty values\n",
    "\n",
    "    temp_df = pd.DataFrame(rows, columns = fields)\n",
    "    temp_df.to_csv(filename, header = fields)\n",
    "\n",
    "def create_kg_csv(subjects, predicates, objects, re_type, language):\n",
    "    \"\"\"\n",
    "    [source(subject) --relation(predicate)--> target(object)]\n",
    "    :param subjects: source\n",
    "    :param predicates: relation\n",
    "    :param objects: target\n",
    "    :param re_type: currently, we have only relation extraction type of \"simple\" and \"predefined-dictionary\"\n",
    "    :return: returns nothing only creates the csv file\n",
    "    \"\"\"\n",
    "    # field names\n",
    "    fields = ['Subject', 'Predicate', 'Object']\n",
    "    filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\kg_of_\" + re_type +  \"_\" + language + \".csv\"\n",
    "\n",
    "    rows = [[subjects[i], predicates[i], objects[i]] for i in range(len(subjects))]\n",
    "\n",
    "    # find out empty and None strings, replacing it with \"-\"\n",
    "    for i in range(len(rows)):\n",
    "        for j in range(len(rows[0])): # so len 3\n",
    "            if rows[i][j] == \"\" or rows[i][j] is None: rows[i][j] = \"-\"\n",
    "\n",
    "    # writing to csv file\n",
    "    with open(filename, 'w', newline = '') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(fields) # first writing fields\n",
    "        csv_writer.writerows(rows) # now the remaining record\n",
    "\n",
    "# German: de, French: fr, but can work for any other languages too\n",
    "def simple_translator(target_language, re_type, model_used): # assumes that en kg csv is already created\n",
    "    translator = Translator()\n",
    "\n",
    "    filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "    filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "    filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_GoogleTrans_\" + target_language + \".csv\"\n",
    "\n",
    "    with open(filename_english, mode =\"r\") as file_original, open(filename_assigned_language, mode = \"r+\", newline = '') as file_2:\n",
    "        csv_reader = csv.reader(file_original)\n",
    "        csv_reader_file2 = csv.reader(file_2)\n",
    "        csv_writer = csv.writer(file_2)\n",
    "\n",
    "        current_size_of_new_kg = 0\n",
    "        for step in csv_reader_file2:\n",
    "            current_size_of_new_kg += 1\n",
    "        print(\"Starting from row: \" + str(current_size_of_new_kg))\n",
    "\n",
    "        current_step = 0\n",
    "        for lines in csv_reader: # each line is a list of 3 elements (source - relation - target)\n",
    "            if current_step > current_size_of_new_kg:\n",
    "                source_en, relation_en, target_en = lines[0], lines[1], lines[2]\n",
    "                # src(source) = english, dest(destination) = language to translate to\n",
    "                translated_source, translated_relation, translated_target = translator.translate(source_en, src = \"en\", dest = target_language), translator.translate(relation_en, src = \"en\", dest = target_language), translator.translate(target_en, src = \"en\", dest = target_language)\n",
    "\n",
    "                temp_row = [translated_source.text, translated_relation.text, translated_target.text]\n",
    "                index = 0\n",
    "                for row in temp_row:\n",
    "                    temp_row[index] = row.replace(u'\\u200b', '') # this \"space\" character gives error, but it just adds extra space so just removing it\n",
    "                    index += 1\n",
    "                csv_writer.writerow(temp_row)\n",
    "            else:\n",
    "                current_step += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:08:11.285923100Z",
     "start_time": "2023-06-04T18:08:11.252080Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PIPELINE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "# your own project path dir here\n",
    "my_path = \"C:\\\\Users\\\\dnaen\\\\PycharmProjects\\\\bachelor_thesis_23\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:08:12.852069200Z",
     "start_time": "2023-06-04T18:08:12.824778300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Loading Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "# for downloading the pipeline\n",
    "# !python -m spacy download fr_dep_news_trf\n",
    "# !python -m spacy download de_dep_news_trf\n",
    "# !python -m spacy download en_core_web_trf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:08:13.723035600Z",
     "start_time": "2023-06-04T18:08:13.710549900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "### Loading Pipeline ###\n",
    "# !python -m spacy download en_core_web_trf\n",
    "# !python -m spacy download fr_dep_news_trf\n",
    "# !python -m spacy download de_dep_news_trf\n",
    "# !pip install spacy-transformers\n",
    "lan = \"de\" # options: en, fr, de\n",
    "nlp = spacy.load(\"de_dep_news_trf\") # A large English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, syntax and entities."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:08:25.717209600Z",
     "start_time": "2023-06-04T18:08:22.264365400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Importing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnaen\\anaconda3\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "### Importing Dataset ###\n",
    "\n",
    "# automatically extracting path\n",
    "current_path = os.getcwd()\n",
    "data_path = my_path + \"\\\\data\\\\\" + lan + \"\\\\directives_pdf\"\n",
    "\n",
    "doc_list = find_folder_with_type(data_path, '.pdf') # detection of pdf files in the folder\n",
    "\n",
    "# importing the file, here doc is like a \"list\" of tokens (each tok is either a word, number, ...)\n",
    "file_list = folder_to_nlp_doc(data_path, doc_list, nlp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:11:56.276800400Z",
     "start_time": "2023-06-04T18:08:28.902515900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Entities (Nodes) and Relations (Edges) Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 1\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:16<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:17<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 2\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:26<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:29<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 3\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:50<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:13<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 4\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:22<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:22<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 5\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202/202 [00:33<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202/202 [00:33<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 6\n",
      "Entities (Nodes - subject/object) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:33<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations (Edges - predicate) Extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:30<00:00,  8.17it/s]\n"
     ]
    }
   ],
   "source": [
    "list_of_entity_pairs = [] # this is a list of lists (so each index contains the info of a data file)\n",
    "list_of_relations = []\n",
    "triple_id = [] # TODO add\n",
    "count = 1\n",
    "\n",
    "for file in file_list:\n",
    "    print(\"Document number \" + str(count))\n",
    "    count += 1\n",
    "\n",
    "    sents = list(file.sents) # extract sentences of the document (only checks for \".\")\n",
    "    sents_pruned = [sentence for sentence in sents if len(sentence) > 5]\n",
    "\n",
    "    print(\"Entities (Nodes - subject/object) Extraction\")\n",
    "    entity_pairs = [] # here \"tqdm\" is just used for creating a progress bar\n",
    "    # Extracting the entity pairs of each sentence.\n",
    "    if lan == \"en\": entity_pairs = [get_entities_for_english(str(i)) for i in tqdm(sents_pruned)]\n",
    "    elif lan == \"fr\": entity_pairs = [get_entities_for_french(str(i)) for i in tqdm(sents_pruned)]\n",
    "    elif lan == \"de\": entity_pairs = [get_entities_for_german(str(i)) for i in tqdm(sents_pruned)]\n",
    "    else: print(\"Current chosen language does not exist!\")\n",
    "\n",
    "    print(\"Relations (Edges - predicate) Extraction\")\n",
    "    relations = []\n",
    "    relations = [get_relation(str(i)) for i in tqdm(sents_pruned)] # Here we assume predicate is the main verb in a sentence.\n",
    "\n",
    "    # pd.Series(relations).value_counts()[:10] # to visualize\n",
    "    list_of_entity_pairs.append(entity_pairs)\n",
    "    list_of_relations.append(relations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:27:56.729647300Z",
     "start_time": "2023-06-04T18:21:25.922952900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Build Knowledge Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [i[0] for i in entity_pairs]\n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in entity_pairs]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:27:56.749982900Z",
     "start_time": "2023-06-04T18:27:56.732640100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', 'pakts über ipbpr recht', 'artikel', 'union', 'anerkennung', 'vertrags über union aeuv zusammenarbeit', 'umsetzung', 'maß']\n"
     ]
    }
   ],
   "source": [
    "print(source[0:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:27:56.802316100Z",
     "start_time": "2023-06-04T18:27:56.745620800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everythings fine\n"
     ]
    }
   ],
   "source": [
    "temp_source = []\n",
    "temp_relation = []\n",
    "temp_target = []\n",
    "\n",
    "source_uni = []\n",
    "relation_uni = []\n",
    "target_uni = []\n",
    "\n",
    "# for removing the soft hyphen (but can be used for removal of other stuff too)\n",
    "# this has to be done before translating to unicode because otherwise it can't detect it for some reason\n",
    "for i in range(len(source)):\n",
    "    temp_source.append(source[i].replace(\" ­\", \"\").replace(\"  \", \" \"))\n",
    "    if relations[i] is not None: temp_relation.append(relations[i].replace(\" ­\", \"\").replace(\"  \", \" \"))\n",
    "    else: temp_relation.append(relations[i])\n",
    "    temp_target.append(target[i].replace(\" ­\", \"\").replace(\"  \", \" \"))\n",
    "\n",
    "# for making it usable in english text\n",
    "for i in range(len(temp_source)):\n",
    "    source_uni.append(unidecode(temp_source[i]))\n",
    "    if temp_relation[i] is not None: relation_uni.append(unidecode(temp_relation[i])) # replacing None with \"-\"\n",
    "    else: relation_uni.append(\"-\")\n",
    "    target_uni.append(unidecode(temp_target[i]))\n",
    "\n",
    "\n",
    "if len(source_uni) == len(relation_uni) == len(target_uni): # making sure we get same number of rows in the end\n",
    "    print(\"Everythings fine\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:30:12.472548Z",
     "start_time": "2023-06-04T18:30:12.446922Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "# kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})\n",
    "create_kg_csv(source_uni, relation_uni, target_uni, \"simple\", lan)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T18:30:13.346275700Z",
     "start_time": "2023-06-04T18:30:13.307982600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. Translate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# TODO REGEX SEARCH\n",
    "# need to do some kind of regex search here\n",
    "simple_translator(\"de\", \"simple\") # CAUTION: since this uses API it can \"timeout\", so if happens just run it again"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T17:44:37.199534Z",
     "end_time": "2023-05-04T17:45:40.833129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "simple_translator(\"fr\", \"simple\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T17:45:40.830776Z",
     "end_time": "2023-05-04T17:46:40.122596Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRASH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kg_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"We will use the networkx library to create a network from this dataframe. Which is going to be a directed graph allowing us to draw a line from subject to object. However, we also need to use pyvis library because networkx's visualize method is currently not working.\"\"\"\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# create a directed-graph from a dataframe\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m directed_graph \u001B[38;5;241m=\u001B[39m nx\u001B[38;5;241m.\u001B[39mfrom_pandas_edgelist(\u001B[43mkg_df\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m, edge_attr\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, create_using\u001B[38;5;241m=\u001B[39mnx\u001B[38;5;241m.\u001B[39mMultiDiGraph())\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyvis\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnetwork\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Network\n\u001B[0;32m      8\u001B[0m net \u001B[38;5;241m=\u001B[39m Network(notebook\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, cdn_resources\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mremote\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'kg_df' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"We will use the networkx library to create a network from this dataframe. Which is going to be a directed graph allowing us to draw a line from subject to object. However, we also need to use pyvis library because networkx's visualize method is currently not working.\"\"\"\n",
    "\n",
    "# create a directed-graph from a dataframe\n",
    "# directed_graph = nx.from_pandas_edgelist(kg_df, \"source\", \"target\", edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "#\n",
    "# from pyvis.network import Network\n",
    "#\n",
    "# net = Network(notebook=True, cdn_resources='remote')\n",
    "#\n",
    "# net.from_nx(directed_graph)\n",
    "# net.show(\"example.html\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# count = 1\n",
    "#\n",
    "# for file in file_list:\n",
    "#     print(\"Document number \" + str(count))\n",
    "#     count += 1\n",
    "#\n",
    "#     sents = list(file.sents) # extract sentences of the document (only checks for \".\")\n",
    "#\n",
    "#     for i in range(30):\n",
    "#         print(\"Sentence number: \" + str(i))\n",
    "#         print(\"The sentence: \" + str(sents[i]))\n",
    "#         print(\"Entities: \" + str(get_entities(str(sents[i]))))\n",
    "#         print(\"Relation: \" + str(get_relation(str(sents[i]))))\n",
    "#         print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from PyPDF2 import PdfReader\n",
    "#\n",
    "# # creating a pdf reader object\n",
    "# reader = PdfReader(\"C:\\\\Users\\\\dnaen\\\\PycharmProjects\\\\bachelor_thesis_23\\\\data\\\\fr\\\\directives_pdf\\\\Directive_(EU)_2016_800_fr.pdf\")\n",
    "#\n",
    "# all_text = \"\"\n",
    "# for i in range(len(reader.pages)):\n",
    "#     text = reader.pages[i].extract_text()\n",
    "#     text = text.replace(\"\\n\", \"\")\n",
    "#     all_text = all_text + text\n",
    "#\n",
    "# print(all_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# count = 1\n",
    "# for sentence in list(doc.sents):\n",
    "#     print(\"Sentence: \" + str(count))\n",
    "#     print(sentence)\n",
    "#     print()\n",
    "#     count += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://spacy.io/models/fr#fr_dep_news_trf\n",
    "french_list = [\"ROOT\", \"acl\", \"acl:relcl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"aux:pass\", \"aux:tense\", \"case\", \"cc\", \"ccomp\", \"conj\", \"cop\", \"dep\", \"det\", \"expl:comp\", \"expl:pass\", \"expl:subj\", \"fixed\", \"flat:foreign\", \"flat:name\", \"iobj\", \"mark\", \"nmod\", \"nsubj\", \"nsubj:pass\", \"nummod\", \"obj\", \"obl:agent\", \"obl:arg\", \"obl:mod\", \"parataxis\", \"punct\", \"vocative\", \"xcomp\"]\n",
    "\n",
    "# https://spacy.io/models/en#en_core_web_trf\n",
    "english_list = [\"ROOT\", \"acl\", \"acomp\", \"advcl\", \"advmod\", \"agent\", \"amod\", \"appos\", \"attr\", \"aux\", \"auxpass\", \"case\", \"cc\", \"ccomp\", \"compound\", \"conj\", \"csubj\", \"csubjpass\", \"dative\", \"dep\", \"det\", \"dobj\", \"expl\", \"intj\", \"mark\", \"meta\", \"neg\", \"nmod\", \"npadvmod\", \"nsubj\", \"nsubjpass\", \"nummod\", \"oprd\", \"parataxis\", \"pcomp\", \"pobj\", \"poss\", \"preconj\", \"predet\", \"prep\", \"prt\", \"punct\", \"quantmod\", \"relcl\", \"xcomp\"]\n",
    "\n",
    "# https://spacy.io/models/de#de_dep_news_trf\n",
    "german_list = [\"ROOT\", \"ac\", \"adc\", \"ag\", \"ams\", \"app\", \"avc\", \"cc\", \"cd\", \"cj\", \"cm\", \"cp\", \"cvc\", \"da\", \"dep\", \"dm\", \"ep\", \"ju\", \"mnr\", \"mo\", \"ng\", \"nk\", \"nmc\", \"oa\", \"oc\", \"og\", \"op\", \"par\", \"pd\", \"pg\", \"ph\", \"pm\", \"pnc\", \"punct\", \"rc\", \"re\", \"rs\", \"sb\", \"sbp\", \"svp\", \"uc\", \"vo\"]\n",
    "\n",
    "for tk in german_list:\n",
    "    print(tk)\n",
    "    print(spacy.explain(tk))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_entities_for_english(the_file):\n",
    "    \"\"\"\n",
    "    Here we extract the elements in an unsupervised manner, i.e., we will use the grammar of the sentences. The extraction of a single word entity from a sentence is not a tough task. We can easily do this with the help of parts of speech (POS) tags. However, when an entity spans across multiple words, then POS tags alone are not sufficient. To fix this we basically save our previous text's info.\n",
    "\n",
    "    requirements:\n",
    "    - punctuation: \"punct\"\n",
    "    - compound: \"compound\"\n",
    "    - modifier: [\"acl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"meta\", \"neg\", \"nmod\", \"npadvmod\", \"nummod\", \"poss\", \"prep\", \"quantmod\", \"relcl\"]\n",
    "    -\n",
    "    \"\"\"\n",
    "\n",
    "    # init\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag (the relationship between any two words is marked by a dependency tag) of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    # for holding the text that is associated with the current subject/object (can be multiple words)\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    # going through each token\n",
    "    for tok in nlp(the_file):\n",
    "        if \"punct\" != tok.dep_.lower(): # if punctuation mark skip\n",
    "\n",
    "            if \"compound\" == tok.dep_.lower():\n",
    "                prefix = tok.text\n",
    "                if \"compound\" == prv_tok_dep.lower(): # if the previous word was also a 'compound' then add to current text\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # check if modifier (a modifier gives information about another word in the same sentence e.g., blue house)\n",
    "            if tok.dep_.lower().endswith(\"mod\"):\n",
    "                modifier = tok.text\n",
    "                if \"compound\" == prv_tok_dep.lower(): # if previous word was also a 'compound' then add to current text\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # extract first entity - subject\n",
    "            if \"subj\" in tok.dep_.lower():\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # extract second entity - object\n",
    "            if \"obj\" in tok.dep_.lower():\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "\n",
    "    return [ent1.strip().lower(), ent2.strip().lower()]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
