{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dnaen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import os, re, csv\n",
    "from googletrans import Translator # don't forget to run \"!pip install googletrans==3.1.0a0\" before using this\n",
    "import spacy_transformers # might seem not being used but it is required to run the transformers\n",
    "from unidecode import unidecode\n",
    "from PyPDF2 import PdfReader\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import jaro # requires !pip install jaro-winkler\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T22:06:53.388619600Z",
     "start_time": "2023-06-12T22:06:46.027619Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# functions\n",
    "def find_folder_with_type(given_path, doc_type): # returns all documents found in path\n",
    "    doc_list = []\n",
    "    for doc in os.listdir(given_path):\n",
    "        if re.search (r'.*\\%s$' % doc_type, doc) is not None: # even though this shows as error in IDE it's fine\n",
    "            doc_list.append(doc)\n",
    "    return doc_list\n",
    "\n",
    "def folder_to_nlp_doc(given_path, list_of_doc_names, given_nlp):\n",
    "    temp_list = []\n",
    "    # the input should be a list of file contained in a folder\n",
    "    for file_name in list_of_doc_names:\n",
    "        file_path = given_path + \"\\\\\" + file_name\n",
    "\n",
    "        with open(file_path, \"r\", encoding='utf8') as my_file:\n",
    "            all_text = my_file.read()\n",
    "            # \\xc2\\xa\n",
    "            # all_text = all_text.replace(\"\\\\xc2\\\\xa\", \" \")\n",
    "            all_text = all_text.replace(\"\\n\", \" \")\n",
    "            all_text = all_text.replace(\"  \", \" \")\n",
    "\n",
    "            temp_list.append(given_nlp(all_text))\n",
    "    return temp_list\n",
    "\n",
    "def folder_list_to_dic(given_path, given_list): # given file names extracts their texts and saves in a dic\n",
    "    dic = {}\n",
    "    old_path = os.getcwd() # saving the previous working dir so we can switch back to that dir later\n",
    "    os.chdir(given_path)\n",
    "\n",
    "    # the input should be a list of file contained in a folder\n",
    "    for file_name in given_list:\n",
    "        print('importing', file_name, '...')\n",
    "        with open(\"%s\" % file_name, \"r\", encoding='utf8') as my_file:\n",
    "            text = my_file.read()\n",
    "        dic[file_name]= text\n",
    "\n",
    "    os.chdir(old_path)\n",
    "    return dic\n",
    "\n",
    "def get_entities_for_english(the_file):\n",
    "    \"\"\"\n",
    "    Here we extract the elements in an unsupervised manner, i.e., we will use the grammar of the sentences. The extraction of a single word entity from a sentence is not a tough task. We can easily do this with the help of dependency labels. However, when an entity spans across multiple words, then dependency labels alone are not sufficient. To fix this we basically save our previous text's info.\n",
    "\n",
    "    requirements:\n",
    "    - punctuation: [\"punct\"]\n",
    "    - compound: [\"compound\", \"acomp\", \"ccomp\", \"pcomp\"]\n",
    "    - modifier: [\"acl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"meta\", \"neg\", \"nmod\", \"npadvmod\", \"nummod\", \"poss\", \"prep\", \"quantmod\", \"relcl\"]\n",
    "    - subject: [\"csubj\", \"csubjpass\", \"nsubj\", \"nsubjpass\"]\n",
    "    - object: [\"dobj\", \"oprd\", \"pobj\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # params\n",
    "    punctuation_parsers = [\"punct\"]\n",
    "    compound_parsers = [\"compound\", \"acomp\", \"ccomp\", \"pcomp\"]\n",
    "    modifier_parsers = [\"acl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"meta\", \"neg\", \"nmod\", \"npadvmod\", \"nummod\", \"poss\", \"prep\", \"quantmod\", \"relcl\"]\n",
    "    subject_parsers = [\"csubj\", \"csubjpass\", \"nsubj\", \"nsubjpass\"]\n",
    "    object_parsers = [\"dobj\", \"oprd\", \"pobj\"]\n",
    "\n",
    "    # init\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag (the relationship between any two words is marked by a dependency tag) of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    # for holding the text that is associated with the current subject/object (can be multiple words)\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    # going through each token\n",
    "    for tok in nlp(the_file):\n",
    "\n",
    "        if tok.dep_ not in punctuation_parsers: # if punctuation mark skip\n",
    "\n",
    "            if tok.dep_ in compound_parsers:\n",
    "                prefix = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if the previous word was also a 'compound' then add to current text\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # check if modifier (a modifier gives information about another word in the same sentence e.g., blue house)\n",
    "            if tok.dep_ in modifier_parsers:\n",
    "                modifier = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if previous word was also a 'compound' then add to current text\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # extract first entity - subject\n",
    "            if tok.dep_ in subject_parsers:\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # extract second entity - object\n",
    "            if tok.dep_ in object_parsers:\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "\n",
    "    return [ent1.strip().lower().replace(\"  \", \" \"), ent2.strip().lower().replace(\"  \", \" \")]\n",
    "\n",
    "def get_entities_for_french(the_file):\n",
    "    \"\"\"\n",
    "    requirements:\n",
    "    - punctuation = [\"punct\"]\n",
    "    - compound: [\"ccomp\", \"expl:comp\", \"xcomp\", \"fixed\", \"flat:foreign\", \"flat:name\"]\n",
    "    - modifier = [\"acl\", \"acl:relcl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nmod\", \"nummod\", \"obl:mod\"]\n",
    "    - subject = [\"expl:subj\", \"nsubj\", \"nsubj:pass\"]\n",
    "    - object = [\"iobj\", \"obj\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # params\n",
    "    punctuation_parsers = [\"punct\"]\n",
    "    compound_parsers = [\"ccomp\", \"expl:comp\", \"xcomp\", \"fixed\", \"flat:foreign\", \"flat:name\"]\n",
    "    modifier_parsers = [\"acl\", \"acl:relcl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nmod\", \"nummod\", \"obl:mod\"]\n",
    "    subject_parsers = [\"expl:subj\", \"nsubj\", \"nsubj:pass\"]\n",
    "    object_parsers = [\"iobj\", \"obj\"]\n",
    "\n",
    "    # init\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag (the relationship between any two words is marked by a dependency tag) of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    # for holding the text that is associated with the current subject/object (can be multiple words)\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    # going through each token\n",
    "    for tok in nlp(the_file):\n",
    "        if tok.dep_ not in punctuation_parsers: # if punctuation mark skip\n",
    "\n",
    "            if tok.dep_ in compound_parsers:\n",
    "                prefix = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if the previous word was also a 'compound' then add to current text\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # check if modifier (a modifier gives information about another word in the same sentence e.g., blue house)\n",
    "            if tok.dep_ in modifier_parsers:\n",
    "                modifier = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if previous word was also a 'compound' then add to current text\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # extract first entity - subject\n",
    "            if tok.dep_ in subject_parsers:\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # extract second entity - object\n",
    "            if tok.dep_ in object_parsers:\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "\n",
    "    return [ent1.strip().lower(), ent2.strip().lower()]\n",
    "\n",
    "def get_entities_for_german(the_file):\n",
    "    \"\"\"\n",
    "    requirements:\n",
    "    - punctuation = [\"punct\"]\n",
    "    - compound = [\"adc\", \"avc\", \"nmc\", \"pnc\", \"uc\", \"svp\", \"re\", \"pm\", \"par\", \"dep\", \"cvc\", \"app\", \"ag\", \"ac\"]\n",
    "    - modifier = [\"ams\", \"mnr\", \"mo\"]\n",
    "    - subject = [\"sb\", \"sbp\"]\n",
    "    - object = [\"oa\", \"oc\", \"og\", \"op\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # params\n",
    "    punctuation_parsers = [\"punct\"]\n",
    "    compound_parsers = [\"adc\", \"avc\", \"nmc\", \"pnc\", \"uc\", \"svp\", \"re\", \"pm\", \"par\", \"dep\", \"cvc\", \"app\", \"ag\", \"ac\"]\n",
    "    modifier_parsers = [\"ams\", \"mnr\", \"mo\"]\n",
    "    subject_parsers = [\"sb\", \"sbp\"]\n",
    "    object_parsers = [\"oa\", \"oc\", \"og\", \"op\"]\n",
    "\n",
    "    # init\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag (the relationship between any two words is marked by a dependency tag) of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    # for holding the text that is associated with the current subject/object (can be multiple words)\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    # going through each token\n",
    "    for tok in nlp(the_file):\n",
    "        if tok.dep_ not in punctuation_parsers: # if punctuation mark skip\n",
    "\n",
    "            if tok.dep_ in compound_parsers:\n",
    "                prefix = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if the previous word was also a 'compound' then add to current text\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # check if modifier (a modifier gives information about another word in the same sentence e.g., blue house)\n",
    "            if tok.dep_ in modifier_parsers:\n",
    "                modifier = tok.text\n",
    "                if prv_tok_dep in compound_parsers: # if previous word was also a 'compound' then add to current text\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "            # extract first entity - subject\n",
    "            if tok.dep_ in subject_parsers:\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # extract second entity - object\n",
    "            if tok.dep_ in object_parsers:\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "                # reset info\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "\n",
    "    return [ent1.strip().lower(), ent2.strip().lower()]\n",
    "\n",
    "def get_relation(the_sentence):\n",
    "    temp_doc = nlp(the_sentence)\n",
    "\n",
    "    # creating the rule-based Matcher object\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # defining the pattern - Each pattern should be a list of dicts and each pattern should be saved in another list\n",
    "    # ex: patterns = [[{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}], [{\"ORTH\": \"Google\"}, {\"ORTH\": \"Maps\"}]]\n",
    "\n",
    "    # This pattern tries to find the ROOT word in the sentence. Once the ROOT is identified, then the pattern checks whether it is followed by a preposition (‘prep’) or an agent word. If yes, then it is added to the ROOT word.\n",
    "    pattern = [{'DEP':'ROOT'}, # check for token with dependency label root\n",
    "            {'DEP':'prep','OP':\"?\"}, # other stuff\n",
    "            {'DEP':'agent','OP':\"?\"},\n",
    "            {'POS':'ADJ','OP':\"?\"}]\n",
    "\n",
    "    # matcher.add(\"match_id\", \"patterns\")\n",
    "    matcher.add(\"matching_1\", [pattern])\n",
    "\n",
    "    matches = matcher(temp_doc)\n",
    "\n",
    "    k = len(matches) - 1\n",
    "    if k == -1: # meaning no match was found so return null\n",
    "        return None\n",
    "\n",
    "    span = temp_doc[matches[k][1]:matches[k][2]]\n",
    "    return span.text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T22:06:53.483972400Z",
     "start_time": "2023-06-12T22:06:53.390615100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# COPY PASTE FROM VERSION 2!\n",
    "\n",
    "def create_kg_csv(subjects, predicates, objects, re_type, language, given_id_list, model_used = \"\"):\n",
    "    \"\"\"\n",
    "    [source(subject) --relation(predicate)--> target(object)]\n",
    "    :param subjects: source\n",
    "    :param predicates: relation\n",
    "    :param objects: target\n",
    "    :param re_type: currently, we have only relation extraction type of \"simple\" and \"predefined-dictionary\"\n",
    "    :param model_used: currently only \"stanford_OpenIE\"\n",
    "    :param language: en, fr, de\n",
    "    :param given_id_list: represents the id of triple, in this case it's from which directive/legislation it came from\n",
    "    :return: returns nothing only creates the csv file\n",
    "    \"\"\"\n",
    "    # field names\n",
    "    fields = [language + '_Subject', language + '_Predicate', language + '_Object', \"Triple_ID\"]\n",
    "\n",
    "    if model_used:\n",
    "        filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used + \"\\\\kg_of_\" + re_type + \"_\" + language + \".csv\"\n",
    "    else:\n",
    "        filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\kg_of_\" + re_type + \"_\" + language + \".csv\"\n",
    "\n",
    "\n",
    "\n",
    "    rows = [[subjects[i], predicates[i], objects[i], given_id_list[i]] for i in range(len(subjects))]\n",
    "\n",
    "    # find out empty and None strings, replacing it with \"-\"\n",
    "    for i in range(len(rows)):\n",
    "        for j in range(len(rows[0])): # so len 3\n",
    "            if rows[i][j] == \"\" or rows[i][j] is None: rows[i][j] = \"-\"\n",
    "            rows[i][j] = unidecode(rows[i][j].lower())\n",
    "\n",
    "    # writing to csv file\n",
    "    with open(filename, 'w', newline = '') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(fields) # first writing fields\n",
    "        csv_writer.writerows(rows) # now the remaining record\n",
    "\n",
    "# German: de, French: fr, but can work for any other languages too\n",
    "def simple_translator(target_language, re_type, extra_info = \"\", model_used = \"\"): # assumes that en kg csv is already created\n",
    "    translator = Translator()\n",
    "\n",
    "    if extra_info: extra_info = \"_\" + extra_info\n",
    "\n",
    "    if model_used:\n",
    "        filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "        filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "        filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_\" + target_language + extra_info + \".csv\"\n",
    "    else:\n",
    "        filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type\n",
    "        filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "        filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_\" + target_language + extra_info + \".csv\"\n",
    "\n",
    "    with open(filename_english, mode =\"r\") as file_original, open(filename_assigned_language, mode = \"w+\", newline = '') as file_2:\n",
    "        csv_reader = csv.reader(file_original)\n",
    "        csv_reader_file2 = csv.reader(file_2)\n",
    "        csv_writer = csv.writer(file_2)\n",
    "\n",
    "        current_size_of_new_kg = 0\n",
    "        for step in csv_reader_file2:\n",
    "            current_size_of_new_kg += 1\n",
    "        print(\"Starting from row: \" + str(current_size_of_new_kg))\n",
    "\n",
    "        # first line is headers\n",
    "        if current_size_of_new_kg == 0: csv_writer.writerow([target_language + \"_Subject\", target_language + \"_Predicate\", target_language + \"_Object\", \"Triple_ID\"])\n",
    "        current_step = 0\n",
    "        for lines in csv_reader: # each line is a list of 3 elements (source - relation - target)\n",
    "            if current_step > current_size_of_new_kg:\n",
    "                source_en, relation_en, target_en = lines[0], lines[1], lines[2]\n",
    "                # src(source) = english, dest(destination) = language to translate to\n",
    "                translated_source, translated_relation, translated_target = translator.translate(source_en, src = \"en\", dest = target_language), translator.translate(relation_en, src = \"en\", dest = target_language), translator.translate(target_en, src = \"en\", dest = target_language)\n",
    "\n",
    "                temp_row = [unidecode(translated_source.text), unidecode(translated_relation.text), unidecode(translated_target.text), unidecode(lines[3])]\n",
    "                csv_writer.writerow(temp_row)\n",
    "            else:\n",
    "                current_step += 1\n",
    "\n",
    "def get_unique_words_from_triples(given_list_of_triples):\n",
    "    unique_subjects = []\n",
    "    unique_relations = []\n",
    "    unique_objects = []\n",
    "\n",
    "    for triple in given_list_of_triples:\n",
    "        if triple[0] not in unique_subjects: unique_subjects.append(triple[0])\n",
    "        if triple[1] not in unique_relations: unique_relations.append(triple[1])\n",
    "        if triple[2] not in unique_objects: unique_objects.append(triple[2])\n",
    "\n",
    "    return unique_subjects, unique_relations, unique_objects\n",
    "\n",
    "def get_neighbouring_groups(given_text):\n",
    "    text_split = given_text.split()\n",
    "    group = \"\"\n",
    "    group_list = []\n",
    "\n",
    "    # groups of 2, groups of 3, groups of 4, ...\n",
    "    for i in range(len(text_split)):\n",
    "        for j in range(len(text_split) - i):\n",
    "            # group_list.append([(group := group + text_split[k]) if k == 1 else (group := group + text_split[k] + \" \") for k in range(1, len(text_split) - 1)][-1])\n",
    "            for k in range(j, j+i+1):\n",
    "                if k == j+i: group = group + text_split[k]\n",
    "                else: group = group + text_split[k] + \" \"\n",
    "            group_list.append(group)\n",
    "            group = \"\"\n",
    "    return group_list\n",
    "\n",
    "def create_combined_eurovoc(given_lans):\n",
    "    complete_data = []\n",
    "    fields = ['ID']\n",
    "\n",
    "    # adding id\n",
    "    data = pd.read_csv(\"eurovoc_\" + given_lans[0] + \".tsv\",sep='\\t')\n",
    "    data = data.sort_values(by=[\"ID\"])\n",
    "    complete_data.append(list(data.iloc[:, 0]))\n",
    "\n",
    "    # adding the concepts\n",
    "    for lan in given_lans:\n",
    "        fields.append(lan.upper())\n",
    "        data = pd.read_csv(\"eurovoc_\" + lan + \".tsv\",sep='\\t') # TODO what about updated version?\n",
    "        data = data.sort_values(by=[\"ID\"])\n",
    "        complete_data.append(list(data.iloc[:, 1]))\n",
    "\n",
    "\n",
    "    for i in range(1, len(complete_data)):\n",
    "        complete_data[i] = [unidecode(x.lower()) for x in complete_data[i]]\n",
    "\n",
    "    rows = [[complete_data[0][i], complete_data[1][i], complete_data[2][i], complete_data[3][i]] for i in range(len(complete_data[0]))] # TODO find a way to make this line work for any given number of columns\n",
    "\n",
    "    # writing to csv file\n",
    "    filename = os.getcwd() + \"\\\\combined_eurovoc.csv\"\n",
    "    with open(filename, 'w', newline = '') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(fields) # first writing fields\n",
    "        csv_writer.writerows(rows) # now the remaining record\n",
    "\n",
    "def update_triple_csv_file_with_concepts(re_type, language, given_concept_list, model_used = \"\", extra_info = \"\"):\n",
    "    old_path = os.getcwd()\n",
    "    if model_used:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "    else:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type\n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    if extra_info: extra_info = \"_\" + extra_info\n",
    "    data = pd.read_csv(\"kg_of_\" + re_type + \"_\" + language + extra_info + \".csv\")\n",
    "\n",
    "    col_list = list(data.columns)\n",
    "    col_list.remove(\"Triple_ID\")\n",
    "    stopwords_list = []\n",
    "    if language == \"en\": stopwords_list = stopwords.words('english')\n",
    "    if language == \"fr\": stopwords_list = stopwords.words('french')\n",
    "    if language == \"de\": stopwords_list = stopwords.words('german')\n",
    "\n",
    "    for col in col_list:\n",
    "        print(\"Currently at column: \" + col)\n",
    "        new_col = col + \"_concept\"\n",
    "        temp_col = []\n",
    "        old_col = data[col].copy()\n",
    "\n",
    "        current_index = 0\n",
    "        for cell in old_col:\n",
    "            if current_index % 1000 == 0: print(\"Row\" + str(current_index), end = \", \")\n",
    "            if not (cell == \"-\"): temp_col.append(find_corresponding_eurovoc_concept_jaro_winkler(cell, given_concept_list, stopwords_list))\n",
    "            else: temp_col.append(\"[]\")\n",
    "            current_index += 1\n",
    "\n",
    "        data[new_col] = temp_col\n",
    "\n",
    "    # writing to csv file\n",
    "    file_name = path + \"\\\\kg_of_\" + re_type + \"_with_concepts_\" + language + extra_info + \".csv\"\n",
    "    data.to_csv(file_name, index=False)\n",
    "    os.chdir(old_path)\n",
    "\n",
    "def update_df_according_to_neo4j(re_type, language, model_used = \"\", extra_info = \"\"):\n",
    "    old_path = os.getcwd()\n",
    "\n",
    "    if model_used:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "    else:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type\n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    if extra_info: extra_info = \"_\" + extra_info\n",
    "    data = pd.read_csv(\"kg_of_\" + re_type + \"_with_concepts_\" + language + extra_info + \".csv\")\n",
    "\n",
    "    col_list = [language + \"_Subject_concept\", language + \"_Predicate_concept\", language + \"_Object_concept\"]\n",
    "\n",
    "    for col in col_list:\n",
    "        print(\"Currently at column: \" + col)\n",
    "        list_of_col = list(data[col])\n",
    "        temp_col = []\n",
    "\n",
    "        for cell in list_of_col:\n",
    "            if str(cell) == (\"[]\" or None): temp_col.append(\"-\")\n",
    "            else: temp_col.append(str(cell.replace(\", \", \":\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\\"\", \"\").replace(\"'\", \"\")))\n",
    "\n",
    "        data[col] = temp_col\n",
    "\n",
    "    # writing to csv file\n",
    "    file_name = path + \"\\\\kg_of_\" + re_type + \"_with_concepts_\" + language + extra_info + \".csv\"\n",
    "    data.to_csv(file_name, index=False)\n",
    "    os.chdir(old_path)\n",
    "\n",
    "def find_corresponding_eurovoc_concept(given_text, given_concept_list, given_stopwords_list):\n",
    "    \"\"\"\n",
    "    The goal of this function is to find the concepts the given text might be linked to.\n",
    "    :param given_text: takes in a string (this string can be a single word or a compound word (list of words)\n",
    "    :param given_concept_list: the original concept list downloaded from Eurovoc (MAKE SURE THIS IS A LIST!)\n",
    "    :param given_stopwords_list: a list words we don't want to look into (in our case it's stopwords)\n",
    "    :return: a list of concepts it might be related to\n",
    "    \"\"\"\n",
    "    elems_concept = []\n",
    "\n",
    "    # such that we can also check on possible compounds by bruteforce\n",
    "    possible_words_list = get_neighbouring_groups(given_text) # ex: \"I need to\" -> [\"I\", \"need\", \"to, \"I need\", \"need to\", \"I need to\"]\n",
    "\n",
    "    # TODO change the type of string measure you do\n",
    "    for word in possible_words_list:\n",
    "        word = word.lower()\n",
    "        for concept in given_concept_list:\n",
    "            concept = unidecode(concept)\n",
    "            contains = False\n",
    "            if len(word.split()) == len(concept.split()): # checking whether there is same number of words\n",
    "                word_list = word.split()\n",
    "                concept_list_1 = concept.split()\n",
    "\n",
    "                contains = True # set it to false when a mismatch occurs\n",
    "                for i in range(len(word_list)): # this was done such that we can go through compounds too (and applyiing specific operations (e.g., lemmatization) to each word)\n",
    "                    current_word = word_list[i]\n",
    "                    current_concept = concept_list_1[i]\n",
    "\n",
    "                    if current_word not in given_stopwords_list:\n",
    "                        concept_uni = unidecode(current_concept) # because the triples of other languages will be in unidecode\n",
    "                        if current_word not in concept_uni:\n",
    "                            contains = False\n",
    "                    else: contains = False\n",
    "\n",
    "            if contains: elems_concept.append(unidecode(concept))\n",
    "\n",
    "    return elems_concept\n",
    "\n",
    "def fix_duplicate(re_type, language, model_used = \"\"):\n",
    "    old_path = os.getcwd()\n",
    "\n",
    "    if model_used:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "    else:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type\n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    data = pd.read_csv(\"kg_of_\" + re_type + \"_\" + language + \".csv\")\n",
    "    temp_data = data[~data.duplicated()]\n",
    "    target_path = path + \"\\\\kg_of_\" + re_type + \"_\" + language + \".csv\"\n",
    "    temp_data.to_csv(target_path, index = False)\n",
    "    os.chdir(old_path)\n",
    "\n",
    "def find_corresponding_eurovoc_concept_jaro_winkler(given_text, given_concept_list, given_stopwords_list):\n",
    "    \"\"\"\n",
    "    The goal of this function is to find the concepts the given text might be linked to.\n",
    "    :param given_text: takes in a string (this string can be a single word or a compound word (list of words)\n",
    "    :param given_concept_list: the original concept list downloaded from Eurovoc (MAKE SURE THIS IS A LIST!)\n",
    "    :param given_stopwords_list: a list words we don't want to look into (in our case it's stopwords)\n",
    "    :return: a list of concepts it might be related to\n",
    "    \"\"\"\n",
    "    elems_concept = []\n",
    "\n",
    "    # such that we can also check on possible compounds by bruteforce\n",
    "    possible_words_list = get_neighbouring_groups(given_text) # ex: \"I need to\" -> [\"I\", \"need\", \"to, \"I need\", \"need to\", \"I need to\"]\n",
    "\n",
    "    for word in possible_words_list:\n",
    "        word = word.lower()\n",
    "        if word not in given_stopwords_list:\n",
    "            for concept in given_concept_list:\n",
    "                concept = unidecode(concept)\n",
    "                if len(word.split()) == len(concept.split()): # checking whether there is same number of words\n",
    "                    if jaro.jaro_winkler_metric(word, concept) > 0.95: elems_concept.append(unidecode(concept))\n",
    "\n",
    "    return elems_concept"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T22:06:53.553663300Z",
     "start_time": "2023-06-12T22:06:53.501077Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PIPELINE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# your own project path dir here\n",
    "my_path = \"C:\\\\Users\\\\dnaen\\\\PycharmProjects\\\\bachelor_thesis_23\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T22:33:48.118048800Z",
     "start_time": "2023-06-12T22:33:48.050321300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Loading Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# for downloading the pipeline\n",
    "# !python -m spacy download fr_dep_news_trf\n",
    "# !python -m spacy download de_dep_news_trf\n",
    "# !python -m spacy download en_core_web_trf\n",
    "# !pip install spacy-transformers # needed to install this to run trf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T22:33:48.413686400Z",
     "start_time": "2023-06-12T22:33:48.367665900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "lan = \"de\" # options: en, fr, de\n",
    "nlp = None\n",
    "\n",
    "if lan == \"en\": nlp = spacy.load(\"en_core_web_trf\") # English transformer pipeline (roberta-base)\n",
    "if lan == \"fr\": nlp = spacy.load(\"fr_dep_news_trf\")\n",
    "if lan == \"de\": nlp = spacy.load(\"de_dep_news_trf\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T22:47:15.471094Z",
     "start_time": "2023-06-12T22:47:05.337639500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Importing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnaen\\anaconda3\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "### Importing Dataset ###\n",
    "\n",
    "# automatically extracting path\n",
    "current_path = os.getcwd()\n",
    "data_path = my_path + \"\\\\data\\\\\" + lan + \"\\\\directives_txt\"\n",
    "\n",
    "doc_list = find_folder_with_type(data_path, '.txt') # detection of pdf files in the folder\n",
    "\n",
    "# importing the file, here doc is like a \"list\" of tokens (each tok is either a word, number, ...)\n",
    "file_list = folder_to_nlp_doc(data_path, doc_list, nlp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T22:54:08.634485400Z",
     "start_time": "2023-06-12T22:47:15.476588800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Entities (Nodes) and Relations (Edges) Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number 1\n",
      "Entities (Nodes - subject/object) Extraction\n",
      "Relations (Edges - predicate) Extraction\n",
      "Document number 2\n",
      "Entities (Nodes - subject/object) Extraction\n",
      "Relations (Edges - predicate) Extraction\n",
      "Document number 3\n",
      "Entities (Nodes - subject/object) Extraction\n",
      "Relations (Edges - predicate) Extraction\n",
      "Document number 4\n",
      "Entities (Nodes - subject/object) Extraction\n",
      "Relations (Edges - predicate) Extraction\n",
      "Document number 5\n",
      "Entities (Nodes - subject/object) Extraction\n",
      "Relations (Edges - predicate) Extraction\n",
      "Document number 6\n",
      "Entities (Nodes - subject/object) Extraction\n",
      "Relations (Edges - predicate) Extraction\n"
     ]
    }
   ],
   "source": [
    "list_of_entity_pairs = [] # this is a list of lists (so each index contains the info of a data file)\n",
    "list_of_relations = []\n",
    "triple_id = []\n",
    "count = 1\n",
    "\n",
    "for file, doc_name in zip(file_list, doc_list):\n",
    "    print(\"Document number \" + str(count))\n",
    "    count += 1\n",
    "\n",
    "    sents = list(file.sents) # extract sentences of the document (only checks for \".\")\n",
    "    sents_pruned = [sentence for sentence in sents if len(sentence) > 5]\n",
    "\n",
    "    print(\"Entities (Nodes - subject/object) Extraction\")\n",
    "    entity_pairs = []\n",
    "\n",
    "    # Extracting the entity pairs of each sentence.\n",
    "    if lan == \"en\": entity_pairs = [get_entities_for_english(str(sent)) for sent in sents_pruned]\n",
    "    elif lan == \"fr\": entity_pairs = [get_entities_for_french(str(sent)) for sent in sents_pruned]\n",
    "    elif lan == \"de\": entity_pairs = [get_entities_for_german(str(sent)) for sent in sents_pruned]\n",
    "    else: print(\"Current chosen language does not exist!\")\n",
    "\n",
    "    print(\"Relations (Edges - predicate) Extraction\")\n",
    "    relations = [get_relation(str(sent)) for sent in sents_pruned] # Here we assume predicate is the main verb in a sentence.\n",
    "\n",
    "    # pd.Series(relations).value_counts()[:10] # to visualize\n",
    "    triple_id.extend([doc_name] * len(sents_pruned))\n",
    "    list_of_entity_pairs.extend(entity_pairs)\n",
    "    list_of_relations.extend(relations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T22:59:48.994644200Z",
     "start_time": "2023-06-12T22:54:08.643547900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Build Knowledge Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [i[0] for i in list_of_entity_pairs]\n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in list_of_entity_pairs]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T22:59:49.032455800Z",
     "start_time": "2023-06-12T22:59:49.010966500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything's fine\n"
     ]
    }
   ],
   "source": [
    "# temp_source = []\n",
    "# temp_relation = []\n",
    "# temp_target = []\n",
    "\n",
    "source_uni = []\n",
    "relation_uni = []\n",
    "target_uni = []\n",
    "\n",
    "# # for removing the soft hyphen (but can be used for removal of other stuff too)\n",
    "# # this has to be done before translating to unicode because otherwise it can't detect it for some reason\n",
    "# for i in range(len(source)):\n",
    "#     temp_source.append(source[i].replace(\" ­\", \"\").replace(\"  \", \" \"))\n",
    "#     if list_of_relations[i] is not None: temp_relation.append(list_of_relations[i].replace(\" ­\", \"\").replace(\"  \", \" \"))\n",
    "#     else: temp_relation.append(list_of_relations[i])\n",
    "#     temp_target.append(target[i].replace(\" ­\", \"\").replace(\"  \", \" \"))\n",
    "\n",
    "# for making it usable in english text\n",
    "for i in range(len(source)):\n",
    "    source_uni.append(unidecode(source[i]))\n",
    "    if list_of_relations[i] is not None: relation_uni.append(unidecode(list_of_relations[i])) # replacing None with \"-\"\n",
    "    else: relation_uni.append(\"-\")\n",
    "    target_uni.append(unidecode(target[i]))\n",
    "\n",
    "\n",
    "if len(source_uni) == len(relation_uni) == len(target_uni): # making sure we get same number of rows in the end\n",
    "    print(\"Everything's fine\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T22:59:49.052861100Z",
     "start_time": "2023-06-12T22:59:49.026931500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1561.0669739246368\n"
     ]
    }
   ],
   "source": [
    "# Handling the \"main\" language first\n",
    "data_path = my_path + \"\\\\src\\\\main\"\n",
    "os.chdir(data_path)\n",
    "\n",
    "path_to_eurovoc = \"eurovoc_\" + lan + \".tsv\"\n",
    "data_of_lan = pd.read_csv(path_to_eurovoc,sep='\\t')\n",
    "\n",
    "# remember to pass only the unidecode version of the string to here\n",
    "create_kg_csv(source_uni, relation_uni, target_uni, \"simple\", lan, triple_id)\n",
    "print(time.time() - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T22:59:49.134167800Z",
     "start_time": "2023-06-12T22:59:49.065015500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "fix_duplicate(\"simple\", lan) # removing duplicates"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T11:44:38.185653400Z",
     "start_time": "2023-06-09T11:44:38.098450300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at column: de_Subject\n",
      "Currently at column: de_Predicate\n",
      "Currently at column: de_Object\n"
     ]
    }
   ],
   "source": [
    "# adding to which concept each triple is related to\n",
    "update_triple_csv_file_with_concepts(\"simple\", lan, list(data_of_lan[lan.upper()]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T11:58:11.175427100Z",
     "start_time": "2023-06-09T11:44:38.129356600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at column: fr_Subject_concept\n",
      "Currently at column: fr_Predicate_concept\n",
      "Currently at column: fr_Object_concept\n"
     ]
    }
   ],
   "source": [
    "# turning list to a:b:c:d style instead of [a,b,c,d] because \",\" causes problem in neo4j\n",
    "update_df_according_to_neo4j(\"simple\", lan)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T09:42:11.097298900Z",
     "start_time": "2023-06-09T09:42:11.038537Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. Translate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from row: 0\n"
     ]
    }
   ],
   "source": [
    "simple_translator(\"de\", \"simple\", extra_info = \"translated\") # CAUTION: since this uses API it can \"timeout\", so if happens just run it again"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T18:20:14.209218300Z",
     "start_time": "2023-06-08T18:10:07.832525400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from row: 0\n"
     ]
    }
   ],
   "source": [
    "simple_translator(\"fr\", \"simple\", extra_info = \"translated\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T18:44:20.490971700Z",
     "start_time": "2023-06-08T18:35:33.723497300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6. KG finalization for other languages\n",
    "(this part has to be ran after the \"translation\" has been done)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at column: de_Subject\n",
      "Row0, Row1000, Currently at column: de_Predicate\n",
      "Row0, Row1000, Currently at column: de_Object\n",
      "Row0, Row1000, Currently at column: de_Subject_concept\n",
      "Currently at column: de_Predicate_concept\n",
      "Currently at column: de_Object_concept\n"
     ]
    }
   ],
   "source": [
    "# French\n",
    "lan = \"fr\"\n",
    "\n",
    "path_to_eurovoc = \"eurovoc_\" + lan + \".tsv\"\n",
    "data_of_lan = pd.read_csv(path_to_eurovoc, sep='\\t')\n",
    "update_triple_csv_file_with_concepts(\"simple\", lan, list(data_of_lan[lan.upper()]))\n",
    "update_df_according_to_neo4j(\"simple\", lan)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T16:54:14.963300700Z",
     "start_time": "2023-06-11T16:41:48.697661300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at column: de_Subject\n",
      "Row0, Row1000, Row2000, Row3000, Row4000, Row5000, Row6000, Row7000, Row8000, Row9000, Row10000, Row11000, Row12000, Currently at column: de_Predicate\n",
      "Row0, Row1000, Row2000, Row3000, Row4000, Row5000, Row6000, Row7000, Row8000, Row9000, Row10000, Row11000, Row12000, Currently at column: de_Object\n",
      "Row0, Row1000, Row2000, Row3000, Row4000, Row5000, Row6000, Row7000, Row8000, Row9000, Row10000, Row11000, Row12000, Currently at column: de_Subject_concept\n",
      "Currently at column: de_Predicate_concept\n",
      "Currently at column: de_Object_concept\n"
     ]
    }
   ],
   "source": [
    "lan = \"de\"\n",
    "\n",
    "path_to_eurovoc = \"eurovoc_\" + lan + \".tsv\"\n",
    "data_of_lan = pd.read_csv(path_to_eurovoc, sep='\\t')\n",
    "update_triple_csv_file_with_concepts(\"predefined_dictionary\", lan, list(data_of_lan[lan.upper()]), \"stanford_OpenIE\")\n",
    "update_df_according_to_neo4j(\"predefined_dictionary\", lan, \"stanford_OpenIE\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T08:18:43.240516900Z",
     "start_time": "2023-06-11T23:09:22.551481800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STORAGE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# create a directed-graph from a dataframe\n",
    "# directed_graph = nx.from_pandas_edgelist(kg_df, \"source\", \"target\", edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "#\n",
    "# from pyvis.network import Network\n",
    "#\n",
    "# net = Network(notebook=True, cdn_resources='remote')\n",
    "#\n",
    "# net.from_nx(directed_graph)\n",
    "# net.show(\"example.html\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T08:46:16.619837400Z",
     "start_time": "2023-06-07T08:46:16.564005300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# count = 1\n",
    "#\n",
    "# for file in file_list:\n",
    "#     print(\"Document number \" + str(count))\n",
    "#     count += 1\n",
    "#\n",
    "#     sents = list(file.sents) # extract sentences of the document (only checks for \".\")\n",
    "#\n",
    "#     for i in range(30):\n",
    "#         print(\"Sentence number: \" + str(i))\n",
    "#         print(\"The sentence: \" + str(sents[i]))\n",
    "#         print(\"Entities: \" + str(get_entities(str(sents[i]))))\n",
    "#         print(\"Relation: \" + str(get_relation(str(sents[i]))))\n",
    "#         print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from PyPDF2 import PdfReader\n",
    "#\n",
    "# # creating a pdf reader object\n",
    "# reader = PdfReader(\"C:\\\\Users\\\\dnaen\\\\PycharmProjects\\\\bachelor_thesis_23\\\\data\\\\fr\\\\directives_pdf\\\\Directive_(EU)_2016_800_fr.pdf\")\n",
    "#\n",
    "# all_text = \"\"\n",
    "# for i in range(len(reader.pages)):\n",
    "#     text = reader.pages[i].extract_text()\n",
    "#     text = text.replace(\"\\n\", \"\")\n",
    "#     all_text = all_text + text\n",
    "#\n",
    "# print(all_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# count = 1\n",
    "# for sentence in list(doc.sents):\n",
    "#     print(\"Sentence: \" + str(count))\n",
    "#     print(sentence)\n",
    "#     print()\n",
    "#     count += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://spacy.io/models/fr#fr_dep_news_trf\n",
    "# french_list = [\"ROOT\", \"acl\", \"acl:relcl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"aux:pass\", \"aux:tense\", \"case\", \"cc\", \"ccomp\", \"conj\", \"cop\", \"dep\", \"det\", \"expl:comp\", \"expl:pass\", \"expl:subj\", \"fixed\", \"flat:foreign\", \"flat:name\", \"iobj\", \"mark\", \"nmod\", \"nsubj\", \"nsubj:pass\", \"nummod\", \"obj\", \"obl:agent\", \"obl:arg\", \"obl:mod\", \"parataxis\", \"punct\", \"vocative\", \"xcomp\"]\n",
    "#\n",
    "# # https://spacy.io/models/en#en_core_web_trf\n",
    "# english_list = [\"ROOT\", \"acl\", \"acomp\", \"advcl\", \"advmod\", \"agent\", \"amod\", \"appos\", \"attr\", \"aux\", \"auxpass\", \"case\", \"cc\", \"ccomp\", \"compound\", \"conj\", \"csubj\", \"csubjpass\", \"dative\", \"dep\", \"det\", \"dobj\", \"expl\", \"intj\", \"mark\", \"meta\", \"neg\", \"nmod\", \"npadvmod\", \"nsubj\", \"nsubjpass\", \"nummod\", \"oprd\", \"parataxis\", \"pcomp\", \"pobj\", \"poss\", \"preconj\", \"predet\", \"prep\", \"prt\", \"punct\", \"quantmod\", \"relcl\", \"xcomp\"]\n",
    "#\n",
    "# # https://spacy.io/models/de#de_dep_news_trf\n",
    "# german_list = [\"ROOT\", \"ac\", \"adc\", \"ag\", \"ams\", \"app\", \"avc\", \"cc\", \"cd\", \"cj\", \"cm\", \"cp\", \"cvc\", \"da\", \"dep\", \"dm\", \"ep\", \"ju\", \"mnr\", \"mo\", \"ng\", \"nk\", \"nmc\", \"oa\", \"oc\", \"og\", \"op\", \"par\", \"pd\", \"pg\", \"ph\", \"pm\", \"pnc\", \"punct\", \"rc\", \"re\", \"rs\", \"sb\", \"sbp\", \"svp\", \"uc\", \"vo\"]\n",
    "#\n",
    "# for tk in german_list:\n",
    "#     print(tk)\n",
    "#     print(spacy.explain(tk))\n",
    "#     print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def get_entities_for_english(the_file):\n",
    "#     \"\"\"\n",
    "#     Here we extract the elements in an unsupervised manner, i.e., we will use the grammar of the sentences. The extraction of a single word entity from a sentence is not a tough task. We can easily do this with the help of parts of speech (POS) tags. However, when an entity spans across multiple words, then POS tags alone are not sufficient. To fix this we basically save our previous text's info.\n",
    "#\n",
    "#     requirements:\n",
    "#     - punctuation: \"punct\"\n",
    "#     - compound: \"compound\"\n",
    "#     - modifier: [\"acl\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"meta\", \"neg\", \"nmod\", \"npadvmod\", \"nummod\", \"poss\", \"prep\", \"quantmod\", \"relcl\"]\n",
    "#     -\n",
    "#     \"\"\"\n",
    "#\n",
    "#     # init\n",
    "#     ent1 = \"\"\n",
    "#     ent2 = \"\"\n",
    "#\n",
    "#     prv_tok_dep = \"\"    # dependency tag (the relationship between any two words is marked by a dependency tag) of previous token in the sentence\n",
    "#     prv_tok_text = \"\"   # previous token in the sentence\n",
    "#\n",
    "#     # for holding the text that is associated with the current subject/object (can be multiple words)\n",
    "#     prefix = \"\"\n",
    "#     modifier = \"\"\n",
    "#\n",
    "#     # going through each token\n",
    "#     for tok in nlp(the_file):\n",
    "#         if \"punct\" != tok.dep_.lower(): # if punctuation mark skip\n",
    "#\n",
    "#             if \"compound\" == tok.dep_.lower():\n",
    "#                 prefix = tok.text\n",
    "#                 if \"compound\" == prv_tok_dep.lower(): # if the previous word was also a 'compound' then add to current text\n",
    "#                     prefix = prv_tok_text + \" \"+ tok.text\n",
    "#\n",
    "#             # check if modifier (a modifier gives information about another word in the same sentence e.g., blue house)\n",
    "#             if tok.dep_.lower().endswith(\"mod\"):\n",
    "#                 modifier = tok.text\n",
    "#                 if \"compound\" == prv_tok_dep.lower(): # if previous word was also a 'compound' then add to current text\n",
    "#                     modifier = prv_tok_text + \" \"+ tok.text\n",
    "#\n",
    "#             # extract first entity - subject\n",
    "#             if \"subj\" in tok.dep_.lower():\n",
    "#                 ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "#                 # reset info\n",
    "#                 prefix = \"\"\n",
    "#                 modifier = \"\"\n",
    "#                 prv_tok_dep = \"\"\n",
    "#                 prv_tok_text = \"\"\n",
    "#\n",
    "#             # extract second entity - object\n",
    "#             if \"obj\" in tok.dep_.lower():\n",
    "#                 ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "#                 # reset info\n",
    "#                 prefix = \"\"\n",
    "#                 modifier = \"\"\n",
    "#                 prv_tok_dep = \"\"\n",
    "#                 prv_tok_text = \"\"\n",
    "#\n",
    "#             # update variables\n",
    "#             prv_tok_dep = tok.dep_\n",
    "#             prv_tok_text = tok.text\n",
    "#\n",
    "#     return [ent1.strip().lower(), ent2.strip().lower()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for file, doc_name in zip(file_list, doc_list):\n",
    "#     print(\"Document number \" + str(count))\n",
    "#     count += 1\n",
    "#\n",
    "#     sents = list(file.sents) # extract sentences of the document (only checks for \".\")\n",
    "#     sents_pruned = [sentence for sentence in sents if len(sentence) > 5]\n",
    "#\n",
    "#     print(\"Entities (Nodes - subject/object) Extraction\")\n",
    "#     entity_pairs = [] # here \"tqdm\" is just used for creating a progress bar\n",
    "#     # Extracting the entity pairs of each sentence.\n",
    "#     if lan == \"en\": entity_pairs = [get_entities_for_english(str(i)) for i in tqdm(sents_pruned, position=0, leave=True)]\n",
    "#     elif lan == \"fr\": entity_pairs = [get_entities_for_french(str(i)) for i in tqdm(sents_pruned, position=0, leave=True)]\n",
    "#     elif lan == \"de\": entity_pairs = [get_entities_for_german(str(i)) for i in tqdm(sents_pruned, position=0, leave=True)]\n",
    "#     else: print(\"Current chosen language does not exist!\")\n",
    "#\n",
    "#     print(\"Relations (Edges - predicate) Extraction\")\n",
    "#     relations = []\n",
    "#     relations = [get_relation(str(i)) for i in tqdm(sents_pruned, position=0, leave=True)] # Here we assume predicate is the main verb in a sentence.\n",
    "#\n",
    "#     triple_id.extend([doc_name for i in range(len(sents_pruned))])\n",
    "#\n",
    "#     # pd.Series(relations).value_counts()[:10] # to visualize\n",
    "#     list_of_entity_pairs.append(entity_pairs)\n",
    "#     list_of_relations.append(relations)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# old\n",
    "# German: de, French: fr, but can work for any other languages too\n",
    "# def simple_translator(target_language, re_type, model_used, extra_info = \"\"): # assumes that en kg csv is already created\n",
    "#     translator = Translator()\n",
    "#\n",
    "#     filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "#     filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "#     filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_GoogleTrans_\" + target_language + \".csv\"\n",
    "#\n",
    "#     with open(filename_english, mode =\"r\") as file_original, open(filename_assigned_language, mode = \"r+\", newline = '') as file_2:\n",
    "#         csv_reader = csv.reader(file_original)\n",
    "#         csv_reader_file2 = csv.reader(file_2)\n",
    "#         csv_writer = csv.writer(file_2)\n",
    "#\n",
    "#         current_size_of_new_kg = 0\n",
    "#         for step in csv_reader_file2:\n",
    "#             current_size_of_new_kg += 1\n",
    "#         print(\"Starting from row: \" + str(current_size_of_new_kg))\n",
    "#\n",
    "#         current_step = 0\n",
    "#         for lines in csv_reader: # each line is a list of 3 elements (source - relation - target)\n",
    "#             if current_step > current_size_of_new_kg:\n",
    "#                 source_en, relation_en, target_en = lines[0], lines[1], lines[2]\n",
    "#                 # src(source) = english, dest(destination) = language to translate to\n",
    "#                 translated_source, translated_relation, translated_target = translator.translate(source_en, src = \"en\", dest = target_language), translator.translate(relation_en, src = \"en\", dest = target_language), translator.translate(target_en, src = \"en\", dest = target_language)\n",
    "#\n",
    "#                 temp_row = [translated_source.text, translated_relation.text, translated_target.text]\n",
    "#                 index = 0\n",
    "#                 for row in temp_row:\n",
    "#                     temp_row[index] = row.replace(u'\\u200b', '') # this \"space\" character gives error, but it just adds extra space so just removing it\n",
    "#                     index += 1\n",
    "#                 csv_writer.writerow(temp_row)\n",
    "#             else:\n",
    "#                 current_step += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def create_kg_csv(subjects, predicates, objects, re_type, language, given_id_list):\n",
    "#     \"\"\"\n",
    "#     [source(subject) --relation(predicate)--> target(object)]\n",
    "#     :param subjects: source\n",
    "#     :param predicates: relation\n",
    "#     :param objects: target\n",
    "#     :param re_type: currently, we have only relation extraction type of \"simple\" and \"predefined-dictionary\"\n",
    "#     :param language: en, fr, de\n",
    "#     :param given_id_list: represents the id of triple, in this case it's from which directive/legislation it came from\n",
    "#     :return: returns nothing only creates the csv file\n",
    "#     \"\"\"\n",
    "#     # field names\n",
    "#     fields = [language + '_Subject', language + '_Predicate', language + '_Object', \"Triple_ID\"]\n",
    "#     filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\kg_of_\" + re_type +  \"_\" + language + \".csv\"\n",
    "#\n",
    "#     rows = [[subjects[i], predicates[i], objects[i], given_id_list[i]] for i in range(len(subjects))]\n",
    "#\n",
    "#     # find out empty and None strings, replacing it with \"-\"\n",
    "#     for i in range(len(rows)):\n",
    "#         for j in range(len(rows[0])): # so len 3\n",
    "#             if rows[i][j] == \"\" or rows[i][j] is None: rows[i][j] = \"-\"\n",
    "#             rows[i][j] = unidecode(rows[i][j].lower())\n",
    "#\n",
    "#     # writing to csv file\n",
    "#     with open(filename, 'w', newline = '') as csv_file:\n",
    "#         csv_writer = csv.writer(csv_file)\n",
    "#         csv_writer.writerow(fields) # first writing fields\n",
    "#         csv_writer.writerows(rows) # now the remaining record\n",
    "#\n",
    "# # German: de, French: fr, but can work for any other languages too\n",
    "# def simple_translator(target_language, re_type, model_used = \"\", extra_info = \"\"): # assumes that en kg csv is already created\n",
    "#     translator = Translator()\n",
    "#\n",
    "#     if extra_info: extra_info = \"_\" + extra_info\n",
    "#     if model_used:\n",
    "#         filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "#         filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "#         filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_\" + target_language + extra_info + \".csv\"\n",
    "#     else:\n",
    "#         filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type\n",
    "#         filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "#         filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_\" + target_language + extra_info + \".csv\"\n",
    "#\n",
    "#     with open(filename_english, mode =\"r\") as file_original, open(filename_assigned_language, mode = \"r+\", newline = '') as file_2:\n",
    "#         csv_reader = csv.reader(file_original)\n",
    "#         csv_reader_file2 = csv.reader(file_2)\n",
    "#         csv_writer = csv.writer(file_2)\n",
    "#\n",
    "#         current_size_of_new_kg = 0\n",
    "#         for step in csv_reader_file2:\n",
    "#             current_size_of_new_kg += 1\n",
    "#         print(\"Starting from row: \" + str(current_size_of_new_kg))\n",
    "#\n",
    "#         # first line is headers\n",
    "#         if current_size_of_new_kg == 0: csv_writer.writerow([target_language + \"_Subject\", target_language + \"_Predicate\", target_language + \"_Object\", \"Triple_ID\"])\n",
    "#         current_step = 0\n",
    "#         for lines in csv_reader: # each line is a list of 3 elements (source - relation - target)\n",
    "#             if current_step > current_size_of_new_kg:\n",
    "#                 source_en, relation_en, target_en = lines[0], lines[1], lines[2]\n",
    "#                 # src(source) = english, dest(destination) = language to translate to\n",
    "#                 translated_source, translated_relation, translated_target = translator.translate(source_en, src = \"en\", dest = target_language), translator.translate(relation_en, src = \"en\", dest = target_language), translator.translate(target_en, src = \"en\", dest = target_language)\n",
    "#\n",
    "#                 temp_row = [unidecode(translated_source.text), unidecode(translated_relation.text), unidecode(translated_target.text), unidecode(lines[3])]\n",
    "#                 csv_writer.writerow(temp_row)\n",
    "#             else:\n",
    "#                 current_step += 1\n",
    "#\n",
    "# def fix_duplicate(re_type, model_used, language):\n",
    "#     path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "#     os.chdir(path)\n",
    "#\n",
    "#     data = pd.read_csv(\"kg_of_\" + re_type + \"_\" + language + \".csv\")\n",
    "#     temp_data = data[~data.duplicated()]\n",
    "#     target_path = path + \"\\\\kg_of_\" + re_type + \"_\" + language + \".csv\"\n",
    "#     print(target_path)\n",
    "#     temp_data.to_csv(target_path, index = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def folder_to_nlp_doc(given_path, list_of_doc_names, given_nlp):\n",
    "#     temp_list = []\n",
    "#     # the input should be a list of file contained in a folder\n",
    "#     for file_name in list_of_doc_names:\n",
    "#         file_path = given_path + \"\\\\\" + file_name\n",
    "#         reader = PdfReader(file_path)\n",
    "#\n",
    "#         all_text = \"\"\n",
    "#         for i in range(len(reader.pages)):\n",
    "#             text = reader.pages[i].extract_text()\n",
    "#             text = text.replace(\"\\n\", \"\")\n",
    "#             all_text = all_text + text\n",
    "#\n",
    "#         temp_list.append(given_nlp(all_text))\n",
    "#     return temp_list"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
