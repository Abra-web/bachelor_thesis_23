{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Converting the given XML descriptor file into tsv file (Acquiring the annotated dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as Xet # for parsing and creating XML data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cols = ['ID', 'EN'] # will be saving in a tsv with ids and their corresponding terms\n",
    "rows = []\n",
    "\n",
    "# parsing the xml file\n",
    "temp_path = os.getcwd()\n",
    "temp_path = temp_path.replace(\"src\\\\main\", \"data\\\\en\\\\descriptors\\\\desc_en.xml\")\n",
    "xml_parse = Xet.parse(temp_path)\n",
    "root = xml_parse.getroot()\n",
    "\n",
    "# iterate through the elements of xml file\n",
    "for element in root:\n",
    "    rows.append({\"ID\": element.find(\"DESCRIPTEUR_ID\").text, \"EN\": element.find(\"LIBELLE\").text})\n",
    "\n",
    "# creating the tsv file\n",
    "df = pd.DataFrame(rows, columns=cols)\n",
    "df.to_csv('eurovoc.tsv', sep='\\t', index=False) # using sep='\\t' gives us a tsv file instead of csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T09:32:21.593894Z",
     "end_time": "2023-04-21T09:32:21.686926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "        ID                           EN\n0      594               AAMS countries\n1      759              abandoned child\n2     4444               abandoned land\n3     3509                ABM Agreement\n4     4333  abolition of customs duties\n...    ...                          ...\n6792  6252                        Åland\n6793  8005                Örebro county\n6794  8004          Östergötland county\n6795  7874              Šiauliai county\n6796  7854                Žilina region\n\n[6797 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>EN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>594</td>\n      <td>AAMS countries</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>759</td>\n      <td>abandoned child</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4444</td>\n      <td>abandoned land</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3509</td>\n      <td>ABM Agreement</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4333</td>\n      <td>abolition of customs duties</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6792</th>\n      <td>6252</td>\n      <td>Åland</td>\n    </tr>\n    <tr>\n      <th>6793</th>\n      <td>8005</td>\n      <td>Örebro county</td>\n    </tr>\n    <tr>\n      <th>6794</th>\n      <td>8004</td>\n      <td>Östergötland county</td>\n    </tr>\n    <tr>\n      <th>6795</th>\n      <td>7874</td>\n      <td>Šiauliai county</td>\n    </tr>\n    <tr>\n      <th>6796</th>\n      <td>7854</td>\n      <td>Žilina region</td>\n    </tr>\n  </tbody>\n</table>\n<p>6797 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T09:32:50.276029Z",
     "end_time": "2023-04-21T09:32:50.335995Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Using cached flair-0.12.2-py3-none-any.whl (373 kB)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (2.1.0)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.1\n",
      "  Using cached transformer_smaller_training_vocab-0.2.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: janome in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (0.13.4)\n",
      "Requirement already satisfied: tabulate in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (0.8.10)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Using cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (2022.7.9)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (1.2.1)\n",
      "Requirement already satisfied: ftfy in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (6.1.1)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.18.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (4.28.1)\n",
      "Requirement already satisfied: mpld3==0.3 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (0.3)\n",
      "Collecting gdown==4.4.0\n",
      "  Using cached gdown-4.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (2.0.0)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (1.2.13)\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (2.8.2)\n",
      "Requirement already satisfied: langdetect in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (1.0.9)\n",
      "Requirement already satisfied: gensim>=3.8.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (4.3.0)\n",
      "Requirement already satisfied: pptree in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (3.1)\n",
      "Requirement already satisfied: conllu>=4.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (4.5.2)\n",
      "Collecting pytorch-revgrad\n",
      "  Using cached pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: wikipedia-api in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (0.5.8)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (3.7.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (4.9.1)\n",
      "Requirement already satisfied: boto3 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (1.24.28)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (9.1.0)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (4.64.1)\n",
      "Requirement already satisfied: segtok>=1.5.7 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from flair) (1.5.11)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (4.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (3.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.23.5)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (0.1.98)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
      "Collecting FuzzyTM>=0.4.0\n",
      "  Using cached FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from gensim>=3.8.0->flair) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from gensim>=3.8.0->flair) (6.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (22.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (4.5.0)\n",
      "Requirement already satisfied: py4j in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.10.9.7)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.0.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (3.0)\n",
      "Requirement already satisfied: future in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.18.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.4.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (5.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.25.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (1.11.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from tqdm>=4.26.0->flair) (0.4.6)\n",
      "Requirement already satisfied: datasets<3.0.0,>=2.0.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from transformer-smaller-training-vocab>=0.2.1->flair) (2.11.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]>=4.18.0->flair) (0.13.3)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]>=4.18.0->flair) (3.20.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from boto3->flair) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from boto3->flair) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from boto3->flair) (1.27.59)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from botocore<1.28.0,>=1.27.28->boto3->flair) (1.26.14)\n",
      "Requirement already satisfied: pandas in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.5.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (2022.11.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (3.8.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (3.2.0)\n",
      "Collecting pyfume\n",
      "  Using cached pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=2.2.3->flair) (3.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.2.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from jinja2->torch!=1.8,>=1.5.0->flair) (2.1.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.2.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (4.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dnaen\\anaconda3\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (2022.7)\n",
      "Collecting simpful\n",
      "  Using cached simpful-2.10.0-py3-none-any.whl (31 kB)\n",
      "Collecting fst-pso\n",
      "  Using cached fst_pso-1.8.1-py3-none-any.whl\n",
      "Collecting miniful\n",
      "  Using cached miniful-0.0.6-py3-none-any.whl\n",
      "Installing collected packages: simpful, miniful, hyperopt, pytorch-revgrad, gdown, fst-pso, pyfume, transformer-smaller-training-vocab, FuzzyTM, bpemb, flair\n",
      "Successfully installed FuzzyTM-2.0.5 bpemb-0.3.4 flair-0.12.2 fst-pso-1.8.1 gdown-4.4.0 hyperopt-0.2.7 miniful-0.0.6 pyfume-0.2.25 pytorch-revgrad-0.2.0 simpful-2.10.0 transformer-smaller-training-vocab-0.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install flair"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T09:53:10.429906Z",
     "end_time": "2023-04-21T09:53:21.635157Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T10:15:58.215132Z",
     "end_time": "2023-04-21T10:16:03.030860Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# functions\n",
    "def original_to_annotated_transformer():\n",
    "    return None\n",
    "\n",
    "def get_tokens_with_entities(raw_text: str):\n",
    "    # split the text by spaces only if the space does not occur between square brackets\n",
    "    # we do not want to split \"multi-word\" entity value yet\n",
    "    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n",
    "\n",
    "    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n",
    "    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n",
    "    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M)\n",
    "\n",
    "    tokens_with_entities = []\n",
    "\n",
    "    for raw_token in raw_tokens:\n",
    "        match = entity_value_pattern_compiled.match(raw_token)\n",
    "        if match:\n",
    "            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n",
    "\n",
    "            # we prefix the name of entity differently\n",
    "            # B- indicates beginning of an entity\n",
    "            # I- indicates the token is not a new entity itself but rather a part of existing one\n",
    "            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n",
    "                entity_prefix = \"B\" if i == 0 else \"I\"\n",
    "                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n",
    "                tokens_with_entities.append((raw_entity_token, entity_name))\n",
    "        else:\n",
    "            tokens_with_entities.append((raw_token, \"O\"))\n",
    "\n",
    "    return tokens_with_entities\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T10:18:54.104308Z",
     "end_time": "2023-04-21T10:18:54.123545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m temp_path \u001B[38;5;241m=\u001B[39m temp_path\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mmain\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mmain\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124meurovoc.tsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(temp_path , sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m corpus_functions \u001B[38;5;241m=\u001B[39m \u001B[43mCorpus\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m tag_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mner\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     12\u001B[0m tag_dictionary \u001B[38;5;241m=\u001B[39m corpus_functions\u001B[38;5;241m.\u001B[39mmake_label_dictionary(label_type \u001B[38;5;241m=\u001B[39m tag_type, train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\flair\\data.py:1215\u001B[0m, in \u001B[0;36mCorpus.__init__\u001B[1;34m(self, train, dev, test, name, sample_missing_splits)\u001B[0m\n\u001B[0;32m   1212\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m name\n\u001B[0;32m   1214\u001B[0m \u001B[38;5;66;03m# abort if no data is provided\u001B[39;00m\n\u001B[1;32m-> 1215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m train \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m dev \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m test:\n\u001B[0;32m   1216\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo data provided when initializing corpus object.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1218\u001B[0m \u001B[38;5;66;03m# sample test data from train if none is provided\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:1527\u001B[0m, in \u001B[0;36mNDFrame.__nonzero__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1525\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m   1526\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__nonzero__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NoReturn:\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1528\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe truth value of a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is ambiguous. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1529\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1530\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus # function?\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings # these embeddings helps NER to perform better\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# getting our data\n",
    "temp_path = os.getcwd()\n",
    "temp_path = temp_path.replace(\"src\\\\main\", \"src\\\\main\\\\eurovoc.tsv\")\n",
    "data = pd.read_csv(temp_path , sep='\\t')\n",
    "\n",
    "corpus_functions = Corpus(data)\n",
    "tag_type = 'ner'\n",
    "tag_dictionary = corpus_functions.make_label_dictionary(label_type = tag_type, train = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Preparation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#####################################################\n",
    "####################################################\n",
    "###################################################\n",
    "\n",
    "def TsvDicProcessing(path):\n",
    "    # !!! It only works with a 2-columns TSV file\n",
    "    Dic = {}\n",
    "    RevDic = {}\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    with open(path, 'rt', encoding='utf8') as csvfile:\n",
    "        myreader = csv.reader(csvfile, delimiter='\\t')\n",
    "        rcount = 0\n",
    "        for row in myreader:\n",
    "            rcount += 1\n",
    "            ccount = 0\n",
    "            if rcount > 1:\n",
    "                for cells in row:\n",
    "                    ccount += 1\n",
    "                    if ccount ==1:\n",
    "                        list1.append(cells)\n",
    "                        key = cells\n",
    "                    else:\n",
    "                        list2.append(cells)\n",
    "                        value = cells\n",
    "                Dic[key] = value\n",
    "                RevDic[value] = key\n",
    "    return Dic, RevDic, list1, list2\n",
    "\n",
    "\n",
    "def FolderListWithTerminaison(terminaison):\n",
    "    DocList = []\n",
    "    for doc in os.listdir():\n",
    "        if re.search (r'.*\\%s$' % terminaison, doc) is not None:\n",
    "            DocList.append(doc)\n",
    "    return DocList\n",
    "\n",
    "def FolderListToDic(List):\n",
    "    Dic = {}\n",
    "    # the input should be a list of file contained in a folder\n",
    "    for FileName in List:\n",
    "        print('importing', FileName, '...')\n",
    "        with open(\"%s\" % FileName, \"r\", encoding='utf8') as myfile:\n",
    "            text = myfile.read()\n",
    "        Dic[FileName]= text\n",
    "    return Dic\n",
    "\n",
    "def TokenCleaning(token, stemmer):\n",
    "    token = token.lower()\n",
    "    token = stemmer_en.stem(token)\n",
    "    return token\n",
    "\n",
    "def RegexFromTerm(term, stemmer):\n",
    "\n",
    "    regex = r\"\\b(\" # Regex Opening\n",
    "\n",
    "    # Adding terms to regex\n",
    "    tokensList = nltk.word_tokenize(term)\n",
    "\n",
    "    # in case of one-word term\n",
    "    if len(tokensList) == 1:\n",
    "        for token in tokensList:\n",
    "            regex += TokenCleaning(token, stemmer)\n",
    "    # if it is a multi-word term\n",
    "    else:\n",
    "        decount = len(tokensList)\n",
    "        for token in tokensList:\n",
    "            decount = decount-1\n",
    "            # add between-words\n",
    "            if decount != len(tokensList)-1:\n",
    "                regex+= r'\\w*\\W\\w*\\W*'\n",
    "            # add token\n",
    "            regex += TokenCleaning(token, stemmer)\n",
    "\n",
    "\n",
    "    regex += '''\\w{0,5})(\\W)''' # Regex Closure\n",
    "    return regex"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T09:36:53.619047Z",
     "end_time": "2023-04-24T09:36:53.669255Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EurovocDic:\n",
      "{'594': 'AAMS countries', '759': 'abandoned child', '4444': 'abandoned land', '3509': 'ABM Agreement', '4333': 'abolition of customs duties', '4504': 'abortion', '5075': 'Abruzzi', '5339': 'absenteeism', '1746': 'absolute majority', '5984': 'abstentionism'}\n",
      "\n",
      "EurovocReverseDic:\n",
      "{'AAMS countries': '594', 'abandoned child': '759', 'abandoned land': '4444', 'ABM Agreement': '3509', 'abolition of customs duties': '4333', 'abortion': '4504', 'Abruzzi': '5075', 'absenteeism': '5339', 'absolute majority': '1746', 'abstentionism': '5984'}\n",
      "\n",
      "URIList:\n",
      "['594', '759', '4444', '3509', '4333']\n",
      "\n",
      "ConceptList:\n",
      "['AAMS countries', 'abandoned child', 'abandoned land', 'ABM Agreement', 'abolition of customs duties']\n",
      "\n",
      "Eurovoc importated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dnaen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, csv, re, nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from itertools import islice\n",
    "\n",
    "nltk.download('punkt') # unsupervised trainable model, which means it can be trained on unlabeled data (Data that has not been tagged with information identifying its characteristics, properties, or categories is referred to as unlabeled data.)\n",
    "\n",
    "# creation of a Eurovoc dictionary from the TSF\n",
    "TsvFile = \"eurovoc.tsv\"\n",
    "\n",
    "# getting info of ids and concepts\n",
    "EurovocDic, EurovocReverseDic, URIList, ConceptList = TsvDicProcessing(TsvFile)\n",
    "print('Eurovoc importated!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T10:43:38.580799Z",
     "end_time": "2023-04-24T10:43:38.613875Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "#=====================\n",
    "\n",
    "# move folder\n",
    "\n",
    "print('moving to corpus folder...')\n",
    "\n",
    "# detection of TXT in the folder\n",
    "\n",
    "# storing document content in a dictionary\n",
    "temp_path = os.getcwd()\n",
    "temp_path = temp_path.replace(\"src\\\\main\", \"data\\\\en\\\\directives_txt\\\\Directive_(EU)_2016_343_en.txt\")\n",
    "DocList = [temp_path]\n",
    "\n",
    "DocumentDic = FolderListToDic(DocList)\n",
    "#====================="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagging C:\\Users\\dnaen\\PycharmProjects\\bachelor_thesis_23\\data\\en\\directives_txt\\Directive_(EU)_2016_343_en.txt ...\n"
     ]
    }
   ],
   "source": [
    "# tagging by researching concept-regexed as a substring of the text\n",
    "\n",
    "stemmer_en = SnowballStemmer(\"english\")\n",
    "\n",
    "for DocName in DocList:\n",
    "    tagsList=[]\n",
    "    taggedText = \"\"\n",
    "    print('tagging', DocName,'...')\n",
    "    text = DocumentDic[DocName]\n",
    "    text = text.lower()\n",
    "    taggedText = text # document's initial text\n",
    "\n",
    "\n",
    "#  a concept tag will be done with a star (*), and the identifier with a +\n",
    "    for concept in ConceptList:\n",
    "\n",
    "        if concept != \"\": # IMPORTANT TO AVOID TAGGING ANYTHING\n",
    "            # REGEX CREATION: creating regex of the concept such that it can be used to search in doc later\n",
    "            regex = RegexFromTerm(concept, stemmer_en)\n",
    "\n",
    "            # TAGGING#\n",
    "            # semantically neutral symbols are chosen to prevent eurovoc concepts from matching tags\n",
    "            if re.search(regex, text) != None:\n",
    "                tagsList.append(concept)\n",
    "                subRegex = r\"[\" + concept + r\"]\"\n",
    "                subRegex += r\"(\" + EurovocReverseDic[concept] + r\") \" # insert the identifier\n",
    "                taggedText = re.sub(regex, subRegex, taggedText)\n",
    "\n",
    "# create a new file with the tagged file\n",
    "    file = open(\"%s_TAGGED.txt\" % DocName, \"w\", encoding='utf8')\n",
    "    file.write(taggedText)\n",
    "    file.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:05:22.167781Z",
     "end_time": "2023-04-24T11:05:30.102090Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import re\n",
    "def get_tokens_with_entities(raw_text: str):\n",
    "    # split the text by spaces only if the space does not occur between square brackets\n",
    "    # we do not want to split \"multi-word\" entity value yet\n",
    "    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n",
    "\n",
    "    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n",
    "    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n",
    "    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M) # used to compile a regular expression pattern provided as a string into a regex pattern object\n",
    "    # flags: re.IGNORECASE and re.MULTILINE\n",
    "\n",
    "    tokens_with_entities = []\n",
    "\n",
    "    for raw_token in raw_tokens:\n",
    "        match = entity_value_pattern_compiled.match(raw_token) # if no match then returns None\n",
    "\n",
    "        if match:\n",
    "            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n",
    "\n",
    "            # we prefix the name of entity differently\n",
    "            # B- indicates beginning of an entity\n",
    "            # I- indicates the token is not a new entity itself but rather a part of existing one\n",
    "            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n",
    "                entity_prefix = \"B\" if i == 0 else \"I\"\n",
    "                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n",
    "                tokens_with_entities.append((raw_entity_token, entity_name))\n",
    "        else:\n",
    "            tokens_with_entities.append((raw_token, \"O\")) # no match\n",
    "\n",
    "    return tokens_with_entities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:41:33.334910Z",
     "end_time": "2023-04-24T14:41:33.354223Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "temp_path = os.getcwd()\n",
    "temp_path = temp_path.replace(\"src\\\\main\", \"data\\\\en\\\\directives_txt\\\\Directive_(EU)_2016_343_en.txt_TAGGED.txt\")\n",
    "\n",
    "\n",
    "with open(\"%s\" % temp_path, \"r\", encoding='utf8') as myfile:\n",
    "    temp_text = myfile.read()\n",
    "\n",
    "temp_text_entities = get_tokens_with_entities(temp_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:42:02.574474Z",
     "end_time": "2023-04-24T14:42:02.589119Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class NERDataMaker:\n",
    "    def __init__(self, texts):\n",
    "        self.unique_entities = []\n",
    "        self.processed_texts = []\n",
    "\n",
    "        temp_processed_texts = []\n",
    "        for text in texts:\n",
    "            tokens_with_entities = get_tokens_with_entities(text)\n",
    "            for _, ent in tokens_with_entities:\n",
    "                if ent not in self.unique_entities:\n",
    "                    self.unique_entities.append(ent)\n",
    "            temp_processed_texts.append(tokens_with_entities)\n",
    "\n",
    "        self.unique_entities.sort(key=lambda ent: ent if ent != \"O\" else \"\")\n",
    "\n",
    "        for tokens_with_entities in temp_processed_texts:\n",
    "            self.processed_texts.append([(t, self.unique_entities.index(ent)) for t, ent in tokens_with_entities])\n",
    "\n",
    "    @property\n",
    "    def id2label(self):\n",
    "        return dict(enumerate(self.unique_entities))\n",
    "\n",
    "    @property\n",
    "    def label2id(self):\n",
    "        return {v:k for k, v in self.id2label.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def _process_tokens_for_one_text(id, tokens_with_encoded_entities):\n",
    "            ner_tags = []\n",
    "            tokens = []\n",
    "            for t, ent in tokens_with_encoded_entities:\n",
    "                ner_tags.append(ent)\n",
    "                tokens.append(t)\n",
    "\n",
    "            return {\n",
    "                \"id\": id,\n",
    "                \"ner_tags\": ner_tags,\n",
    "                \"tokens\": tokens\n",
    "            }\n",
    "\n",
    "        tokens_with_encoded_entities = self.processed_texts[idx]\n",
    "        if isinstance(idx, int):\n",
    "            return _process_tokens_for_one_text(idx, tokens_with_encoded_entities)\n",
    "        else:\n",
    "            return [_process_tokens_for_one_text(i+idx.start, tee) for i, tee in enumerate(tokens_with_encoded_entities)]\n",
    "\n",
    "    def as_hf_dataset(self, tokenizer):\n",
    "        from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
    "        def tokenize_and_align_labels(examples):\n",
    "            tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "            labels = []\n",
    "            for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "                word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "                previous_word_idx = None\n",
    "                label_ids = []\n",
    "                for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                        label_ids.append(label[word_idx])\n",
    "                    else:\n",
    "                        label_ids.append(-100)\n",
    "                    previous_word_idx = word_idx\n",
    "                labels.append(label_ids)\n",
    "\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "            return tokenized_inputs\n",
    "\n",
    "        ids, ner_tags, tokens = [], [], []\n",
    "        for i, pt in enumerate(self.processed_texts):\n",
    "            ids.append(i)\n",
    "            pt_tokens,pt_tags = list(zip(*pt))\n",
    "            ner_tags.append(pt_tags)\n",
    "            tokens.append(pt_tokens)\n",
    "        data = {\n",
    "            \"id\": ids,\n",
    "            \"ner_tags\": ner_tags,\n",
    "            \"tokens\": tokens\n",
    "        }\n",
    "        features = Features({\n",
    "            \"tokens\": Sequence(Value(\"string\")),\n",
    "            \"ner_tags\": Sequence(ClassLabel(names=self.unique_entities)),\n",
    "            \"id\": Value(\"int32\")\n",
    "        })\n",
    "        ds = Dataset.from_dict(data, features)\n",
    "        tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
    "        return tokenized_ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:47:05.911382Z",
     "end_time": "2023-04-24T14:47:05.917446Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "dm = NERDataMaker(temp_text.split(\"\\n\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:48:58.844862Z",
     "end_time": "2023-04-24T14:48:59.170920Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples = 702\n",
      "[{'id': 0, 'ner_tags': [0], 'tokens': ['11.3.2016']}, {'id': 1, 'ner_tags': [0], 'tokens': ['']}, {'id': 2, 'ner_tags': [0], 'tokens': ['en']}, {'id': 3, 'ner_tags': [0], 'tokens': ['']}, {'id': 4, 'ner_tags': [14, 109, 0, 0, 59, 0], 'tokens': ['Official', 'Journal', 'of', 'the', 'EURES', 'union']}, {'id': 5, 'ner_tags': [0], 'tokens': ['']}, {'id': 6, 'ner_tags': [0, 0], 'tokens': ['l', '65/1']}, {'id': 7, 'ner_tags': [0], 'tokens': ['']}, {'id': 8, 'ner_tags': [0], 'tokens': ['i']}, {'id': 9, 'ner_tags': [0, 0], 'tokens': ['([legislation](1589)', 'acts)']}, {'id': 10, 'ner_tags': [0], 'tokens': ['']}, {'id': 11, 'ner_tags': [67, 67, 0, 0, 0, 0, 59, 32, 0, 0, 0, 0], 'tokens': ['directive', 'directive', '(eu)', '2016/343', 'of', 'the', 'EURES', 'parliament', 'and', 'of', 'the', 'council']}, {'id': 12, 'ner_tags': [0, 0, 22, 0], 'tokens': ['of', '9', 'Marches', '2016']}, {'id': 13, 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['on', 'the', 'strengthening', 'of', 'certain', 'aspects', 'of', 'the', 'presumption', 'of', 'innocence', 'and', 'of', 'the', 'right', 'to', 'be']}, {'id': 14, 'ner_tags': [0, 0, 0, 0, 0, 84, 118, 0, 59, 32, 0, 0, 4, 108, 108, 0], 'tokens': ['present', 'at', 'the', 'trial', 'in', 'criminal', 'proceedings', 'the', 'EURES', 'parliament', 'and', 'the', 'Council', 'of', 'Europe', 'union,']}, {'id': 15, 'ner_tags': [0], 'tokens': ['']}, {'id': 16, 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0, 0, 0, 0, 0, 0, 0, 39, 0], 'tokens': ['having', 'regard', 'to', 'the', 'treaty', 'on', 'the', 'functioning', 'of', 'the', 'EURES', 'union,', 'and', 'in', 'particular', 'point', '(b)', 'of', 'arts', '82(2)']}, {'id': 17, 'ner_tags': [0], 'tokens': ['thereof,']}, {'id': 18, 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 59, 0], 'tokens': ['having', 'regard', 'to', 'the', 'proposal', 'from', 'the', 'EURES', 'commission,']}, {'id': 19, 'ner_tags': [0, 0, 0, 0, 0, 20, 0, 0, 0, 28, 0, 0, 32, 0], 'tokens': ['after', 'transmission', 'of', 'the', 'draft', 'legislation', 'act', 'to', 'the', '[[nationality', '(2033)', '(3521)', 'parliament', '']}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"total examples = {len(dm)}\")\n",
    "print(dm[0:20])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:49:25.444445Z",
     "end_time": "2023-04-24T14:49:25.467691Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbd650168ecb45d493ca4b2a3401446f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(dm.unique_entities), id2label=dm.id2label, label2id=dm.label2id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:50:53.272976Z",
     "end_time": "2023-04-24T14:52:28.559711Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/702 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "032e0117227e43b7ac4086604ef7230f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnaen\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1760 : < :, Epoch 0.02/40]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=40,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "train_ds = dm.as_hf_dataset(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=train_ds, # eval on training set! ONLY for DEMO!!\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tempt\n",
    "import PyPDF2\n",
    "\n",
    "#create file object variable\n",
    "#opening method will be rb\n",
    "pdffileobj=open('1.pdf','rb')\n",
    "\n",
    "#create reader variable that will read the pdffileobj\n",
    "pdfreader=PyPDF2.PdfFileReader(pdffileobj)\n",
    "\n",
    "#This will store the number of pages of this pdf file\n",
    "x=pdfreader.numPages\n",
    "\n",
    "#create a variable that will select the selected number of pages\n",
    "pageobj=pdfreader.getPage(x+1)\n",
    "\n",
    "#(x+1) because python indentation starts with 0.\n",
    "#create text variable which will store all text datafrom pdf file\n",
    "text=pageobj.extractText()\n",
    "\n",
    "#save the extracted data from pdf to a txt file\n",
    "#we will use file handling here\n",
    "#dont forget to put r before you put the file path\n",
    "#go to the file location copy the path by right clicking on the file\n",
    "#click properties and copy the location path and paste it here.\n",
    "#put \"\\\\your_txtfilename\"\n",
    "file1=open(r\"C:\\Users\\SIDDHI\\AppData\\Local\\Programs\\Python\\Python38\\\\1.txt\",\"a\")\n",
    "file1.writelines(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
