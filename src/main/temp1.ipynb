{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Dependencies & Loading dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from spacypdfreader import pdf_reader\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # A small English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, syntax and entities."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Importing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "# automatically extracting path\n",
    "temp_path = os.getcwd()\n",
    "temp_path = temp_path.replace(\"src\\\\main\", \"data\\\\en\\\\Directive_(EU)_2016_343_en.pdf\")\n",
    "\n",
    "# importing the file, here doc is like a \"list\" of tokens (each tok is either a word, number, ...)\n",
    "doc = pdf_reader(temp_path, nlp)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentence Segmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3.2016 ... compound\n",
      "\n",
      "\n",
      " ... dep\n",
      "EN ... compound\n",
      "    \n",
      "\n",
      " ... dep\n",
      "Official ... nmod\n",
      "  ... dep\n",
      "Journal ... conj\n",
      "of ... prep\n",
      "  ... dep\n",
      "the ... det\n",
      "European ... compound\n",
      "Union ... compound\n",
      "\n",
      "\n",
      " ... dep\n",
      "L ... appos\n",
      "65/1 ... nummod\n",
      "\n",
      "\n",
      " ... dep\n",
      "I ... compound\n",
      "\n",
      "\n",
      " ... dep\n",
      "( ... punct\n",
      "Legislative ... compound\n"
     ]
    }
   ],
   "source": [
    "# an example\n",
    "for tok in doc[0:20]:\n",
    "  print(tok.text, \"...\", tok.dep_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entities (Nodes) Extraction\n",
    "Here we will extract these elements in an unsupervised manner, i.e., we will use the grammar of the sentences. The extraction of a single word entity from a sentence is not a tough task. We can easily do this with the help of parts of speech (POS) tags. However, when an entity spans across multiple words, then POS tags alone are not sufficient. To fix this we basically save our previous text's info."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "def get_entities(the_file):\n",
    "  # init\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag (the relationship between any two words is marked by a dependency tag) of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  # for holding the text that is associated with the current subject/object (can be multiple words)\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  # going through each token\n",
    "  for tok in nlp(the_file):\n",
    "    if tok.dep_ != \"punct\": # if punctuation mark skip\n",
    "\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        if prv_tok_dep == \"compound\": # if the previous word was also a 'compound' then add to current text\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "      # check if modifier (a modifier gives information about another word in the same sentence e.g., blue house)\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        if prv_tok_dep == \"compound\": # if previous word was also a 'compound' then add to current text\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "      # extract first entity - subject\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        # reset info\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"\n",
    "\n",
    "      # extract second entity - object\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        # reset info\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"\n",
    "\n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "# saving each sentence of the document as a list to be used later\n",
    "sents = list(doc.sents) # extract sentences of the document (only checks for \".\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence: \n",
      "\n",
      "(5)  \n",
      "\n",
      "Although  the  Member  States  are  party  to  the  ECHR  and  to  the  ICCPR,  experience  has  shown  that  this  in  itself \n",
      "does not always  provide  a sufficient degree  of  trust in  the criminal  justice  systems of other  Member States.\n",
      "\n",
      "The subject and object of the sentence: \n",
      "['ICCPR  that', 'other Member States']\n"
     ]
    }
   ],
   "source": [
    "# this cell is for testing \"get_entities\" function\n",
    "temp = sents[11]\n",
    "\n",
    "print(\"The sentence: \" + str(temp) + \"\\n\")\n",
    "print(\"The subject and object of the sentence: \\n\" + str(get_entities(str(temp))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extracting the entity pairs of all the sentences in our data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [00:02<00:00, 77.74it/s]\n"
     ]
    }
   ],
   "source": [
    "entity_pairs = []\n",
    "\n",
    "for i in tqdm(sents): # here \"tqdm\" is just used for creating a progress bar\n",
    "  entity_pairs.append(get_entities(str(i)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "[['which', 'principle'],\n ['ICCPR  that', 'other Member States'],\n ['', 'criminal  proceedings'],\n ['who', 'vulnerable  measure'],\n ['European Roadmap it', 'citizens'],\n ['exhaustive  character', 'better  cooperation'],\n ['', 'Council'],\n ['purpose', 'trial'],\n ['', 'criminal  matters'],\n ['', 'Member States']]"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the entity pairs that are extracted\n",
    "entity_pairs[10:20]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Relations (Edges) Extraction\n",
    "Here we assume that the predicate is actually the main verb in a sentence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "def get_relation(the_sentence):\n",
    "  temp_doc = nlp(the_sentence)\n",
    "\n",
    "  # creating the rule-based Matcher object\n",
    "  matcher = Matcher(nlp.vocab)\n",
    "\n",
    "  # defining the pattern - Each pattern should be a list of dicts and each pattern should be saved in another list\n",
    "  # ex: patterns = [[{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}], [{\"ORTH\": \"Google\"}, {\"ORTH\": \"Maps\"}]]\n",
    "\n",
    "  # This pattern tries to find the ROOT word in the sentence. Once the ROOT is identified, then the pattern checks whether it is followed by a preposition (‘prep’) or an agent word. If yes, then it is added to the ROOT word.\n",
    "  pattern = [{'DEP':'ROOT'}, # check for token with dependency label root\n",
    "            {'DEP':'prep','OP':\"?\"}, # other stuff\n",
    "            {'DEP':'agent','OP':\"?\"},\n",
    "            {'POS':'ADJ','OP':\"?\"}]\n",
    "\n",
    "  # matcher.add(\"match_id\", \"patterns\")\n",
    "  matcher.add(\"matching_1\", [pattern])\n",
    "\n",
    "  matches = matcher(temp_doc)\n",
    "\n",
    "  k = len(matches) - 1\n",
    "  if k == -1: # meaning no match was found so return null\n",
    "    return None\n",
    "\n",
    "  span = temp_doc[matches[k][1]:matches[k][2]]\n",
    "  return span.text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "'completed'"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "get_relation(\"John completed the task\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [00:02<00:00, 75.41it/s]\n"
     ]
    }
   ],
   "source": [
    "relations = [get_relation(str(i)) for i in tqdm(sents)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "accused       12\nremain         5\nensure         4\nare            4\nis             4\nunderstood     4\napply          3\nprovided       3\ntake           3\nviolated       2\ndtype: int64"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize\n",
    "pd.Series(relations).value_counts()[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Knowledge Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "# creating a dataframe of entities and predicates:\n",
    "\n",
    "# extract subject\n",
    "source = [i[0] for i in entity_pairs]\n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in entity_pairs]\n",
    "\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the networkx library to create a network from this dataframe. Which is going to be a directed graph allowing us to draw a line from subject to object. However, we also need to use pyvis library because networkx's visualize method is currently not working."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "# create a directed-graph from a dataframe\n",
    "directed_graph = nx.from_pandas_edgelist(kg_df, \"source\", \"target\", edge_attr=True, create_using=nx.MultiDiGraph())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example.html\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x1ffe3f4b6a0>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"600px\"\n            src=\"example.html\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, cdn_resources='remote')\n",
    "\n",
    "net.from_nx(directed_graph)\n",
    "net.show(\"example.html\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Knowledge Graph Database and Query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
