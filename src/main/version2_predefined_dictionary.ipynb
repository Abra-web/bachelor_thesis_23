{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:12:34.456623800Z",
     "start_time": "2023-06-18T16:12:24.486065300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dnaen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import xml.etree.ElementTree as Xet # for parsing and creating XML data\n",
    "import pandas as pd\n",
    "import os, csv, re, nltk\n",
    "from flair.data import Corpus # in order to use the functions tha flair has\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings # these embeddings helps NER to perform better\n",
    "from itertools import islice\n",
    "from nltk.stem import WordNetLemmatizer # previously need to download \"nltk.download('wordnet')\" and \"nltk.download('omw-1.4')\". But beware if new version comes out\n",
    "from tqdm import tqdm # to display loop in a bar\n",
    "from openie import StanfordOpenIE # for using our OIE tool\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from googletrans import Translator # don't forget to run \"!pip install googletrans==3.1.0a0\" before using this\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import time\n",
    "import jaro # requires !pip install jaro-winkler\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# run this cell if you aim to use semantic distance\n",
    "# from gensim.models import Word2Vec\n",
    "# import gensim.downloader as model\n",
    "# corpus_of_word2vec = model.load(\"word2vec-google-news-300\") # have to run it once to download to pc, in later runs it just takes the downloaded file from pc (so takes much shorter after second and future times)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T07:59:30.946184Z",
     "start_time": "2023-06-09T07:59:30.935698500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NER (Named-Entity Recognition)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# functions\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# English: en, German: de, French: fr, ... -> creates the tsv of given descriptor of any language\n",
    "def create_tsv_of_language(given_language):\n",
    "    \"\"\"\n",
    "    Before running this function below, the \"desc_\"\".xml\" file (that is downloaded from EuroVoc website) needs to be downloaded and added to package \"data/\"\"/descriptors/...\"\n",
    "    PS: Even after this function finishes it takes some time for the new file to appear\n",
    "    \"\"\"\n",
    "    cols = ['ID', given_language.upper()] # will be saving in a tsv with ids and their corresponding terms\n",
    "    rows = []\n",
    "\n",
    "    # parsing the xml file -> with the given EuroVoc descriptors\n",
    "    temp_path = os.getcwd()\n",
    "    temp_path = temp_path.replace(\"src\\\\main\", \"data\\\\\" + given_language + \"\\\\descriptors\\\\desc_\" + given_language + \".xml\")\n",
    "    xml_parse = Xet.parse(temp_path)\n",
    "    root = xml_parse.getroot()\n",
    "\n",
    "    # iterate through the elements of xml file\n",
    "    for element in root:\n",
    "        rows.append({\"ID\": element.find(\"DESCRIPTEUR_ID\").text, given_language.upper(): element.find(\"LIBELLE\").text})\n",
    "\n",
    "    # creating the tsv file\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    df.to_csv(\"eurovoc_\" + given_language + \".tsv\", sep='\\t', index=False) # using sep='\\t' gives us a tsv file instead of csv\n",
    "\n",
    "def create_tsv_of_any_given_concept(given_concept_dict, given_language):\n",
    "    \"\"\"\n",
    "    :param given_concept_dict: A dictionary in style of .e.g, {EN:..., ID:...}\n",
    "    :param given_language: en: english, fr: fran√ßais, de: deutsch\n",
    "    :return: creates a new tsv called \"updated_eurovoc_en.tsv\"\n",
    "    \"\"\"\n",
    "    cols = ['ID', given_language.upper()] # will be saving in a tsv with ids and their corresponding terms\n",
    "    rows = []\n",
    "\n",
    "    # iterate through the elements of xml file\n",
    "    for key, value in given_concept_dict.items():\n",
    "        rows.append({\"ID\": value, given_language.upper(): key})\n",
    "\n",
    "    # creating the tsv file\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    df.to_csv(\"updated_eurovoc_\" + given_language + \".tsv\", sep='\\t', index=False) # using sep='\\t' gives us a tsv file instead of csv\n",
    "\n",
    "# this function assumes we get the text annotated as [entity_value](entity_name), and assigns prefixes B, I, and 0 to each token\n",
    "def get_tokens_with_entities(raw_text: str):\n",
    "    # split the text by spaces (but not splitting the space inside the square brackets (so not splitting the \"multi-word\" entity value yet))\n",
    "    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n",
    "\n",
    "    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n",
    "    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n",
    "\n",
    "    # flags: re.IGNORECASE and re.MULTILINE\n",
    "    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M) # using it to compile a regular expression pattern provided as a string into a regex pattern object\n",
    "\n",
    "    tokens_with_entities = []\n",
    "\n",
    "    for raw_token in raw_tokens:\n",
    "        match = entity_value_pattern_compiled.match(raw_token) # if no match then returns None\n",
    "\n",
    "        if match:\n",
    "            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n",
    "\n",
    "            # we prefix the name of entity differently\n",
    "            # B- indicates beginning of an entity\n",
    "            # I- indicates the token is not a new entity itself but rather a part of existing one\n",
    "            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n",
    "                entity_prefix = \"B\" if i == 0 else \"I\"\n",
    "                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n",
    "                tokens_with_entities.append((raw_entity_token, entity_name))\n",
    "        else:\n",
    "            tokens_with_entities.append((raw_token, \"O\")) # no match\n",
    "\n",
    "    return tokens_with_entities\n",
    "\n",
    "# NLTK VERSION\n",
    "# TODO add automatic noun-verb-... identifier to aid lemmatization\n",
    "def regex_from_term_nltk(term, lemmatizer):\n",
    "    regex = r\"\\b(\" # Regex Opening\n",
    "    tokensList = nltk.word_tokenize(term)\n",
    "\n",
    "    # Adding terms to regex\n",
    "    if len(tokensList) == 1: # in case of one-word term\n",
    "        for token in tokensList:\n",
    "            regex += token_cleaning(token, lemmatizer)\n",
    "\n",
    "    else: # if it is a multi-word term\n",
    "        decount = len(tokensList)\n",
    "        for token in tokensList:\n",
    "            decount = decount-1\n",
    "            # add between-words\n",
    "            if decount != len(tokensList)-1:\n",
    "                regex+= r'\\w*\\W\\w*\\W*'\n",
    "            # add token\n",
    "            regex += token_cleaning(token, lemmatizer)\n",
    "\n",
    "    regex += '''\\w{0,5})(\\W)''' # Regex Closure\n",
    "    return regex\n",
    "\n",
    "def token_cleaning(token, lemmatizer):\n",
    "    token = token.lower()\n",
    "    token = lemmatizer.lemmatize(token)\n",
    "    return token\n",
    "\n",
    "# Inspired from @https://github.com/shashankmc/eurovoc_entity_link/blob/master/EurovocTagger.py\n",
    "def tsv_dic_processing(path):\n",
    "    \"\"\"\n",
    "    :param path: the name of the eurovoc.tsv file\n",
    "    :return: Dic: Dictionary in style of {ID: Word}\n",
    "    :return: RevDic: Dictionary in style of {Word: ID}\n",
    "    :return: list1: list of IDs\n",
    "    :return: list2: list of words (concepts)\n",
    "    \"\"\"\n",
    "    # Dic, RevDic, list1, list2\n",
    "    # Only works with a 2-columns ([ID], [EN]) TSV file\n",
    "    Dic = {}\n",
    "    RevDic = {}\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    with open(path, 'rt', encoding='utf8') as csvfile:\n",
    "        myreader = csv.reader(csvfile, delimiter='\\t')\n",
    "        rcount = 0\n",
    "        for row in myreader:\n",
    "            rcount += 1\n",
    "            ccount = 0\n",
    "            if rcount > 1:\n",
    "                for cells in row:\n",
    "                    ccount += 1\n",
    "                    if ccount ==1:\n",
    "                        list1.append(cells)\n",
    "                        key = cells\n",
    "                    else:\n",
    "                        list2.append(cells)\n",
    "                        value = cells\n",
    "                Dic[key] = value\n",
    "                RevDic[value] = key\n",
    "    return Dic, RevDic, list1, list2\n",
    "\n",
    "def find_folder_with_type(given_path, doc_type): # returns all documents found in path\n",
    "    doc_list = []\n",
    "    for doc in os.listdir(given_path):\n",
    "        if re.search (r'.*\\%s$' % doc_type, doc) is not None: # even though this shows as error in IDE it's fine\n",
    "            doc_list.append(doc)\n",
    "    return doc_list\n",
    "\n",
    "\n",
    "def folder_list_to_dic(given_path, given_list): # given file names extracts their texts and saves in a dic\n",
    "    dic = {}\n",
    "    old_path = os.getcwd() # saving the previous working dir so we can switch back to that dir later\n",
    "    os.chdir(given_path)\n",
    "\n",
    "    # the input should be a list of file contained in a folder\n",
    "    for file_name in given_list:\n",
    "        print('importing', file_name, '...')\n",
    "        with open(\"%s\" % file_name, \"r\", encoding='utf8') as my_file:\n",
    "            text = my_file.read()\n",
    "        dic[file_name]= text\n",
    "\n",
    "    os.chdir(old_path)\n",
    "    return dic\n",
    "\n",
    "# tagging by researching concept-regexed as a substring of the text (by using NLTK)\n",
    "def tagging_document(path_of_tagged, given_doc_list, given_doc_dic, given_concept_list, given_eurovoc_reverse_dic):\n",
    "    \"\"\"\n",
    "    This function takes the information of the descriptor (e.g., {id:concept}, id list, concept list, ...) and then with the given document information it creates the new tagged document in tagged folder. Additionally, it returns the new updated concept list which contains additional \"concepts\" found in the document text that seems to be related to one of the original concepts. Thus, expanding the vocabulary we have.\n",
    "\n",
    "    :param path_of_tagged: the location (dir) of the tagged folder\n",
    "    :param given_doc_list: a list of names of the documents\n",
    "    :param given_doc_dic: a dic that contains the contents of the document i.e. {doc_name: doc_text}\n",
    "    :param given_concept_list: the original concept list downloaded from Eurovoc\n",
    "    :param given_eurovoc_reverse_dic: opposite of \"given_concept_list\" so {concept: id}\n",
    "    :return: new_concept: this is the new expanded concept list\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    old_path = os.getcwd() # saving the previous working dir so we can switch back to that dir later\n",
    "    os.chdir(path_of_tagged)\n",
    "    new_concept_dic = given_eurovoc_reverse_dic.copy() # using the reverse eurovoc dict instead because can't add words with same id\n",
    "\n",
    "    for doc_name in given_doc_list:\n",
    "        tags_list=[]\n",
    "        tagged_text = \"\"\n",
    "        print('tagging', doc_name,'...')\n",
    "        text = given_doc_dic[doc_name]\n",
    "        text = text.lower()\n",
    "        tagged_text = text # document's initial text\n",
    "\n",
    "        # a concept tag will be done with a star (*), and the identifier with a +\n",
    "        for concept in given_concept_list:\n",
    "\n",
    "            if concept != \"\": # if concept empty, will tag everything (so need to make sure that it's not empty)\n",
    "                # REGEX CREATION: creating regex of the concept such that it can be used to search in doc later\n",
    "                regex = regex_from_term_nltk(concept, lemmatizer)\n",
    "\n",
    "                # concept = concept.strip()\n",
    "                # TAGGING #\n",
    "                # semantically neutral symbols are chosen to prevent eurovoc concepts from matching tags\n",
    "                if re.search(regex, text) is not None:\n",
    "                    # these prints can be used to check performance\n",
    "                    # print(\"Match made!\")\n",
    "                    # print(\"Found: \" + re.search(regex, text).group() + \", for concept: \" + concept)\n",
    "                    match_in_text = re.search(regex, text).group()\n",
    "                    if match_in_text not in given_concept_list:\n",
    "                        # cleaning up the matched text\n",
    "                        match_in_text = match_in_text.replace(\"\\n\", \"\")\n",
    "                        # match_in_text = match_in_text.replace(\"[V4.3]\", \"\")\n",
    "                        match_in_text = match_in_text.strip()\n",
    "                        match_in_text = match_in_text.strip(\".,-\")\n",
    "                        new_concept_dic[match_in_text] = given_eurovoc_reverse_dic[concept]\n",
    "\n",
    "                    tags_list.append(concept)\n",
    "                    sub_regex = r\"[\" + concept + r\"]\"\n",
    "                    sub_regex += r\"(\" + given_eurovoc_reverse_dic[concept] + r\") \" # insert the identifier\n",
    "                    tagged_text = re.sub(regex, sub_regex, tagged_text)\n",
    "\n",
    "    # create a new file with the tagged file\n",
    "        file = open(\"%s_TAGGED.txt\" % doc_name, \"w\", encoding='utf8')\n",
    "        file.write(tagged_text)\n",
    "        file.close()\n",
    "\n",
    "    os.chdir(old_path) # change back to previous path\n",
    "\n",
    "    return new_concept_dic\n",
    "\n",
    "# updating the concept list by going through the given documents (by using NLTK)\n",
    "def update_concept_list(given_doc_list, given_doc_dic, given_concept_list, given_eurovoc_reverse_dic):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_concept_dic = given_eurovoc_reverse_dic.copy() # using the reverse eurovoc dict instead because can't add words with same id\n",
    "\n",
    "    for doc_name in given_doc_list:\n",
    "        print('Going through ', doc_name,'...')\n",
    "        text = given_doc_dic[doc_name]\n",
    "        text = text.lower()\n",
    "\n",
    "        # a concept tag will be done with a star (*), and the identifier with a +\n",
    "        for concept in given_concept_list:\n",
    "\n",
    "            if concept != \"\": # if concept empty, will tag everything (so need to make sure that it's not empty)\n",
    "                # REGEX CREATION: creating regex of the concept such that it can be used to search in doc later\n",
    "                regex = regex_from_term_nltk(concept, lemmatizer)\n",
    "\n",
    "                # concept = concept.strip()\n",
    "                # TAGGING #\n",
    "                # semantically neutral symbols are chosen to prevent eurovoc concepts from matching tags\n",
    "                if re.search(regex, text) is not None:\n",
    "                    # these prints can be used to check performance\n",
    "                    # print(\"Match made!\")\n",
    "                    # print(\"Found: \" + re.search(regex, text).group() + \", for concept: \" + concept)\n",
    "                    match_in_text = re.search(regex, text).group()\n",
    "                    if match_in_text not in given_concept_list:\n",
    "                        # cleaning up the matched text\n",
    "                        match_in_text = match_in_text.replace(\"\\n\", \"\")\n",
    "                        # match_in_text = match_in_text.replace(\"[V4.3]\", \"\")\n",
    "                        match_in_text = match_in_text.strip()\n",
    "                        match_in_text = match_in_text.strip(\".,-\")\n",
    "                        new_concept_dic[match_in_text] = given_eurovoc_reverse_dic[concept]\n",
    "\n",
    "                    # tags_list.append(concept)\n",
    "\n",
    "    return new_concept_dic\n",
    "\n",
    "def find_corresponding_eurovoc_concept_basic_overlap(given_text, given_concept_list, given_stopwords_list):\n",
    "    \"\"\"\n",
    "    The goal of this function is to find the concepts the given text might be linked to.\n",
    "    :param given_text: takes in a string (this string can be a single word or a compound word (list of words)\n",
    "    :param given_concept_list: the original concept list downloaded from Eurovoc (MAKE SURE THIS IS A LIST!)\n",
    "    :param given_stopwords_list: a list words we don't want to look into (in our case it's stopwords)\n",
    "    :return: a list of concepts it might be related to\n",
    "    \"\"\"\n",
    "    elems_concept = []\n",
    "\n",
    "    # such that we can also check on possible compounds by bruteforce\n",
    "    possible_words_list = get_neighbouring_groups(given_text) # ex: \"I need to\" -> [\"I\", \"need\", \"to, \"I need\", \"need to\", \"I need to\"]\n",
    "\n",
    "    # TODO change the type of string measure you do\n",
    "    for word in possible_words_list:\n",
    "        word = word.lower()\n",
    "        for concept in given_concept_list:\n",
    "            concept = unidecode(concept)\n",
    "            contains = False\n",
    "            if len(word.split()) == len(concept.split()): # checking whether there is same number of words\n",
    "                word_list = word.split()\n",
    "                concept_list_1 = concept.split()\n",
    "\n",
    "                contains = True # set it to false when a mismatch occurs\n",
    "                for i in range(len(word_list)): # this was done such that we can go through compounds too (and applyiing specific operations (e.g., lemmatization) to each word)\n",
    "                    # TODO add lemmatization?\n",
    "                    current_word = word_list[i]\n",
    "                    current_concept = concept_list_1[i]\n",
    "\n",
    "                    if current_word not in given_stopwords_list:\n",
    "                        concept_uni = unidecode(current_concept) # because the triples of other languages will be in unidecode\n",
    "                        if current_word not in concept_uni:\n",
    "                            contains = False\n",
    "                    else: contains = False\n",
    "\n",
    "            if contains: elems_concept.append(unidecode(concept))\n",
    "\n",
    "    return elems_concept\n",
    "\n",
    "def find_corresponding_eurovoc_concept_jaro_winkler(given_text, given_concept_list, given_stopwords_list):\n",
    "    \"\"\"\n",
    "    The goal of this function is to find the concepts the given text might be linked to.\n",
    "    :param given_text: takes in a string (this string can be a single word or a compound word (list of words)\n",
    "    :param given_concept_list: the original concept list downloaded from Eurovoc (MAKE SURE THIS IS A LIST!)\n",
    "    :param given_stopwords_list: a list words we don't want to look into (in our case it's stopwords)\n",
    "    :return: a list of concepts it might be related to\n",
    "    \"\"\"\n",
    "    elems_concept = []\n",
    "\n",
    "    # such that we can also check on possible compounds by bruteforce\n",
    "    possible_words_list = get_neighbouring_groups(given_text) # ex: \"I need to\" -> [\"I\", \"need\", \"to, \"I need\", \"need to\", \"I need to\"]\n",
    "\n",
    "    for word in possible_words_list:\n",
    "        word = word.lower()\n",
    "        if word not in given_stopwords_list:\n",
    "            for concept in given_concept_list:\n",
    "                concept = unidecode(concept)\n",
    "                if len(word.split()) == len(concept.split()): # checking whether there is same number of words\n",
    "                    if jaro.jaro_winkler_metric(word, concept) > 0.95: elems_concept.append(unidecode(concept))\n",
    "\n",
    "    return elems_concept\n",
    "\n",
    "def find_corresponding_eurovoc_concept_jaro_winkler_with_english_semantic(given_text, given_concept_list, given_stopwords_list):\n",
    "    \"\"\"\n",
    "    The goal of this function is to find the concepts the given text might be linked to.\n",
    "    :param given_text: takes in a string (this string can be a single word or a compound word (list of words)\n",
    "    :param given_concept_list: the original concept list downloaded from Eurovoc (MAKE SURE THIS IS A LIST!)\n",
    "    :param given_stopwords_list: a list words we don't want to look into (in our case it's stopwords)\n",
    "    :return: a list of concepts it might be related to\n",
    "    \"\"\"\n",
    "    elems_concept = []\n",
    "\n",
    "    # such that we can also check on possible compounds by bruteforce\n",
    "    possible_words_list = get_neighbouring_groups(given_text) # ex: \"I need to\" -> [\"I\", \"need\", \"to, \"I need\", \"need to\", \"I need to\"]\n",
    "\n",
    "    for word in possible_words_list:\n",
    "        word = word.lower()\n",
    "        if word not in given_stopwords_list:\n",
    "            for concept in given_concept_list:\n",
    "                concept = unidecode(concept)\n",
    "                if len(word.split()) == len(concept.split()): # checking whether there is same number of words\n",
    "                    # calculation of semantic distance and string distance\n",
    "                    try: # if word/concept not exists in the corpus it throws KeyError, so we would use only string distance in this case\n",
    "                        semantic_dist = 1 - spatial.distance.cosine(get_vector(word), get_vector(concept))\n",
    "                        string_dist = jaro.jaro_winkler_metric(word, concept)\n",
    "\n",
    "                        if semantic_dist > 0.7: elems_concept.append(unidecode(concept)) # higher priority to semantic dist\n",
    "                        elif string_dist > 0.8: elems_concept.append(unidecode(concept))\n",
    "                    except KeyError:\n",
    "                        string_dist = jaro.jaro_winkler_metric(word, concept)\n",
    "                        if string_dist > 0.8: elems_concept.append(unidecode(concept))\n",
    "\n",
    "    return elems_concept\n",
    "\n",
    "def preprocess(s):\n",
    "    return [i.lower() for i in s.split()]\n",
    "\n",
    "def get_vector(s):\n",
    "    return np.sum(np.array([model[i] for i in preprocess(s)]), axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T23:09:51.678268100Z",
     "start_time": "2023-06-11T23:09:51.640371100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Relations Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_triples_stanford_openie(given_client, given_sentence):\n",
    "    triples_list = []\n",
    "\n",
    "    # returns dict in this style: {'subject': 'Obama', 'relation': 'was born in', 'object': 'Hawaii'}\n",
    "    # for triple in tqdm(given_client.annotate(given_sentence)): # this can be used for debugging\n",
    "    for triple in given_client.annotate(given_sentence):\n",
    "        triples_list.append([triple.get(\"subject\"), triple.get(\"relation\"), triple.get(\"object\")])\n",
    "\n",
    "    return triples_list\n",
    "\n",
    "def split_text_into_sentence(given_doc_names, given_docs):\n",
    "    given_docs_tokenized = {}\n",
    "    for doc_name in given_doc_names:\n",
    "        text_of_doc = given_docs[doc_name]\n",
    "        text_of_doc = text_of_doc.replace('\\n',' ') # removing the newline string from text\n",
    "        text_of_doc = text_of_doc.replace('\\xad ','') # removing the hyphen used for line breaking\n",
    "\n",
    "        text_of_doc_tokenized = sent_tokenize(text_of_doc)\n",
    "        given_docs_tokenized[doc_name] = text_of_doc_tokenized\n",
    "\n",
    "    return given_docs_tokenized\n",
    "\n",
    "# this function is modified from \"version1_simple\" file\n",
    "def create_kg_csv(subjects, predicates, objects, re_type, language, given_id_list, model_used = \"\"):\n",
    "    \"\"\"\n",
    "    [source(subject) --relation(predicate)--> target(object)]\n",
    "    :param subjects: source\n",
    "    :param predicates: relation\n",
    "    :param objects: target\n",
    "    :param re_type: currently, we have only relation extraction type of \"simple\" and \"predefined-dictionary\"\n",
    "    :param model_used: currently only \"stanford_OpenIE\"\n",
    "    :param language: en, fr, de\n",
    "    :param given_id_list: represents the id of triple, in this case it's from which directive/legislation it came from\n",
    "    :return: returns nothing only creates the csv file\n",
    "    \"\"\"\n",
    "    # field names\n",
    "    fields = [language + '_Subject', language + '_Predicate', language + '_Object', \"Triple_ID\"]\n",
    "\n",
    "    if model_used:\n",
    "        filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used + \"\\\\kg_of_\" + re_type + \"_\" + language + \".csv\"\n",
    "    else:\n",
    "        filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\kg_of_\" + re_type + \"_\" + language + \".csv\"\n",
    "\n",
    "\n",
    "\n",
    "    rows = [[subjects[i], predicates[i], objects[i], given_id_list[i]] for i in range(len(subjects))]\n",
    "\n",
    "    # find out empty and None strings, replacing it with \"-\"\n",
    "    for i in range(len(rows)):\n",
    "        for j in range(len(rows[0])): # so len 3\n",
    "            if rows[i][j] == \"\" or rows[i][j] is None: rows[i][j] = \"-\"\n",
    "            rows[i][j] = unidecode(rows[i][j].lower())\n",
    "\n",
    "    # writing to csv file\n",
    "    with open(filename, 'w', newline = '') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(fields) # first writing fields\n",
    "        csv_writer.writerows(rows) # now the remaining record\n",
    "\n",
    "# German: de, French: fr, but can work for any other languages too\n",
    "def simple_translator(target_language, re_type, extra_info = \"\", model_used = \"\"): # assumes that en kg csv is already created\n",
    "    translator = Translator()\n",
    "\n",
    "    if model_used:\n",
    "        filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "        filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "        filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_\" + target_language + extra_info + \".csv\"\n",
    "    else:\n",
    "        filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type\n",
    "        filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "        filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_\" + target_language + extra_info + \".csv\"\n",
    "\n",
    "    with open(filename_english, mode =\"r\") as file_original, open(filename_assigned_language, mode = \"r+\", newline = '') as file_2:\n",
    "        csv_reader = csv.reader(file_original)\n",
    "        csv_reader_file2 = csv.reader(file_2)\n",
    "        csv_writer = csv.writer(file_2)\n",
    "\n",
    "        current_size_of_new_kg = 0\n",
    "        for step in csv_reader_file2:\n",
    "            current_size_of_new_kg += 1\n",
    "        print(\"Starting from row: \" + str(current_size_of_new_kg))\n",
    "\n",
    "        # first line is headers\n",
    "        if current_size_of_new_kg == 0: csv_writer.writerow([target_language + \"_Subject\", target_language + \"_Predicate\", target_language + \"_Object\", \"Triple_ID\"])\n",
    "        current_step = 0\n",
    "        for lines in csv_reader: # each line is a list of 3 elements (source - relation - target)\n",
    "            if current_step > current_size_of_new_kg:\n",
    "                source_en, relation_en, target_en = lines[0], lines[1], lines[2]\n",
    "                # src(source) = english, dest(destination) = language to translate to\n",
    "                translated_source, translated_relation, translated_target = translator.translate(source_en, src = \"en\", dest = target_language), translator.translate(relation_en, src = \"en\", dest = target_language), translator.translate(target_en, src = \"en\", dest = target_language)\n",
    "\n",
    "                temp_row = [unidecode(translated_source.text), unidecode(translated_relation.text), unidecode(translated_target.text), unidecode(lines[3])]\n",
    "                csv_writer.writerow(temp_row)\n",
    "            else:\n",
    "                current_step += 1\n",
    "\n",
    "# German: de, French: fr, but can work for any other languages too\n",
    "def simple_translator_with_concept(target_language, re_type, given_concept_en, given_concept_na, extra_info = \"\", model_used = \"\"): # assumes that en kg csv is already created\n",
    "    \"\"\"\n",
    "    :param target_language: language to translate to, e.g. fr, de, ...\n",
    "    :param re_type: currently, we have only relation extraction type of \"simple\" and \"predefined_dictionary\"\n",
    "    :param model_used: currently we only have \"stanford_OpenIE\" model\n",
    "    :param given_concept_en: concept dataframe (as [\"ID\", \"EN\"]) of english\n",
    "    :param given_concept_na: concept dataframe (as [\"ID\", \"..\"]) of target language\n",
    "    :param extra_info: default is empty, but can be used to pass any information about the \"file\" to be initialized\n",
    "    :return: creates a new translated KG with the use of concepts\n",
    "    \"\"\"\n",
    "    translator = Translator()\n",
    "\n",
    "    if model_used:\n",
    "        filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "        filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "        filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_\" + target_language + extra_info + \".csv\"\n",
    "    else:\n",
    "        filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type\n",
    "        filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "        filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_\" + target_language + extra_info + \".csv\"\n",
    "\n",
    "    list_of_concept_en = list(given_concept_en[\"EN\"])\n",
    "    target_language_capitalized = target_language.upper()\n",
    "    testing = False\n",
    "\n",
    "    with open(filename_english, mode =\"r\") as file_original, open(filename_assigned_language, mode = \"r+\", newline = '') as file_2:\n",
    "        csv_reader = csv.reader(file_original)\n",
    "        csv_writer = csv.writer(file_2)\n",
    "        csv_reader_file2 = csv.reader(file_2)\n",
    "\n",
    "        current_size_of_new_kg = 0\n",
    "        for step in csv_reader_file2:\n",
    "            current_size_of_new_kg += 1\n",
    "        print(\"Starting from row: \" + str(current_size_of_new_kg))\n",
    "\n",
    "        current_step = 0\n",
    "        for lines in csv_reader: # each line is a list of 3 elements (source - relation - target)\n",
    "            if current_step > current_size_of_new_kg:\n",
    "                source_en, relation_en, target_en = lines[0], lines[1], lines[2]\n",
    "\n",
    "                # src(source) = english, dest(destination) = language to translate to\n",
    "                translated_source, translated_relation, translated_target = translator.translate(source_en, src = \"en\", dest = target_language), translator.translate(relation_en, src = \"en\", dest = target_language), translator.translate(target_en, src = \"en\", dest = target_language)\n",
    "\n",
    "                translated_source = translated_source.text\n",
    "                translated_relation = translated_relation.text\n",
    "                translated_target = translated_target.text\n",
    "\n",
    "                # if the text exists in concept list, then replace it with it's corresponding id\n",
    "                if any(s in list_of_concept_en for s in (source_en, relation_en, target_en)):\n",
    "                    if source_en in list_of_concept_en:\n",
    "                        if testing: print(\"Source:\")\n",
    "                        if testing: print(\"English version: \" + source_en)\n",
    "                        if testing: print(\"Pre: \" + translated_source)\n",
    "                        concept_id = int(given_concept_en.loc[given_concept_en[\"EN\"] == source_en][\"ID\"])\n",
    "                        translated_source = given_concept_na.loc[given_concept_na[\"ID\"] == concept_id, target_language_capitalized].item()\n",
    "                        if testing:print(\"Pro: \" + translated_source)\n",
    "\n",
    "                    if relation_en in list_of_concept_en:\n",
    "                        if testing: print(\"Relation:\")\n",
    "                        if testing: print(\"English version: \" + relation_en)\n",
    "                        if testing: print(\"Pre: \" + translated_relation)\n",
    "                        concept_id = int(given_concept_en.loc[given_concept_en[\"EN\"] == relation_en][\"ID\"])\n",
    "                        translated_relation = given_concept_na.loc[given_concept_na[\"ID\"] == concept_id, target_language_capitalized].item()\n",
    "                        if testing: print(\"Pro: \" + translated_relation)\n",
    "\n",
    "                    if target_en in list_of_concept_en:\n",
    "                        if testing: print(\"Target\")\n",
    "                        if testing: print(\"English version: \" + target_en)\n",
    "                        if testing: print(\"Pre: \" + translated_target)\n",
    "                        concept_id = int(given_concept_en.loc[given_concept_en[\"EN\"] == target_en][\"ID\"])\n",
    "                        translated_target = given_concept_na.loc[given_concept_na[\"ID\"] == concept_id, target_language_capitalized].item()\n",
    "                        if testing: print(\"Pro: \" + translated_target)\n",
    "\n",
    "                temp_row = [translated_source, translated_relation, translated_target]\n",
    "\n",
    "                index = 0\n",
    "                for row in temp_row:\n",
    "                    temp_row[index] = row.replace(u'\\u200b', '') # this \"space\" character gives error, but it just adds extra space so just removing it\n",
    "                    index += 1\n",
    "\n",
    "                csv_writer.writerow(temp_row)\n",
    "            else:\n",
    "                current_step += 1\n",
    "\n",
    "def get_translation_through_eurovoc(given_text, given_concept, given_concept_target):\n",
    "    # given concepts need to be a dataframe as [\"ID\", \"..\"]\n",
    "\n",
    "    list_of_concept = list(given_concept[given_concept.keys()[1]])\n",
    "    if given_text in list_of_concept:\n",
    "        itsID = given_concept[\"ID\"].get(list_of_concept.index(given_text))\n",
    "        corresponding_eurovoc = given_concept_target.loc[given_concept_target['ID'] == itsID][given_concept_target.keys()[1]].values[0]\n",
    "        return corresponding_eurovoc\n",
    "\n",
    "def get_unique_words_from_triples(given_list_of_triples):\n",
    "    unique_subjects = []\n",
    "    unique_relations = []\n",
    "    unique_objects = []\n",
    "\n",
    "    for triple in given_list_of_triples:\n",
    "        if triple[0] not in unique_subjects: unique_subjects.append(triple[0])\n",
    "        if triple[1] not in unique_relations: unique_relations.append(triple[1])\n",
    "        if triple[2] not in unique_objects: unique_objects.append(triple[2])\n",
    "\n",
    "    return unique_subjects, unique_relations, unique_objects\n",
    "\n",
    "def get_neighbouring_groups(given_text):\n",
    "    text_split = given_text.split()\n",
    "    group = \"\"\n",
    "    group_list = []\n",
    "\n",
    "    # groups of 2, groups of 3, groups of 4, ...\n",
    "    for i in range(len(text_split)):\n",
    "        for j in range(len(text_split) - i):\n",
    "            # group_list.append([(group := group + text_split[k]) if k == 1 else (group := group + text_split[k] + \" \") for k in range(1, len(text_split) - 1)][-1])\n",
    "            for k in range(j, j+i+1):\n",
    "                if k == j+i: group = group + text_split[k]\n",
    "                else: group = group + text_split[k] + \" \"\n",
    "            group_list.append(group)\n",
    "            group = \"\"\n",
    "    return group_list\n",
    "\n",
    "def create_combined_eurovoc(given_lans):\n",
    "    complete_data = []\n",
    "    fields = ['ID']\n",
    "\n",
    "    # adding id\n",
    "    data = pd.read_csv(\"eurovoc_\" + given_lans[0] + \".tsv\",sep='\\t')\n",
    "    data = data.sort_values(by=[\"ID\"])\n",
    "    complete_data.append(list(data.iloc[:, 0]))\n",
    "\n",
    "    # adding the concepts\n",
    "    for lan in given_lans:\n",
    "        fields.append(lan.upper())\n",
    "        data = pd.read_csv(\"eurovoc_\" + lan + \".tsv\",sep='\\t') # TODO what about updated version?\n",
    "        data = data.sort_values(by=[\"ID\"])\n",
    "        complete_data.append(list(data.iloc[:, 1]))\n",
    "\n",
    "\n",
    "    for i in range(1, len(complete_data)):\n",
    "        complete_data[i] = [unidecode(x.lower()) for x in complete_data[i]]\n",
    "\n",
    "    rows = [[complete_data[0][i], complete_data[1][i], complete_data[2][i], complete_data[3][i]] for i in range(len(complete_data[0]))] # TODO find a way to make this line work for any given number of columns\n",
    "\n",
    "    # writing to csv file\n",
    "    filename = os.getcwd() + \"\\\\combined_eurovoc.csv\"\n",
    "    with open(filename, 'w', newline = '') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(fields) # first writing fields\n",
    "        csv_writer.writerows(rows) # now the remaining record\n",
    "\n",
    "def update_triple_csv_file_with_concepts(re_type, language, given_concept_list, model_used = \"\", extra_info = \"\"):\n",
    "    old_path = os.getcwd()\n",
    "    if model_used:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "    else:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type\n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    if extra_info: extra_info = \"_\" + extra_info\n",
    "    data = pd.read_csv(\"kg_of_\" + re_type + \"_\" + language + extra_info + \".csv\")\n",
    "\n",
    "    col_list = list(data.columns)\n",
    "    col_list.remove(\"Triple_ID\")\n",
    "    stopwords_list = []\n",
    "    if language == \"en\": stopwords_list = stopwords.words('english')\n",
    "    if language == \"fr\": stopwords_list = stopwords.words('french')\n",
    "    if language == \"de\": stopwords_list = stopwords.words('german')\n",
    "\n",
    "    for col in col_list:\n",
    "        print(\"Currently at column: \" + col)\n",
    "        new_col = col + \"_concept\"\n",
    "        temp_col = []\n",
    "        old_col = data[col].copy()\n",
    "\n",
    "        current_index = 0\n",
    "        for cell in old_col:\n",
    "            if current_index % 1000 == 0: print(\"Row\" + str(current_index), end = \", \")\n",
    "            if not (cell == \"-\"): temp_col.append(find_corresponding_eurovoc_concept_jaro_winkler(cell, given_concept_list, stopwords_list))\n",
    "            else: temp_col.append(\"[]\")\n",
    "            current_index += 1\n",
    "\n",
    "        data[new_col] = temp_col\n",
    "\n",
    "    # writing to csv file\n",
    "    file_name = path + \"\\\\kg_of_\" + re_type + \"_with_concepts_\" + language + extra_info + \".csv\"\n",
    "    data.to_csv(file_name, index=False)\n",
    "    os.chdir(old_path)\n",
    "\n",
    "def update_df_according_to_neo4j(re_type, language, model_used = \"\", extra_info = \"\"):\n",
    "    old_path = os.getcwd()\n",
    "\n",
    "    if model_used:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "    else:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type\n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    if extra_info: extra_info = \"_\" + extra_info\n",
    "    data = pd.read_csv(\"kg_of_\" + re_type + \"_with_concepts_\" + language + extra_info + \".csv\")\n",
    "\n",
    "    col_list = [language + \"_Subject_concept\", language + \"_Predicate_concept\", language + \"_Object_concept\"]\n",
    "\n",
    "    for col in col_list:\n",
    "        print(\"Currently at column: \" + col)\n",
    "        list_of_col = list(data[col])\n",
    "        temp_col = []\n",
    "\n",
    "        for cell in list_of_col:\n",
    "            if str(cell) == (\"[]\" or None): temp_col.append(\"-\")\n",
    "            else: temp_col.append(str(cell.replace(\", \", \":\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\\"\", \"\").replace(\"'\", \"\")))\n",
    "\n",
    "        data[col] = temp_col\n",
    "\n",
    "    # writing to csv file\n",
    "    file_name = path + \"\\\\kg_of_\" + re_type + \"_with_concepts_\" + language + extra_info + \".csv\"\n",
    "    data.to_csv(file_name, index=False)\n",
    "    os.chdir(old_path)\n",
    "\n",
    "def fix_duplicate(re_type, language, model_used = \"\"):\n",
    "    old_path = os.getcwd()\n",
    "\n",
    "    if model_used:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "    else:\n",
    "        path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type\n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    data = pd.read_csv(\"kg_of_\" + re_type + \"_\" + language + \".csv\")\n",
    "    temp_data = data[~data.duplicated()]\n",
    "    target_path = path + \"\\\\kg_of_\" + re_type + \"_\" + language + \".csv\"\n",
    "    temp_data.to_csv(target_path, index = False)\n",
    "    os.chdir(old_path)\n",
    "\n",
    "def translate_text_with_google_api(given_text, target_lan):\n",
    "    translator = Translator()\n",
    "    translated = translator.translate(given_text, src = \"en\", dest = target_lan)\n",
    "    return unidecode(translated.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T20:47:33.719979600Z",
     "start_time": "2023-06-12T20:47:33.604519100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# your own project path dir here\n",
    "start_time = time.time()\n",
    "my_path = \"C:\\\\Users\\\\dnaen\\\\PycharmProjects\\\\bachelor_thesis_23\"\n",
    "lan = \"en\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T16:12:34.907790600Z",
     "start_time": "2023-06-18T16:12:34.885490200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.Importing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# this has to be ran only once, because it creates the eurovoc_en.tsv file (which should already be there)\n",
    "# create_tsv_of_language(\"en\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:21:57.835833Z",
     "start_time": "2023-06-09T08:21:57.827856600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnaen\\PycharmProjects\\bachelor_thesis_23\\src\\main\n"
     ]
    }
   ],
   "source": [
    "# to make sure that we are in the original working directory\n",
    "data_path = my_path + \"\\\\src\\\\main\"\n",
    "os.chdir(data_path)\n",
    "print(os.getcwd()) # this should return something like \"...\\src\\main\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:21:58.691170300Z",
     "start_time": "2023-06-09T08:21:58.666680600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eurovoc imported\n"
     ]
    }
   ],
   "source": [
    "tsv_file = \"eurovoc_\" + lan + \".tsv\"\n",
    "\n",
    "# getting info of ids and concepts from the tsv file\n",
    "eurovoc_dic, eurovoc_reverse_dic, id_list, concept_list = tsv_dic_processing(tsv_file)\n",
    "print('Eurovoc imported')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:21:59.852955900Z",
     "start_time": "2023-06-09T08:21:59.823928400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Directive_(EU)_2016_1919_en.txt ...\n",
      "importing Directive_(EU)_2016_343_en.txt ...\n",
      "importing Directive_(EU)_2016_800_en.txt ...\n",
      "importing Directive_2010_64_EU_en.txt ...\n",
      "importing Directive_2012_13_EU_en.txt ...\n",
      "importing Directive_2013_48_EU_en.txt ...\n"
     ]
    }
   ],
   "source": [
    "# Extracting all existing txt documents in the path\n",
    "data_path = my_path + \"\\\\data\\\\\" + lan + \"\\\\directives_txt\"\n",
    "document_name_list = find_folder_with_type(data_path, '.txt') # detection of txt files in the folder\n",
    "document_dic = folder_list_to_dic(data_path, document_name_list) # storing document content in a dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:22:02.136156400Z",
     "start_time": "2023-06-09T08:22:02.057289700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Entity Extraction (NER)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through  Directive_(EU)_2016_1919_en.txt ...\n",
      "Going through  Directive_(EU)_2016_343_en.txt ...\n",
      "Going through  Directive_(EU)_2016_800_en.txt ...\n",
      "Going through  Directive_2010_64_EU_en.txt ...\n",
      "Going through  Directive_2012_13_EU_en.txt ...\n",
      "Going through  Directive_2013_48_EU_en.txt ...\n"
     ]
    }
   ],
   "source": [
    "# tagging document\n",
    "# data_path = my_path + \"\\\\data\\\\\" + lan + \"\\\\directives_txt_tagged\"\n",
    "updated_concept_list = update_concept_list(document_name_list, document_dic, concept_list, eurovoc_reverse_dic)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:23:21.349036100Z",
     "start_time": "2023-06-09T08:22:20.664934500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# have to run this only once (if file \"updated_eurovoc_en.tsv\" exists no need to run it)\n",
    "create_tsv_of_any_given_concept(updated_concept_list, lan)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:23:21.408808300Z",
     "start_time": "2023-06-09T08:23:21.363517800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Relation Extraction (RE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# run it once\n",
    "# !pip install stanford_openie"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:23:21.423727900Z",
     "start_time": "2023-06-09T08:23:21.409708500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# https://stanfordnlp.github.io/CoreNLP/openie.html#api\n",
    "# Default value of openie.affinity_probability_cap was 1/3.\n",
    "properties = {\n",
    "    'openie.affinity_probability_cap': 2 / 3,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:23:21.481668800Z",
     "start_time": "2023-06-09T08:23:21.428723600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx8G -cp C:\\Users\\dnaen\\.stanfordnlp_resources\\stanford-corenlp-4.5.3/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-bded9fa9149844ab.props -preload openie\n",
      "Document 'Directive_(EU)_2016_1919_en.txt' finished\n",
      "Document 'Directive_(EU)_2016_343_en.txt' finished\n",
      "Document 'Directive_(EU)_2016_800_en.txt' finished\n",
      "Document 'Directive_2010_64_EU_en.txt' finished\n",
      "Document 'Directive_2012_13_EU_en.txt' finished\n",
      "Document 'Directive_2013_48_EU_en.txt' finished\n"
     ]
    }
   ],
   "source": [
    "document_dic_tokenized = split_text_into_sentence(document_name_list, document_dic)\n",
    "list_of_triples = []\n",
    "triple_id = []\n",
    "testing = False\n",
    "\n",
    "with StanfordOpenIE(properties=properties) as client: # opening the server\n",
    "    if testing: print()\n",
    "    for doc_name in document_name_list:\n",
    "        for sentence in document_dic_tokenized[doc_name]:\n",
    "            if testing: print(\"Sentence\")\n",
    "            if testing: print(sentence)\n",
    "            current_triple_list_of_sentence = get_triples_stanford_openie(client, sentence) # returns [subject, relation, object]\n",
    "            if current_triple_list_of_sentence: # only run the loop if list not empty\n",
    "                index = 0\n",
    "                for current_triple in current_triple_list_of_sentence: # a sentence can have multiple triples\n",
    "                    if testing: print(\"Triple \" + str(index) + \": \" + str(current_triple))\n",
    "                    list_of_triples.append(current_triple)\n",
    "                    index += 1\n",
    "\n",
    "                triple_id.extend([doc_name] * len(current_triple_list_of_sentence))\n",
    "            if testing: print()\n",
    "        print(\"Document '\" + doc_name + \"' finished\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:25:45.089426200Z",
     "start_time": "2023-06-09T08:23:21.486657Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [i[0] for i in list_of_triples]\n",
    "\n",
    "# extract relation\n",
    "relation = [i[1] for i in list_of_triples]\n",
    "\n",
    "# extract object\n",
    "target = [i[2] for i in list_of_triples]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:25:45.119428900Z",
     "start_time": "2023-06-09T08:25:45.090423200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Build Knowledge Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "create_combined_eurovoc([\"en\", \"fr\", \"de\"]) # has to be ran only once"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:25:45.335976800Z",
     "start_time": "2023-06-09T08:25:45.121457100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253.50349164009094\n"
     ]
    }
   ],
   "source": [
    "# Handling the \"main\" language first\n",
    "data_path = my_path + \"\\\\src\\\\main\"\n",
    "os.chdir(data_path)\n",
    "\n",
    "path_to_eurovoc = \"eurovoc_\" + lan + \".tsv\"\n",
    "data_of_lan = pd.read_csv(path_to_eurovoc,sep='\\t')\n",
    "\n",
    "create_kg_csv(source, relation, target, \"predefined_dictionary\", lan, triple_id, \"stanford_OpenIE\")\n",
    "print(time.time() - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:25:45.505412Z",
     "start_time": "2023-06-09T08:25:45.340960800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "fix_duplicate(\"predefined_dictionary\", lan, \"stanford_OpenIE\") # removing duplicates"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:25:45.636298500Z",
     "start_time": "2023-06-09T08:25:45.509401800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at column: en_Subject\n",
      "Row0, Row1000, Row2000, Row3000, Row4000, Row5000, Row6000, Row7000, Row8000, Row9000, Row10000, Row11000, Row12000, Currently at column: en_Predicate\n",
      "Row0, Row1000, Row2000, Row3000, Row4000, Row5000, Row6000, Row7000, Row8000, Row9000, Row10000, Row11000, Row12000, Currently at column: en_Object\n",
      "Row0, Row1000, Row2000, Row3000, Row4000, Row5000, Row6000, Row7000, Row8000, Row9000, Row10000, Row11000, Row12000, "
     ]
    }
   ],
   "source": [
    "# adding to which concept each triple is related to\n",
    "data_path = my_path + \"\\\\src\\\\main\"\n",
    "os.chdir(data_path)\n",
    "update_triple_csv_file_with_concepts(\"predefined_dictionary\", lan, list(data_of_lan[lan.upper()]), \"stanford_OpenIE\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T00:32:34.470161200Z",
     "start_time": "2023-06-09T21:25:25.348994200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at column: en_Subject_concept\n",
      "Currently at column: en_Predicate_concept\n",
      "Currently at column: en_Object_concept\n"
     ]
    }
   ],
   "source": [
    "# turning list to a:b:c:d style instead of [a,b,c,d] because \",\" causes problem in neo4j\n",
    "update_df_according_to_neo4j(\"predefined_dictionary\", lan, \"stanford_OpenIE\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T09:02:13.022012500Z",
     "start_time": "2023-06-10T09:02:12.645428400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. Translate\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from row: 5689\n"
     ]
    }
   ],
   "source": [
    "# creating french KG\n",
    "simple_translator(\"fr\", \"predefined_dictionary\", model_used = \"stanford_OpenIE\") # CAUTION: since this uses API it can \"timeout\", so if happens just run it again (as the translator just starts from the place it has left it's fine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T08:15:29.177109Z",
     "start_time": "2023-06-11T07:32:16.578782600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from row: 4301\n"
     ]
    }
   ],
   "source": [
    "# creating german KG\n",
    "simple_translator(\"de\", \"predefined_dictionary\", model_used = \"stanford_OpenIE\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T09:57:11.989380Z",
     "start_time": "2023-06-11T09:05:24.272145300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6. KG finalization for other languages\n",
    "(this part has to be ran after the \"translation\" has been done)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at column: fr_Subject\n",
      "Row0, Row1000, Row2000, Row3000, Row4000, Row5000, Row6000, Row7000, Row8000, Row9000, Row10000, Row11000, Row12000, Currently at column: fr_Predicate\n",
      "Row0, Row1000, Row2000, Row3000, Row4000, Row5000, Row6000, Row7000, Row8000, Row9000, Row10000, Row11000, Row12000, Currently at column: fr_Object\n",
      "Row0, Row1000, Row2000, Row3000, Row4000, Row5000, Row6000, Row7000, Row8000, Row9000, Row10000, Row11000, Row12000, Currently at column: fr_Subject_concept\n",
      "Currently at column: fr_Predicate_concept\n",
      "Currently at column: fr_Object_concept\n"
     ]
    }
   ],
   "source": [
    "# my_path = \"C:\\\\Users\\\\dnaen\\\\PycharmProjects\\\\bachelor_thesis_23\"\n",
    "# data_path = my_path + \"\\\\src\\\\main\"\n",
    "# os.chdir(data_path)\n",
    "\n",
    "lan = \"fr\"\n",
    "\n",
    "path_to_eurovoc = \"eurovoc_\" + lan + \".tsv\"\n",
    "data_of_lan = pd.read_csv(path_to_eurovoc, sep='\\t')\n",
    "update_triple_csv_file_with_concepts(\"predefined_dictionary\", lan, list(data_of_lan[lan.upper()]), \"stanford_OpenIE\")\n",
    "update_df_according_to_neo4j(\"predefined_dictionary\", lan, \"stanford_OpenIE\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T12:39:22.195832400Z",
     "start_time": "2023-06-11T23:10:08.225999300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lan = \"de\"\n",
    "\n",
    "path_to_eurovoc = \"eurovoc_\" + lan + \".tsv\"\n",
    "data_of_lan = pd.read_csv(path_to_eurovoc, sep='\\t')\n",
    "update_triple_csv_file_with_concepts(\"predefined_dictionary\", lan, list(data_of_lan[lan.upper()]), \"stanford_OpenIE\")\n",
    "update_df_according_to_neo4j(\"predefined_dictionary\", lan, \"stanford_OpenIE\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Storage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "# nltk.download('punkt') # unsupervised trainable model, which means it can be trained on unlabeled data (Data that has not been tagged with information identifying its characteristics, properties, or categories is referred to as unlabeled data.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# To check what new concepts were added\n",
    "\n",
    "# temp_data_of_en_old = pd.read_csv('eurovoc_en.tsv',sep='\\t')\n",
    "# temp = list(updated_concept_list.values())\n",
    "# my_dict = {i: temp.count(i) for i in temp}\n",
    "# multiple_elements = []\n",
    "# for key, value in my_dict.items():\n",
    "#     if value > 1:\n",
    "#         multiple_elements.append(key)\n",
    "#\n",
    "# for elem in multiple_elements:\n",
    "#     value = {i for i in updated_concept_list if updated_concept_list[i] == elem}\n",
    "#\n",
    "#     for val in value:\n",
    "#         if val in list(temp_data_of_en_old[\"EN\"]): print(\"Old one is: \" + val)\n",
    "#\n",
    "#     print(str(value) + \" in id: \" + elem)\n",
    "#     print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T08:29:09.126516200Z",
     "start_time": "2023-06-06T08:29:09.105708400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# To try out triple extraction\n",
    "\n",
    "# text = \"How does the principle of mutual recognition in criminal matters rely on trust among Member States and the protection of suspects' rights as outlined in relevant articles and directives?\"\n",
    "#\n",
    "# properties = {\n",
    "#     'openie.affinity_probability_cap': 2 / 3,\n",
    "# }\n",
    "#\n",
    "# list_of_triples = []\n",
    "#\n",
    "# with StanfordOpenIE(properties=properties) as client: # opening the server\n",
    "#     current_triple_list_of_sentence = get_triples_stanford_openie(client, text) # returns [subject, relation, object]\n",
    "#     if current_triple_list_of_sentence: # only run the loop if list not empty\n",
    "#         index = 0\n",
    "#         for current_triple in current_triple_list_of_sentence: # a sentence can have multiple triples\n",
    "#             list_of_triples.append(current_triple)\n",
    "#             index += 1\n",
    "#\n",
    "# a, b, c = get_unique_words_from_triples(list_of_triples)\n",
    "# print(\"Subjects to look into:\")\n",
    "# print(a)\n",
    "# print()\n",
    "# print(\"Relations to look into:\")\n",
    "# print(b)\n",
    "# print()\n",
    "# print(\"Objects to look into:\")\n",
    "# print(c)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-23T14:12:12.703298100Z",
     "start_time": "2023-06-23T14:12:12.657623200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To check whether grouping function works\n",
    "\n",
    "# temp_text = \"protection of suspect 's procedural rights regarding specific mechanisms\"\n",
    "# temp_text = temp_text.split()\n",
    "# temp_group = \"\"\n",
    "# group_list = []\n",
    "# # groups of 2, groups of 3, groups of 4, ...\n",
    "#\n",
    "# for i in range(len(temp_text)):\n",
    "#     print()\n",
    "#     print(\"Groups of \", i+1)\n",
    "#     for j in range(len(temp_text) - i):\n",
    "#         for k in range(j, j+i+1):\n",
    "#             if k == j+i: temp_group = temp_group + temp_text[k]\n",
    "#             else: temp_group = temp_group + temp_text[k] + \" \"\n",
    "#             print(temp_text[k], end=\" \")\n",
    "#         group_list.append(temp_group)\n",
    "#         temp_group = \"\"\n",
    "#         print(\"\", end=\" | \")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for word in possible_words_list:\n",
    "#         word = word.lower()\n",
    "#         if word not in given_stopwords_list:\n",
    "#             for concept in given_concept_list:\n",
    "#                 concept_uni = unidecode(concept) # because the triples of other languages will be in unidecode\n",
    "#                 if word in concept_uni and len(word.split()) == len(concept_uni.split()):\n",
    "#                     print(\"Word: \", word)\n",
    "#                     print(\"Found concept \", concept_uni)\n",
    "#                     print()\n",
    "#                     elems_concept.append(concept_uni)\n",
    "#\n",
    "#     return elems_concept"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To check whether \"find_corresponding_eurovoc_concept\" works\n",
    "\n",
    "# --------------\n",
    "# text = \"Can the prosecution guarantee the protection of suspect's procedural rights regarding the specific mechanisms and common minimum standards that are in place, such as the Charter of Fundamental Rights and the European Convention on Human Rights?\"\n",
    "#\n",
    "#\n",
    "# properties = {\n",
    "#     'openie.affinity_probability_cap': 2 / 3,\n",
    "# }\n",
    "#\n",
    "# list_of_triples = []\n",
    "#\n",
    "# with StanfordOpenIE(properties=properties) as client: # opening the server\n",
    "#     current_triple_list_of_sentence = get_triples_stanford_openie(client, text) # returns [subject, relation, object]\n",
    "#     if current_triple_list_of_sentence: # only run the loop if list not empty\n",
    "#         index = 0\n",
    "#         for current_triple in current_triple_list_of_sentence: # a sentence can have multiple triples\n",
    "#             list_of_triples.append(current_triple)\n",
    "#             index += 1\n",
    "#\n",
    "# temp_triples = list_of_triples\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# for triple in list_of_triples:\n",
    "#     print(\"Triple\")\n",
    "#     print(triple)\n",
    "#     print(\"Found concepts\")\n",
    "#     for elem in triple:\n",
    "#         a = find_corresponding_eurovoc_concept(elem, concept_list, stopwords.words('english'))\n",
    "#         print(a, end = \", \")\n",
    "#\n",
    "#     print()\n",
    "#     print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for testing the speed of \"get_neighbouring_groups\" function\n",
    "\n",
    "# start_time = time.time()\n",
    "# text = \"Can the prosecution guarantee the protection of suspect's procedural rights regarding the specific mechanisms and common minimum standards that are in place, such as the Charter of Fundamental Rights and the European Convention on Human Rights. Can the prosecution guarantee the protection of suspect's procedural rights regarding the specific mechanisms and common minimum standards that are in place, such as the Charter of Fundamental Rights and the European Convention on Human Rights.\"\n",
    "#\n",
    "# a = get_neighbouring_groups(text)\n",
    "#\n",
    "# end_time = time.time()\n",
    "# print(end_time - start_time)\n",
    "# print()\n",
    "# print(len(a))\n",
    "# # 0.06703352928161621 # 2628"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# saving the already created concepts\n",
    "# if it's not, remember you can use \"create_tsv_of_language(given_language)\" or \"create_tsv_of_any_given_concept(given_concept_dict, given_language)\" to create them\n",
    "# data_of_en = pd.read_csv('updated_eurovoc_en.tsv',sep='\\t')\n",
    "# data_of_de = pd.read_csv('eurovoc_de.tsv',sep='\\t')\n",
    "# data_of_fr = pd.read_csv('eurovoc_fr.tsv',sep='\\t')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for testing\n",
    "\n",
    "# my_path = \"C:\\\\Users\\\\dnaen\\\\PycharmProjects\\\\bachelor_thesis_23\\\\src\\\\main\"\n",
    "# os.chdir(my_path)\n",
    "#\n",
    "# data_of_en = pd.read_csv('updated_eurovoc_en.tsv',sep='\\t')\n",
    "# lan = \"en\"\n",
    "#\n",
    "# data_path = \"C:\\\\Users\\\\dnaen\\\\PycharmProjects\\\\bachelor_thesis_23\\\\src\\\\main\\\\triples_data\\\\predefined_dictionary\\\\stanford_OpenIE\"\n",
    "# os.chdir(data_path)\n",
    "# temp_data = pd.read_csv(\"kg_of_predefined_dictionary_with_concepts_en.csv\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# TO CHECK THE PERFORMANCE OF THE STRING MEASURES\n",
    "\n",
    "# To check whether \"find_corresponding_eurovoc_concept\" works\n",
    "\n",
    "# text = \"What efforts have the written directives undertaken to safeguard children who are suspects or accused individuals?\"\n",
    "#\n",
    "#\n",
    "# properties = {\n",
    "#     'openie.affinity_probability_cap': 2 / 3,\n",
    "# }\n",
    "#\n",
    "# list_of_triples = []\n",
    "#\n",
    "# with StanfordOpenIE(properties=properties) as client: # opening the server\n",
    "#     current_triple_list_of_sentence = get_triples_stanford_openie(client, text) # returns [subject, relation, object]\n",
    "#     if current_triple_list_of_sentence: # only run the loop if list not empty\n",
    "#         index = 0\n",
    "#         for current_triple in current_triple_list_of_sentence: # a sentence can have multiple triples\n",
    "#             list_of_triples.append(current_triple)\n",
    "#             index += 1\n",
    "#\n",
    "# temp_triples = list_of_triples\n",
    "#\n",
    "# lan = \"en\"\n",
    "# tsv_file = \"eurovoc_\" + lan + \".tsv\"\n",
    "#\n",
    "# # getting info of ids and concepts from the tsv file\n",
    "# eurovoc_dic, eurovoc_reverse_dic, id_list, concept_list = tsv_dic_processing(tsv_file)\n",
    "#\n",
    "# for triple in list_of_triples:\n",
    "#     print(\"Triple\")\n",
    "#     print(triple)\n",
    "#     print(\"Found concepts\")\n",
    "#     for elem in triple:\n",
    "#         a = find_corresponding_eurovoc_concept_jaro_winkler_with_english_semantic(elem, concept_list, stopwords.words('english'))\n",
    "#         print(a, end = \", \")\n",
    "#\n",
    "#     print()\n",
    "#     print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-23T14:12:12.733946300Z",
     "start_time": "2023-06-23T14:12:12.682324100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# TO CHECK PERFORMANCE OF SEMANTIC MATCH\n",
    "from gensim.models import Word2Vec\n",
    "# import gensim.downloader as api\n",
    "# # TODO train word2vec\n",
    "# model = api.load(\"word2vec-google-news-300\") # have to run it once to download to pc, in later runs it just takes the downloaded file from pc (so takes much shorter after second and future times)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:01:52.468795100Z",
     "start_time": "2023-06-09T08:00:41.175604400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s0 vs s1 -> 0.07994840294122696\n"
     ]
    }
   ],
   "source": [
    "# continuation\n",
    "# import numpy as np\n",
    "# from scipy import spatial\n",
    "# s0 = 'place'\n",
    "# s1 = 'plate'\n",
    "#\n",
    "#\n",
    "# def preprocess(s):\n",
    "#     return [i.lower() for i in s.split()]\n",
    "#\n",
    "# def get_vector(s):\n",
    "#     return np.sum(np.array([model[i] for i in preprocess(s)]), axis=0)\n",
    "#\n",
    "# print('s0 vs s1 ->',1 - spatial.distance.cosine(get_vector(s0), get_vector(s1)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:09:49.042735100Z",
     "start_time": "2023-06-09T08:09:49.028840100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for observing the translation\n",
    "#\n",
    "# subject = ['principle', 'mutual recognition']\n",
    "# relation = ['does rely as', 'does rely on', 'is in']\n",
    "# objects = ['as outlined relevant articles', 'as outlined articles', 'trust', 'trust among Member States', 'criminal matters']\n",
    "#\n",
    "# lan = \"de\"\n",
    "# print(\"Subject\")\n",
    "# for elem in subject:\n",
    "#     print(translate_text_with_google_api(elem, lan))\n",
    "#\n",
    "# print()\n",
    "# print(\"Relation\")\n",
    "# for elem in relation:\n",
    "#     print(translate_text_with_google_api(elem, lan))\n",
    "#\n",
    "# print()\n",
    "# print(\"Object\")\n",
    "# for elem in objects:\n",
    "#     print(translate_text_with_google_api(elem, lan))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
