{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-06T14:18:20.405338100Z",
     "start_time": "2023-06-06T14:18:20.197263400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dnaen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import xml.etree.ElementTree as Xet # for parsing and creating XML data\n",
    "import pandas as pd\n",
    "import os, csv, re, nltk\n",
    "from flair.data import Corpus # in order to use the functions tha flair has\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings # these embeddings helps NER to perform better\n",
    "from itertools import islice\n",
    "from nltk.stem import WordNetLemmatizer # previously need to download \"nltk.download('wordnet')\" and \"nltk.download('omw-1.4')\". But beware if new version comes out\n",
    "from tqdm import tqdm # to display loop in a bar\n",
    "from openie import StanfordOpenIE # for using our OIE tool\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from googletrans import Translator # don't forget to run \"!pip install googletrans==3.1.0a0\" before using this\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NER (Named-Entity Recognition)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "# functions\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# English: en, German: de, French: fr, ... -> creates the tsv of given descriptor of any language\n",
    "def create_tsv_of_language(given_language):\n",
    "    \"\"\"\n",
    "    Before running this function below, the \"desc_\"\".xml\" file (that is downloaded from EuroVoc website) needs to be downloaded and added to package \"data/\"\"/descriptors/...\"\n",
    "    PS: Even after this function finishes it takes some time for the new file to appear\n",
    "    \"\"\"\n",
    "    cols = ['ID', given_language.upper()] # will be saving in a tsv with ids and their corresponding terms\n",
    "    rows = []\n",
    "\n",
    "    # parsing the xml file -> with the given EuroVoc descriptors\n",
    "    temp_path = os.getcwd()\n",
    "    temp_path = temp_path.replace(\"src\\\\main\", \"data\\\\\" + given_language + \"\\\\descriptors\\\\desc_\" + given_language + \".xml\")\n",
    "    xml_parse = Xet.parse(temp_path)\n",
    "    root = xml_parse.getroot()\n",
    "\n",
    "    # iterate through the elements of xml file\n",
    "    for element in root:\n",
    "        rows.append({\"ID\": element.find(\"DESCRIPTEUR_ID\").text, given_language.upper(): element.find(\"LIBELLE\").text})\n",
    "\n",
    "    # creating the tsv file\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    df.to_csv(\"eurovoc_\" + given_language + \".tsv\", sep='\\t', index=False) # using sep='\\t' gives us a tsv file instead of csv\n",
    "\n",
    "def create_tsv_of_any_given_concept(given_concept_dict, given_language):\n",
    "    \"\"\"\n",
    "    :param given_concept_dict: A dictionary in style of .e.g, {EN:..., ID:...}\n",
    "    :param given_language: en: english, fr: fran√ßais, de: deutsch\n",
    "    :return: creates a new tsv called \"updated_eurovoc_en.tsv\"\n",
    "    \"\"\"\n",
    "    cols = ['ID', given_language.upper()] # will be saving in a tsv with ids and their corresponding terms\n",
    "    rows = []\n",
    "\n",
    "    # iterate through the elements of xml file\n",
    "    for key, value in given_concept_dict.items():\n",
    "        rows.append({\"ID\": value, given_language.upper(): key})\n",
    "\n",
    "    # creating the tsv file\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    df.to_csv(\"updated_eurovoc_\" + given_language + \".tsv\", sep='\\t', index=False) # using sep='\\t' gives us a tsv file instead of csv\n",
    "\n",
    "# this function assumes we get the text annotated as [entity_value](entity_name), and assigns prefixes B, I, and 0 to each token\n",
    "def get_tokens_with_entities(raw_text: str):\n",
    "    # split the text by spaces (but not splitting the space inside the square brackets (so not splitting the \"multi-word\" entity value yet))\n",
    "    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n",
    "\n",
    "    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n",
    "    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n",
    "\n",
    "    # flags: re.IGNORECASE and re.MULTILINE\n",
    "    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M) # using it to compile a regular expression pattern provided as a string into a regex pattern object\n",
    "\n",
    "    tokens_with_entities = []\n",
    "\n",
    "    for raw_token in raw_tokens:\n",
    "        match = entity_value_pattern_compiled.match(raw_token) # if no match then returns None\n",
    "\n",
    "        if match:\n",
    "            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n",
    "\n",
    "            # we prefix the name of entity differently\n",
    "            # B- indicates beginning of an entity\n",
    "            # I- indicates the token is not a new entity itself but rather a part of existing one\n",
    "            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n",
    "                entity_prefix = \"B\" if i == 0 else \"I\"\n",
    "                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n",
    "                tokens_with_entities.append((raw_entity_token, entity_name))\n",
    "        else:\n",
    "            tokens_with_entities.append((raw_token, \"O\")) # no match\n",
    "\n",
    "    return tokens_with_entities\n",
    "\n",
    "# NLTK VERSION\n",
    "# TODO add some kind of automatic noun-verb-... identifier for lemmatization (so parts-of-speech required to add)\n",
    "def regex_from_term_nltk(term, lemmatizer):\n",
    "    regex = r\"\\b(\" # Regex Opening\n",
    "    tokensList = nltk.word_tokenize(term)\n",
    "\n",
    "    # Adding terms to regex\n",
    "    if len(tokensList) == 1: # in case of one-word term\n",
    "        for token in tokensList:\n",
    "            regex += token_cleaning(token, lemmatizer)\n",
    "\n",
    "    else: # if it is a multi-word term\n",
    "        decount = len(tokensList)\n",
    "        for token in tokensList:\n",
    "            decount = decount-1\n",
    "            # add between-words\n",
    "            if decount != len(tokensList)-1:\n",
    "                regex+= r'\\w*\\W\\w*\\W*'\n",
    "            # add token\n",
    "            regex += token_cleaning(token, lemmatizer)\n",
    "\n",
    "    regex += '''\\w{0,5})(\\W)''' # Regex Closure\n",
    "    return regex\n",
    "\n",
    "def token_cleaning(token, lemmatizer):\n",
    "    token = token.lower()\n",
    "    token = lemmatizer.lemmatize(token)\n",
    "    return token\n",
    "\n",
    "# Functions for document processing were modified from @https://github.com/shashankmc/eurovoc_entity_link/blob/master/EurovocTagger.py\n",
    "def tsv_dic_processing(path):\n",
    "    \"\"\"\n",
    "    :param path: the name of the eurovoc.tsv file\n",
    "    :return: Dic: Dictionary in style of {ID: Word}\n",
    "    :return: RevDic: Dictionary in style of {Word: ID}\n",
    "    :return: list1: list of IDs\n",
    "    :return: list2: list of words (concepts)\n",
    "    \"\"\"\n",
    "    # Dic, RevDic, list1, list2\n",
    "    # Only works with a 2-columns ([ID], [EN]) TSV file\n",
    "    Dic = {}\n",
    "    RevDic = {}\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    with open(path, 'rt', encoding='utf8') as csvfile:\n",
    "        myreader = csv.reader(csvfile, delimiter='\\t')\n",
    "        rcount = 0\n",
    "        for row in myreader:\n",
    "            rcount += 1\n",
    "            ccount = 0\n",
    "            if rcount > 1:\n",
    "                for cells in row:\n",
    "                    ccount += 1\n",
    "                    if ccount ==1:\n",
    "                        list1.append(cells)\n",
    "                        key = cells\n",
    "                    else:\n",
    "                        list2.append(cells)\n",
    "                        value = cells\n",
    "                Dic[key] = value\n",
    "                RevDic[value] = key\n",
    "    return Dic, RevDic, list1, list2\n",
    "\n",
    "def find_folder_with_type(given_path, doc_type): # returns all documents found in path\n",
    "    doc_list = []\n",
    "    for doc in os.listdir(given_path):\n",
    "        if re.search (r'.*\\%s$' % doc_type, doc) is not None: # even though this shows as error in IDE it's fine\n",
    "            doc_list.append(doc)\n",
    "    return doc_list\n",
    "\n",
    "\n",
    "def folder_list_to_dic(given_path, given_list): # given file names extracts their texts and saves in a dic\n",
    "    dic = {}\n",
    "    old_path = os.getcwd() # saving the previous working dir so we can switch back to that dir later\n",
    "    os.chdir(given_path)\n",
    "\n",
    "    # the input should be a list of file contained in a folder\n",
    "    for file_name in given_list:\n",
    "        print('importing', file_name, '...')\n",
    "        with open(\"%s\" % file_name, \"r\", encoding='utf8') as my_file:\n",
    "            text = my_file.read()\n",
    "        dic[file_name]= text\n",
    "\n",
    "    os.chdir(old_path)\n",
    "    return dic\n",
    "\n",
    "# tagging by researching concept-regexed as a substring of the text (by using NLTK)\n",
    "def tagging_document(path_of_tagged, given_doc_list, given_doc_dic, given_concept_list, given_eurovoc_reverse_dic):\n",
    "    \"\"\"\n",
    "    This function takes the information of the descriptor (e.g., {id:concept}, id list, concept list, ...) and then with the given document information it creates the new tagged document in tagged folder. Additionally, it returns the new updated concept list which contains additional \"concepts\" found in the document text that seems to be related to one of the original concepts. Thus, expanding the vocabulary we have.\n",
    "\n",
    "    :param path_of_tagged: the location (dir) of the tagged folder\n",
    "    :param given_doc_list: a list of names of the documents\n",
    "    :param given_doc_dic: a dic that contains the contents of the document i.e. {doc_name: doc_text}\n",
    "    :param given_concept_list: the original concept list downloaded from Eurovoc\n",
    "    :param given_eurovoc_reverse_dic: opposite of \"given_concept_list\" so {concept: id}\n",
    "    :return: new_concept: this is the new expanded concept list\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    old_path = os.getcwd() # saving the previous working dir so we can switch back to that dir later\n",
    "    os.chdir(path_of_tagged)\n",
    "    new_concept_dic = given_eurovoc_reverse_dic.copy() # using the reverse eurovoc dict instead because can't add words with same id\n",
    "\n",
    "    for doc_name in given_doc_list:\n",
    "        tags_list=[]\n",
    "        tagged_text = \"\"\n",
    "        print('tagging', doc_name,'...')\n",
    "        text = given_doc_dic[doc_name]\n",
    "        text = text.lower()\n",
    "        tagged_text = text # document's initial text\n",
    "\n",
    "        # a concept tag will be done with a star (*), and the identifier with a +\n",
    "        for concept in given_concept_list:\n",
    "\n",
    "            if concept != \"\": # if concept empty, will tag everything (so need to make sure that it's not empty)\n",
    "                # REGEX CREATION: creating regex of the concept such that it can be used to search in doc later\n",
    "                regex = regex_from_term_nltk(concept, lemmatizer)\n",
    "\n",
    "                # concept = concept.strip()\n",
    "                # TAGGING #\n",
    "                # semantically neutral symbols are chosen to prevent eurovoc concepts from matching tags\n",
    "                if re.search(regex, text) is not None:\n",
    "                    # these prints can be used to check performance\n",
    "                    # print(\"Match made!\")\n",
    "                    # print(\"Found: \" + re.search(regex, text).group() + \", for concept: \" + concept)\n",
    "                    match_in_text = re.search(regex, text).group()\n",
    "                    if match_in_text not in given_concept_list:\n",
    "                        # cleaning up the matched text\n",
    "                        match_in_text = match_in_text.replace(\"\\n\", \"\")\n",
    "                        # match_in_text = match_in_text.replace(\"[V4.3]\", \"\")\n",
    "                        match_in_text = match_in_text.strip()\n",
    "                        match_in_text = match_in_text.strip(\".,-\")\n",
    "                        new_concept_dic[match_in_text] = given_eurovoc_reverse_dic[concept]\n",
    "\n",
    "                    tags_list.append(concept)\n",
    "                    sub_regex = r\"[\" + concept + r\"]\"\n",
    "                    sub_regex += r\"(\" + given_eurovoc_reverse_dic[concept] + r\") \" # insert the identifier\n",
    "                    tagged_text = re.sub(regex, sub_regex, tagged_text)\n",
    "\n",
    "    # create a new file with the tagged file\n",
    "        file = open(\"%s_TAGGED.txt\" % doc_name, \"w\", encoding='utf8')\n",
    "        file.write(tagged_text)\n",
    "        file.close()\n",
    "\n",
    "    os.chdir(old_path) # change back to previous path\n",
    "\n",
    "    return new_concept_dic\n",
    "\n",
    "# updating the concept list by going through the given documents (by using NLTK)\n",
    "def update_concept_list(given_doc_list, given_doc_dic, given_concept_list, given_eurovoc_reverse_dic):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_concept_dic = given_eurovoc_reverse_dic.copy() # using the reverse eurovoc dict instead because can't add words with same id\n",
    "\n",
    "    for doc_name in given_doc_list:\n",
    "        print('Going through ', doc_name,'...')\n",
    "        text = given_doc_dic[doc_name]\n",
    "        text = text.lower()\n",
    "\n",
    "        # a concept tag will be done with a star (*), and the identifier with a +\n",
    "        for concept in given_concept_list:\n",
    "\n",
    "            if concept != \"\": # if concept empty, will tag everything (so need to make sure that it's not empty)\n",
    "                # REGEX CREATION: creating regex of the concept such that it can be used to search in doc later\n",
    "                regex = regex_from_term_nltk(concept, lemmatizer)\n",
    "\n",
    "                # concept = concept.strip()\n",
    "                # TAGGING #\n",
    "                # semantically neutral symbols are chosen to prevent eurovoc concepts from matching tags\n",
    "                if re.search(regex, text) is not None:\n",
    "                    # these prints can be used to check performance\n",
    "                    # print(\"Match made!\")\n",
    "                    # print(\"Found: \" + re.search(regex, text).group() + \", for concept: \" + concept)\n",
    "                    match_in_text = re.search(regex, text).group()\n",
    "                    if match_in_text not in given_concept_list:\n",
    "                        # cleaning up the matched text\n",
    "                        match_in_text = match_in_text.replace(\"\\n\", \"\")\n",
    "                        # match_in_text = match_in_text.replace(\"[V4.3]\", \"\")\n",
    "                        match_in_text = match_in_text.strip()\n",
    "                        match_in_text = match_in_text.strip(\".,-\")\n",
    "                        new_concept_dic[match_in_text] = given_eurovoc_reverse_dic[concept]\n",
    "\n",
    "                    # tags_list.append(concept)\n",
    "\n",
    "    return new_concept_dic\n",
    "\n",
    "def find_corresponding_eurovoc_concept(given_text, given_concept_list, given_stopwords_list):\n",
    "    \"\"\"\n",
    "    The goal of this function is to find the concepts the given text might be linked to.\n",
    "    :param given_text: takes in a string (this string can be a single word or a compound word (list of words)\n",
    "    :param given_concept_list: the original concept list downloaded from Eurovoc (MAKE SURE THIS IS A LIST!)\n",
    "    :param given_stopwords_list: a list words we don't want to look into (in our case it's stopwords)\n",
    "    :return: a list of concepts it might be related to\n",
    "    \"\"\"\n",
    "    elems_concept = []\n",
    "\n",
    "    # such that we can also check on possible compounds by bruteforce\n",
    "    possible_words_list = get_neighbouring_groups(given_text) # ex: \"I need to\" -> [\"I\", \"need\", \"to, \"I need\", \"need to\", \"I need to\"]\n",
    "\n",
    "    for word in possible_words_list:\n",
    "        word = word.lower()\n",
    "        for concept in given_concept_list:\n",
    "            contains = False\n",
    "            if len(word.split()) == len(concept.split()): # checking whether there is same number of words\n",
    "                word_list = word.split()\n",
    "                concept_list_1 = concept.split()\n",
    "\n",
    "                contains = True # set it to false when a mismatch occurs\n",
    "                for i in range(len(word_list)): # this was done such that we can go through compounds too (and applyiing specific operations (e.g., lemmatization) to each word)\n",
    "                    # TODO add lemmatization here?\n",
    "                    current_word = word_list[i]\n",
    "                    current_concept = concept_list_1[i]\n",
    "\n",
    "                    if current_word not in given_stopwords_list:\n",
    "                        concept_uni = unidecode(current_concept) # because the triples of other languages will be in unidecode\n",
    "                        if current_word not in concept_uni:\n",
    "                            contains = False\n",
    "                    else: contains = False\n",
    "\n",
    "            if contains: elems_concept.append(unidecode(concept))\n",
    "\n",
    "    return elems_concept\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T14:18:22.563088800Z",
     "start_time": "2023-06-06T14:18:22.546829400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [],
   "source": [
    "# for word in possible_words_list:\n",
    "#         word = word.lower()\n",
    "#\n",
    "#         for concept in given_concept_list:\n",
    "#             if word in concept:\n",
    "#             # match_in_text = re.search(word_regex, text).group()\n",
    "#             match_in_text = re.search(word_regex, text)\n",
    "#             if match_in_text is not None:\n",
    "#                 print(\"Word: \", word)\n",
    "#                 print(\"Found concept \", concept)\n",
    "#                 print()\n",
    "#                 elems_concept.append(concept)\n",
    "#             else: elems_concept.append(\"-\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T14:18:24.521552800Z",
     "start_time": "2023-06-06T14:18:24.507908600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Relations Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_triples_stanford_openie(given_client, given_sentence):\n",
    "    triples_list = []\n",
    "\n",
    "    # returns dict in this style: {'subject': 'Obama', 'relation': 'was born in', 'object': 'Hawaii'}\n",
    "    # for triple in tqdm(given_client.annotate(given_sentence)): # this can be used for debugging\n",
    "    for triple in given_client.annotate(given_sentence):\n",
    "        triples_list.append([triple.get(\"subject\"), triple.get(\"relation\"), triple.get(\"object\")])\n",
    "\n",
    "    return triples_list\n",
    "\n",
    "def split_text_into_sentence(given_doc_names, given_docs):\n",
    "    given_docs_tokenized = {}\n",
    "    for doc_name in given_doc_names:\n",
    "        text_of_doc = given_docs[doc_name]\n",
    "        text_of_doc = text_of_doc.replace('\\n',' ') # removing the newline string from text\n",
    "        text_of_doc = text_of_doc.replace('\\xad ','') # removing the hyphen used for line breaking\n",
    "\n",
    "        text_of_doc_tokenized = sent_tokenize(text_of_doc)\n",
    "        given_docs_tokenized[doc_name] = text_of_doc_tokenized\n",
    "\n",
    "    return given_docs_tokenized\n",
    "\n",
    "# this function is modified from \"version1_simple\" file\n",
    "def create_kg_csv(subjects, predicates, objects, re_type, model_used, language, given_id_list):\n",
    "    \"\"\"\n",
    "    [source(subject) --relation(predicate)--> target(object)]\n",
    "    :param subjects: source\n",
    "    :param predicates: relation\n",
    "    :param objects: target\n",
    "    :param re_type: currently, we have only relation extraction type of \"simple\" and \"predefined-dictionary\"\n",
    "    :param model_used: currently only \"stanford_OpenIE\"\n",
    "    :param language: en, fr, de\n",
    "    :param given_id_list: represents the id of triple, in this case it's from which directive/legislation it came from\n",
    "    :return: returns nothing only creates the csv file\n",
    "    \"\"\"\n",
    "    # field names\n",
    "    fields = [language + '_Subject', language + '_Predicate', language + '_Object', \"Triple_ID\"]\n",
    "    filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used + \"\\\\kg_of_\" + re_type + \"_\" + language + \".csv\"\n",
    "\n",
    "\n",
    "    rows = [[subjects[i], predicates[i], objects[i], given_id_list[i]] for i in range(len(subjects))]\n",
    "\n",
    "    # find out empty and None strings, replacing it with \"-\"\n",
    "    for i in range(len(rows)):\n",
    "        for j in range(len(rows[0])): # so len 3\n",
    "            if rows[i][j] == \"\" or rows[i][j] is None: rows[i][j] = \"-\"\n",
    "            rows[i][j] = unidecode(rows[i][j].lower())\n",
    "\n",
    "    # writing to csv file\n",
    "    with open(filename, 'w', newline = '') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(fields) # first writing fields\n",
    "        csv_writer.writerows(rows) # now the remaining record\n",
    "\n",
    "# German: de, French: fr, but can work for any other languages too\n",
    "def simple_translator(target_language, re_type, model_used, extra_info = \"\"): # assumes that en kg csv is already created\n",
    "    translator = Translator()\n",
    "\n",
    "    if extra_info: extra_info = \"_\" + extra_info\n",
    "    filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "    filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "    filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_\" + target_language + extra_info + \".csv\"\n",
    "\n",
    "    with open(filename_english, mode =\"r\") as file_original, open(filename_assigned_language, mode = \"r+\", newline = '') as file_2:\n",
    "        csv_reader = csv.reader(file_original)\n",
    "        csv_reader_file2 = csv.reader(file_2)\n",
    "        csv_writer = csv.writer(file_2)\n",
    "\n",
    "        current_size_of_new_kg = 0\n",
    "        for step in csv_reader_file2:\n",
    "            current_size_of_new_kg += 1\n",
    "        print(\"Starting from row: \" + str(current_size_of_new_kg))\n",
    "\n",
    "        # first line is headers\n",
    "        if current_size_of_new_kg == 0: csv_writer.writerow([target_language + \"_Subject\", target_language + \"_Predicate\", target_language + \"_Object\", \"Triple_ID\"])\n",
    "        current_step = 0\n",
    "        for lines in csv_reader: # each line is a list of 3 elements (source - relation - target)\n",
    "            if current_step > current_size_of_new_kg:\n",
    "                source_en, relation_en, target_en = lines[0], lines[1], lines[2]\n",
    "                # src(source) = english, dest(destination) = language to translate to\n",
    "                translated_source, translated_relation, translated_target = translator.translate(source_en, src = \"en\", dest = target_language), translator.translate(relation_en, src = \"en\", dest = target_language), translator.translate(target_en, src = \"en\", dest = target_language)\n",
    "\n",
    "                temp_row = [unidecode(translated_source.text), unidecode(translated_relation.text), unidecode(translated_target.text), unidecode(lines[3])]\n",
    "                # index = 0\n",
    "                # for row in temp_row:\n",
    "                #     temp_row[index] = row.replace(u'\\u200b', '') # this \"space\" character gives error, but it just adds extra space so just removing it\n",
    "                #     index += 1\n",
    "                csv_writer.writerow(temp_row)\n",
    "            else:\n",
    "                current_step += 1\n",
    "\n",
    "# German: de, French: fr, but can work for any other languages too\n",
    "def simple_translator_with_concept(target_language, re_type, model_used, given_concept_en, given_concept_na, extra_info = \"\"): # assumes that en kg csv is already created\n",
    "    \"\"\"\n",
    "    :param target_language: language to translate to, e.g. fr, de, ...\n",
    "    :param re_type: currently, we have only relation extraction type of \"simple\" and \"predefined_dictionary\"\n",
    "    :param model_used: currently we only have \"stanford_OpenIE\" model\n",
    "    :param given_concept_en: concept dataframe (as [\"ID\", \"EN\"]) of english\n",
    "    :param given_concept_na: concept dataframe (as [\"ID\", \"..\"]) of target language\n",
    "    :param extra_info: default is empty, but can be used to pass any information about the \"file\" to be initialized\n",
    "    :return: creates a new translated KG with the use of concepts\n",
    "    \"\"\"\n",
    "    translator = Translator()\n",
    "\n",
    "    if extra_info: extra_info = \"_\" + extra_info\n",
    "    filename = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "    filename_english = filename + \"\\\\kg_of_\" + re_type + \"_en.csv\"\n",
    "    filename_assigned_language = filename + \"\\\\kg_of_\" + re_type + \"_\" + target_language + extra_info + \".csv\"\n",
    "\n",
    "    list_of_concept_en = list(given_concept_en[\"EN\"])\n",
    "    target_language_capitalized = target_language.upper()\n",
    "    testing = False\n",
    "\n",
    "    with open(filename_english, mode =\"r\") as file_original, open(filename_assigned_language, mode = \"r+\", newline = '') as file_2:\n",
    "        csv_reader = csv.reader(file_original)\n",
    "        csv_writer = csv.writer(file_2)\n",
    "        csv_reader_file2 = csv.reader(file_2)\n",
    "\n",
    "        current_size_of_new_kg = 0\n",
    "        for step in csv_reader_file2:\n",
    "            current_size_of_new_kg += 1\n",
    "        print(\"Starting from row: \" + str(current_size_of_new_kg))\n",
    "\n",
    "        current_step = 0\n",
    "        for lines in csv_reader: # each line is a list of 3 elements (source - relation - target)\n",
    "            if current_step > current_size_of_new_kg:\n",
    "                source_en, relation_en, target_en = lines[0], lines[1], lines[2]\n",
    "\n",
    "                # src(source) = english, dest(destination) = language to translate to\n",
    "                translated_source, translated_relation, translated_target = translator.translate(source_en, src = \"en\", dest = target_language), translator.translate(relation_en, src = \"en\", dest = target_language), translator.translate(target_en, src = \"en\", dest = target_language)\n",
    "\n",
    "                translated_source = translated_source.text\n",
    "                translated_relation = translated_relation.text\n",
    "                translated_target = translated_target.text\n",
    "\n",
    "                # if the text exists in concept list, then replace it with it's corresponding id\n",
    "                if any(s in list_of_concept_en for s in (source_en, relation_en, target_en)):\n",
    "                    if source_en in list_of_concept_en:\n",
    "                        if testing: print(\"Source:\")\n",
    "                        if testing: print(\"English version: \" + source_en)\n",
    "                        if testing: print(\"Pre: \" + translated_source)\n",
    "                        concept_id = int(given_concept_en.loc[given_concept_en[\"EN\"] == source_en][\"ID\"])\n",
    "                        translated_source = given_concept_na.loc[given_concept_na[\"ID\"] == concept_id, target_language_capitalized].item()\n",
    "                        if testing:print(\"Pro: \" + translated_source)\n",
    "\n",
    "                    if relation_en in list_of_concept_en:\n",
    "                        if testing: print(\"Relation:\")\n",
    "                        if testing: print(\"English version: \" + relation_en)\n",
    "                        if testing: print(\"Pre: \" + translated_relation)\n",
    "                        concept_id = int(given_concept_en.loc[given_concept_en[\"EN\"] == relation_en][\"ID\"])\n",
    "                        translated_relation = given_concept_na.loc[given_concept_na[\"ID\"] == concept_id, target_language_capitalized].item()\n",
    "                        if testing: print(\"Pro: \" + translated_relation)\n",
    "\n",
    "                    if target_en in list_of_concept_en:\n",
    "                        if testing: print(\"Target\")\n",
    "                        if testing: print(\"English version: \" + target_en)\n",
    "                        if testing: print(\"Pre: \" + translated_target)\n",
    "                        concept_id = int(given_concept_en.loc[given_concept_en[\"EN\"] == target_en][\"ID\"])\n",
    "                        translated_target = given_concept_na.loc[given_concept_na[\"ID\"] == concept_id, target_language_capitalized].item()\n",
    "                        if testing: print(\"Pro: \" + translated_target)\n",
    "\n",
    "                temp_row = [translated_source, translated_relation, translated_target]\n",
    "\n",
    "                index = 0\n",
    "                for row in temp_row:\n",
    "                    temp_row[index] = row.replace(u'\\u200b', '') # this \"space\" character gives error, but it just adds extra space so just removing it\n",
    "                    index += 1\n",
    "\n",
    "                csv_writer.writerow(temp_row)\n",
    "            else:\n",
    "                current_step += 1\n",
    "\n",
    "def get_translation_through_eurovoc(given_text, given_concept, given_concept_target):\n",
    "    # given concepts need to be a dataframe as [\"ID\", \"..\"]\n",
    "\n",
    "    list_of_concept = list(given_concept[given_concept.keys()[1]])\n",
    "    if given_text in list_of_concept:\n",
    "        itsID = given_concept[\"ID\"].get(list_of_concept.index(given_text))\n",
    "        corresponding_eurovoc = given_concept_target.loc[given_concept_target['ID'] == itsID][given_concept_target.keys()[1]].values[0]\n",
    "        return corresponding_eurovoc\n",
    "\n",
    "def get_unique_words_from_triples(given_list_of_triples):\n",
    "    unique_subjects = []\n",
    "    unique_relations = []\n",
    "    unique_objects = []\n",
    "\n",
    "    for triple in given_list_of_triples:\n",
    "        if triple[0] not in unique_subjects: unique_subjects.append(triple[0])\n",
    "        if triple[1] not in unique_relations: unique_relations.append(triple[1])\n",
    "        if triple[2] not in unique_objects: unique_objects.append(triple[2])\n",
    "\n",
    "    return unique_subjects, unique_relations, unique_objects\n",
    "\n",
    "def get_neighbouring_groups(given_text):\n",
    "    text_split = given_text.split()\n",
    "    group = \"\"\n",
    "    group_list = []\n",
    "\n",
    "    # groups of 2, groups of 3, groups of 4, ...\n",
    "    for i in range(len(text_split)):\n",
    "        for j in range(len(text_split) - i):\n",
    "            for k in range(j, j+i+1):\n",
    "                if k == j+i: group = group + text_split[k]\n",
    "                else: group = group + text_split[k] + \" \"\n",
    "            group_list.append(group)\n",
    "            group = \"\"\n",
    "    return group_list\n",
    "\n",
    "def create_combined_eurovoc(given_lans):\n",
    "    complete_data = []\n",
    "    fields = ['ID']\n",
    "\n",
    "    # adding id\n",
    "    data = pd.read_csv(\"eurovoc_\" + given_lans[0] + \".tsv\",sep='\\t')\n",
    "    data = data.sort_values(by=[\"ID\"])\n",
    "    complete_data.append(list(data.iloc[:, 0]))\n",
    "\n",
    "    # adding the concepts\n",
    "    for lan in given_lans:\n",
    "        fields.append(lan.upper())\n",
    "        data = pd.read_csv(\"eurovoc_\" + lan + \".tsv\",sep='\\t') # TODO what about updated version?\n",
    "        data = data.sort_values(by=[\"ID\"])\n",
    "        complete_data.append(list(data.iloc[:, 1]))\n",
    "\n",
    "\n",
    "    for i in range(1, len(complete_data)):\n",
    "        complete_data[i] = [unidecode(x.lower()) for x in complete_data[i]]\n",
    "\n",
    "    rows = [[complete_data[0][i], complete_data[1][i], complete_data[2][i], complete_data[3][i]] for i in range(len(complete_data[0]))] # TODO find a way to make this line work for any given number of columns\n",
    "\n",
    "    # writing to csv file\n",
    "    filename = os.getcwd() + \"\\\\combined_eurovoc.csv\"\n",
    "    with open(filename, 'w', newline = '') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(fields) # first writing fields\n",
    "        csv_writer.writerows(rows) # now the remaining record\n",
    "\n",
    "def update_triple_csv_file_with_concepts(re_type, model_used, language, given_concept_list):\n",
    "    path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "    os.chdir(path)\n",
    "\n",
    "    #  find_corresponding_eurovoc_concept(given_text, given_concept_list, given_stopwords_list):\n",
    "    data = pd.read_csv(\"kg_of_\" + re_type + \"_\" + language + \".csv\")\n",
    "    col_list = list(data.columns)\n",
    "    col_list.remove(\"Triple_ID\")\n",
    "    stopwords_list = []\n",
    "    if language == \"en\": stopwords_list = stopwords.words('english')\n",
    "    if language == \"fr\": stopwords_list = stopwords.words('french')\n",
    "    if language == \"de\": stopwords_list = stopwords.words('german')\n",
    "\n",
    "    for col in col_list:\n",
    "        new_col = col + \"_concept\"\n",
    "        data[new_col] = \"[ ]\"\n",
    "\n",
    "    for col in col_list:\n",
    "        new_col = col + \"_concept\"\n",
    "        for j in range(len(data.index)): # TODO change this by first getting the list, then appending it (for faster)\n",
    "            # row, column\n",
    "            # print(\"Found concept\")\n",
    "            # print(find_corresponding_eurovoc_concept(data.at[j, col], given_concept_list, stopwords_list))\n",
    "            # print()\n",
    "            data.at[j, new_col] = find_corresponding_eurovoc_concept(data.at[j, col], given_concept_list, stopwords_list)\n",
    "\n",
    "    # writing to csv file\n",
    "    file_name = path + \"\\\\kg_of_\" + re_type + \"_with_concepts_\" + language + \".csv\"\n",
    "    data.to_csv(file_name)\n",
    "\n",
    "def update_df_according_to_neo4j(re_type, model_used, language):\n",
    "    path = os.getcwd() + \"\\\\triples_data\\\\\" + re_type + \"\\\\\" + model_used\n",
    "    os.chdir(path)\n",
    "\n",
    "    #  find_corresponding_eurovoc_concept(given_text, given_concept_list, given_stopwords_list):\n",
    "    data = pd.read_csv(\"kg_of_\" + re_type + \"_with_concepts_\" + language + \".csv\")\n",
    "    col_list = [language + \"_Subject_concept\", language + \"_Predicate_concept\", language + \"_Object_concept\"]\n",
    "\n",
    "    for col in col_list:\n",
    "        list_of_col = list(data[col])\n",
    "        for j in range(len(data.index)):\n",
    "            if str(list_of_col[j]) == (\"[ ]\" or None): data.at[j, col] = \"-\"\n",
    "            else: data.at[j, col] = str(list_of_col[j].replace(\", \", \":\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\\"\", \"\").replace(\"'\", \"\"))\n",
    "\n",
    "    # writing to csv file\n",
    "    file_name = path + \"\\\\kg_of_\" + re_type + \"_with_concepts_\" + language + \".csv\"\n",
    "    data.to_csv(file_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T14:58:38.996457200Z",
     "start_time": "2023-06-06T14:58:38.902314400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [
    "my_path = \"C:\\\\Users\\\\dnaen\\\\PycharmProjects\\\\bachelor_thesis_23\\\\src\\\\main\"\n",
    "os.chdir(my_path)\n",
    "\n",
    "update_df_according_to_neo4j(\"predefined_dictionary\", \"stanford_OpenIE\", \"en\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T14:58:05.639372500Z",
     "start_time": "2023-06-06T14:58:01.094561800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"minimum standards are in place\"\n",
    "a = find_corresponding_eurovoc_concept(text, data_of_en, stopwords.words('english'))\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T10:57:08.658172400Z",
     "start_time": "2023-06-06T10:57:08.622129200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "data": {
      "text/plain": "                   en_Subject    en_Predicate  \\\n0                   legal aid   on council is   \n1           requested persons           is in   \n2      effectiveness provided       for under   \n3      effectiveness provided       for under   \n4      effectiveness provided       for under   \n...                       ...             ...   \n13276        article 17 entry  shall enter on   \n13277      article addressees              is   \n13278      article addressees              is   \n13279   article 18 addressees              is   \n13280   article 18 addressees              is   \n\n                                             en_Object  \\\n0             acting in ordinary legislative procedure   \n1                  european arrest warrant proceedings   \n2                                    directive 2013 48   \n3                          directive 2013 48 eu making   \n4      directive 2013 48 eu european parliament making   \n...                                                ...   \n13276                                    twentieth day   \n13277                                        addressed   \n13278                       addressed to member states   \n13279                       addressed to member states   \n13280                                        addressed   \n\n                             Triple_ID  \n0      directive_(eu)_2016_1919_en.txt  \n1      directive_(eu)_2016_1919_en.txt  \n2      directive_(eu)_2016_1919_en.txt  \n3      directive_(eu)_2016_1919_en.txt  \n4      directive_(eu)_2016_1919_en.txt  \n...                                ...  \n13276      directive_2013_48_eu_en.txt  \n13277      directive_2013_48_eu_en.txt  \n13278      directive_2013_48_eu_en.txt  \n13279      directive_2013_48_eu_en.txt  \n13280      directive_2013_48_eu_en.txt  \n\n[13281 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en_Subject</th>\n      <th>en_Predicate</th>\n      <th>en_Object</th>\n      <th>Triple_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>legal aid</td>\n      <td>on council is</td>\n      <td>acting in ordinary legislative procedure</td>\n      <td>directive_(eu)_2016_1919_en.txt</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>requested persons</td>\n      <td>is in</td>\n      <td>european arrest warrant proceedings</td>\n      <td>directive_(eu)_2016_1919_en.txt</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>effectiveness provided</td>\n      <td>for under</td>\n      <td>directive 2013 48</td>\n      <td>directive_(eu)_2016_1919_en.txt</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>effectiveness provided</td>\n      <td>for under</td>\n      <td>directive 2013 48 eu making</td>\n      <td>directive_(eu)_2016_1919_en.txt</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>effectiveness provided</td>\n      <td>for under</td>\n      <td>directive 2013 48 eu european parliament making</td>\n      <td>directive_(eu)_2016_1919_en.txt</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13276</th>\n      <td>article 17 entry</td>\n      <td>shall enter on</td>\n      <td>twentieth day</td>\n      <td>directive_2013_48_eu_en.txt</td>\n    </tr>\n    <tr>\n      <th>13277</th>\n      <td>article addressees</td>\n      <td>is</td>\n      <td>addressed</td>\n      <td>directive_2013_48_eu_en.txt</td>\n    </tr>\n    <tr>\n      <th>13278</th>\n      <td>article addressees</td>\n      <td>is</td>\n      <td>addressed to member states</td>\n      <td>directive_2013_48_eu_en.txt</td>\n    </tr>\n    <tr>\n      <th>13279</th>\n      <td>article 18 addressees</td>\n      <td>is</td>\n      <td>addressed to member states</td>\n      <td>directive_2013_48_eu_en.txt</td>\n    </tr>\n    <tr>\n      <th>13280</th>\n      <td>article 18 addressees</td>\n      <td>is</td>\n      <td>addressed</td>\n      <td>directive_2013_48_eu_en.txt</td>\n    </tr>\n  </tbody>\n</table>\n<p>13281 rows √ó 4 columns</p>\n</div>"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_of_en = pd.read_csv('kg_of_predefined_dictionary_en.csv')\n",
    "data_of_en"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T10:14:03.443261800Z",
     "start_time": "2023-06-06T10:14:03.375764100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "# your own project path dir here\n",
    "my_path = \"C:\\\\Users\\\\dnaen\\\\PycharmProjects\\\\bachelor_thesis_23\"\n",
    "lan = \"fr\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T13:05:42.025443600Z",
     "start_time": "2023-06-06T13:05:42.006813Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.Importing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "# this has to be ran only once, because it creates the eurovoc_en.tsv file (which should already be there)\n",
    "# create_tsv_of_language(\"en\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T13:05:43.688826300Z",
     "start_time": "2023-06-06T13:05:43.670345600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnaen\\PycharmProjects\\bachelor_thesis_23\\src\\main\n"
     ]
    }
   ],
   "source": [
    "# to make sure that we are in the original working directory\n",
    "data_path = my_path + \"\\\\src\\\\main\"\n",
    "os.chdir(data_path)\n",
    "print(os.getcwd()) # this should return something like \"...\\src\\main\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T13:05:44.649957100Z",
     "start_time": "2023-06-06T13:05:44.628610100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eurovoc imported\n"
     ]
    }
   ],
   "source": [
    "tsv_file = \"eurovoc_\" + lan + \".tsv\"\n",
    "\n",
    "# getting info of ids and concepts from the tsv file\n",
    "eurovoc_dic, eurovoc_reverse_dic, id_list, concept_list = tsv_dic_processing(tsv_file)\n",
    "print('Eurovoc imported')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T09:21:13.501703800Z",
     "start_time": "2023-06-06T09:21:13.472173900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx8G -cp C:\\Users\\dnaen\\.stanfordnlp_resources\\stanford-corenlp-4.5.3/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-b4e1cc12a51c4487.props -preload openie\n"
     ]
    }
   ],
   "source": [
    "text = \"Can the prosecution guarantee the protection of suspect's procedural rights regarding the specific mechanisms and common minimum standards that are in place, such as the Charter of Fundamental Rights and the European Convention on Human Rights?\"\n",
    "\n",
    "# \"Which conclusions adopted a programme of measures to implement the principle of mutual recognition of decision in criminal matters?\"\n",
    "\n",
    "\n",
    "properties = {\n",
    "    'openie.affinity_probability_cap': 2 / 3,\n",
    "}\n",
    "\n",
    "list_of_triples = []\n",
    "\n",
    "with StanfordOpenIE(properties=properties) as client: # opening the server\n",
    "    current_triple_list_of_sentence = get_triples_stanford_openie(client, text) # returns [subject, relation, object]\n",
    "    if current_triple_list_of_sentence: # only run the loop if list not empty\n",
    "        index = 0\n",
    "        for current_triple in current_triple_list_of_sentence: # a sentence can have multiple triples\n",
    "            list_of_triples.append(current_triple)\n",
    "            index += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T08:18:39.449020700Z",
     "start_time": "2023-06-06T08:18:28.356144700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "temp_triples = list_of_triples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T08:18:49.051385400Z",
     "start_time": "2023-06-06T08:18:49.029840800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple\n",
      "['legal aid', 'on COUNCIL is', 'Acting in ordinary legislative procedure']\n",
      "Found concepts\n",
      "['legality', 'legal aid'], [], ['subcontracting', 'legislative procedure'], \n",
      "\n",
      "Triple\n",
      "['requested persons', 'is in', 'European arrest warrant proceedings']\n",
      "Found concepts\n",
      "[], [], ['arrest'], \n",
      "\n",
      "Triple\n",
      "['effectiveness provided', 'for under', 'Directive 2013 48']\n",
      "Found concepts\n",
      "[], [], ['directive'], \n",
      "\n",
      "Triple\n",
      "['effectiveness provided', 'for under', 'Directive 2013 48 EU making']\n",
      "Found concepts\n",
      "[], [], ['directive', 'Ceuta', 'entrepreneur', 'entrepreneurship', 'eugenics', 'euro', 'euthanasia', 'eutrophication', 'liqueur', 'museum', 'neurobiology', 'neurology', 'neutrality', 'pasteurisation', 'petroleum', 'Reunion', 'therapeutics', 'bread-making', 'decision-making', 'pastry-making'], \n",
      "\n",
      "Triple\n",
      "['effectiveness provided', 'for under', 'Directive 2013 48 EU European Parliament making']\n",
      "Found concepts\n",
      "[], [], ['directive', 'Ceuta', 'entrepreneur', 'entrepreneurship', 'eugenics', 'euro', 'euthanasia', 'eutrophication', 'liqueur', 'museum', 'neurobiology', 'neurology', 'neutrality', 'pasteurisation', 'petroleum', 'Reunion', 'therapeutics', 'parliament', 'bread-making', 'decision-making', 'pastry-making'], \n",
      "\n",
      "Triple\n",
      "['EUROPEAN PARLIAMENT', 'criminal proceedings', 'COUNCIL']\n",
      "Found concepts\n",
      "['parliament'], ['criminal proceedings'], [], \n",
      "\n",
      "Triple\n",
      "['Article 82 2', 'of point is', 'b']\n",
      "Found concepts\n",
      "[], [], ['abortion', 'Abruzzi', 'absenteeism', 'abstentionism', 'admissibility', 'Albania', 'antibiotic', 'arbitrage', 'arbitration', 'arboriculture', 'Aruba', 'asbestos', 'Azerbaijan', 'Baden-Wurttemberg', 'bailiff', 'bakery', 'bank', 'banking', 'bankruptcy', 'bar', 'Barbados', 'barge', 'barley', 'barrister', 'barter', 'bauxite', 'bearing', 'beef', 'beer', 'benchmarking', 'beryllium', 'beverage', 'bibliography', 'bilingualism', 'bio-ethics', 'bio-industry', 'biochemistry', 'bioclimatology', 'bioconversion', 'biodegradability', 'biodiversity', 'bioenergy', 'biogas', 'biography', 'biology', 'biomass', 'biomaterials', 'biometrics', 'bioprocess', 'biosphere', 'biotechnology', 'biotope', 'bipolarisation', 'bird', 'births', 'bismuth', 'boiler', 'bomber', 'bond', 'bookshop', 'borrowing', 'botany', 'bottling', 'branch', 'Brandenburg', 'bread', 'bread-making', 'breadwinner', 'brick', 'broadcasting', 'broker', 'bromine', 'browser', 'brucellosis', 'buckwheat', 'budget', 'building', 'bull', 'bus', 'butane', 'butter', 'by-catch', 'by-election', 'by-product', 'Calabria', 'Cambodia', 'Cantabria', 'carbon', 'cobalt', 'cohabitation', 'Colombia', 'Cuba', 'cybernetics', 'database', 'debt', 'diabetes', 'distributor', 'Djibouti', 'eco-label', 'embassy', 'establishment', 'Euribor', 'Eurobond', 'ferryboat', 'Gabon', 'Gambia', 'Gibraltar', 'globalisation', 'haberdashery', 'habitat', 'Hamburg', 'herbicide', 'hydrocarbon', 'icebreaker', 'incompatibility', 'incunabula', 'indebtedness', 'Kiribati', 'labelling', 'Lebanon', 'liability', 'Liberalism', 'Liberec', 'Liberia', 'library', 'Libya', 'Limburg', 'Lombardy', 'lubricants', 'Luxembourg', 'Maghreb', 'molybdenum', 'Mozambique', 'Namibia', 'neurobiology', 'Obalno-kraska', 'observation', 'observer', 'paperboard', 'Pardubice', 'plebiscite', 'prefabrication', 'profitability', 'publication', 'publisher', 'publishing', 'rabbit', 'rabies', 'radiobiology', 'republic', 'robotics', 'robotisation', 'Saba', 'Salzburg', 'sea-bed', 'Serbia', 'shipbuilding', 'Stabex', 'sub-proletariat', 'subcontracting', 'submarine', 'subsidiary', 'tobacco', 'traceability', 'tube', 'turbine', 'Umbria', 'UN-Habitat', 'urbanisation', 'Uzbekistan', 'vegetable', 'Vorarlberg', 'waveband', 'xenophobia', 'yearbook', 'Zambia', 'Zimbabwe'], \n",
      "\n",
      "Triple\n",
      "['regard', 'Having', 'After consulting']\n",
      "Found concepts\n",
      "[], [], [], \n",
      "\n",
      "Triple\n",
      "['EUROPEAN PARLIAMENT', 'persons', 'COUNCIL']\n",
      "Found concepts\n",
      "['parliament'], [], [], \n",
      "\n",
      "Triple\n",
      "['suspects', 'is in', 'criminal proceedings']\n",
      "Found concepts\n",
      "[], [], ['criminal proceedings'], \n",
      "\n",
      "Triple\n",
      "['EUROPEAN PARLIAMENT', 'persons', 'COUNCIL']\n",
      "Found concepts\n",
      "['parliament'], [], [], \n",
      "\n",
      "Triple\n",
      "['EUROPEAN PARLIAMENT', 'requested persons', 'COUNCIL']\n",
      "Found concepts\n",
      "['parliament'], [], [], \n",
      "\n",
      "Triple\n",
      "['purpose', 'effectiveness of', 'right']\n",
      "Found concepts\n",
      "[], [], ['copyright', 'Euroright'], \n",
      "\n",
      "Triple\n",
      "['regard', 'Having regard', 'consulting']\n",
      "Found concepts\n",
      "[], [], [], \n",
      "\n",
      "Triple\n",
      "['regard', 'Having', '1']\n",
      "Found concepts\n",
      "[], [], [], \n",
      "\n",
      "Triple\n",
      "['regard', 'Having', 'regard to opinion']\n",
      "Found concepts\n",
      "[], [], ['opinion'], \n",
      "\n",
      "Triple\n",
      "['EUROPEAN PARLIAMENT', 'proceedings', 'COUNCIL']\n",
      "Found concepts\n",
      "['parliament'], [], [], \n",
      "\n",
      "Triple\n",
      "['purpose', 'effectiveness of', 'right of access']\n",
      "Found concepts\n",
      "[], [], ['copyright', 'Euroright'], \n",
      "\n",
      "Triple\n",
      "['purpose', 'effectiveness of', 'right of access to lawyer']\n",
      "Found concepts\n",
      "[], [], ['copyright', 'Euroright'], \n",
      "\n",
      "Triple\n",
      "['regard', 'Having', 'regard']\n",
      "Found concepts\n",
      "[], [], [], \n",
      "\n",
      "Triple\n",
      "['regard', 'Having', 'consulting']\n",
      "Found concepts\n",
      "[], [], [], \n",
      "\n",
      "Triple\n",
      "['effectiveness provided', 'for under', 'Directive 2013 48 EU']\n",
      "Found concepts\n",
      "[], [], ['directive', 'Ceuta', 'entrepreneur', 'entrepreneurship', 'eugenics', 'euro', 'euthanasia', 'eutrophication', 'liqueur', 'museum', 'neurobiology', 'neurology', 'neutrality', 'pasteurisation', 'petroleum', 'Reunion', 'therapeutics'], \n",
      "\n",
      "Triple\n",
      "['effectiveness provided', 'for under', 'Directive 2013 48 EU European Parliament']\n",
      "Found concepts\n",
      "[], [], ['directive', 'Ceuta', 'entrepreneur', 'entrepreneurship', 'eugenics', 'euro', 'euthanasia', 'eutrophication', 'liqueur', 'museum', 'neurobiology', 'neurology', 'neutrality', 'pasteurisation', 'petroleum', 'Reunion', 'therapeutics', 'parliament'], \n",
      "\n",
      "Triple\n",
      "['COUNCIL', 'Acting in', 'ordinary legislative procedure']\n",
      "Found concepts\n",
      "[], ['subcontracting'], ['legislative procedure'], \n",
      "\n",
      "Triple\n",
      "['26 October 2016', 'of COUNCIL is', 'Acting in ordinary legislative procedure']\n",
      "Found concepts\n",
      "[], [], ['subcontracting', 'legislative procedure'], \n",
      "\n",
      "Triple\n",
      "['Council Framework Decision 2002 584', 'pursuant proceedings is', 'requested persons']\n",
      "Found concepts\n",
      "['decision', 'decision-making', 'framework decision'], [], [], \n",
      "\n",
      "Triple\n",
      "['opinion', 'to regard is', '1']\n",
      "Found concepts\n",
      "['opinion'], [], [], \n",
      "\n",
      "Triple\n",
      "['regard', 'Having', 'regard to opinion of European Economic Social Committee']\n",
      "Found concepts\n",
      "[], [], ['opinion', 'economics', 'macroeconomics', 'microeconomics', 'socialism'], \n",
      "\n",
      "Triple\n",
      "['regard', 'Having regard', 'After consulting']\n",
      "Found concepts\n",
      "[], [], [], \n",
      "\n",
      "Triple\n",
      "['effectiveness provided', 'for under', 'Directive 2013 48 making']\n",
      "Found concepts\n",
      "[], [], ['directive', 'bread-making', 'decision-making', 'pastry-making'], \n",
      "\n",
      "Triple\n",
      "['trust', 'thus improve in', 'matters']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'improve recognition in', 'criminal matters']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'strengthen', 'trust']\n",
      "Found concepts\n",
      "['directive'], [], ['trust'], \n",
      "\n",
      "Triple\n",
      "['trust', 'improve', 'mutual recognition']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'aims By', 'minimum rules concerning']\n",
      "Found concepts\n",
      "['directive'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'thus improve recognition in', 'matters']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'aims By', 'establishing common minimum rules concerning']\n",
      "Found concepts\n",
      "['directive'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'thus improve', 'recognition of decisions']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'aims', 'to strengthen']\n",
      "Found concepts\n",
      "['directive'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'improve in', 'matters']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'strengthen', 'trust of Member States']\n",
      "Found concepts\n",
      "['directive'], [], ['trust'], \n",
      "\n",
      "Triple\n",
      "['trust', 'improve', 'mutual recognition of decisions']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'thus improve recognition in', 'criminal matters']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'thus improve', 'recognition']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'improve recognition in', 'matters']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'improve in', 'criminal matters']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'thus improve', 'mutual recognition of decisions']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'aims', 'strengthen']\n",
      "Found concepts\n",
      "['directive'], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'aims By', 'establishing minimum rules concerning']\n",
      "Found concepts\n",
      "['directive'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'improve', 'recognition']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'aims By', 'common minimum rules']\n",
      "Found concepts\n",
      "['directive'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'thus improve in', 'criminal matters']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'improve', 'recognition of decisions']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'aims By', 'common minimum rules concerning']\n",
      "Found concepts\n",
      "['directive'], [], [], \n",
      "\n",
      "Triple\n",
      "['trust', 'thus improve', 'mutual recognition']\n",
      "Found concepts\n",
      "['trust'], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'aims By', 'minimum rules']\n",
      "Found concepts\n",
      "['directive'], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'aims By', 'establishing common minimum rules']\n",
      "Found concepts\n",
      "['directive'], [], [], \n",
      "\n",
      "Triple\n",
      "['other', 'in', 'criminal justice systems']\n",
      "Found concepts\n",
      "[], [], [], \n",
      "\n",
      "Triple\n",
      "['Directive', 'aims By', 'establishing minimum rules']\n",
      "Found concepts\n",
      "['directive'], [], [], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'Convention for Protection of Human Rights']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['protectionism', 'human rights'], \n",
      "\n",
      "Triple\n",
      "['Article 47', 'of paragraph is', 'c']\n",
      "Found concepts\n",
      "[], [], ['account', 'accountant', 'accounting', 'acculturation', 'acid', 'acidification', 'acoustics', 'advance', 'aerodynamics', 'Africa', 'aircraft', 'alcohol', 'alcoholism', 'Alsace', 'America', 'anarchism', 'Anglicanism', 'Antarctica', 'antibiotic', 'apiculture', 'apprentice', 'apprenticeship', 'aquaculture', 'arboriculture', 'archaeology', 'archipelago', 'architecture', 'archives', 'Arctic', 'association', 'astronautics', 'Attica', 'bankruptcy', 'Basilicata', 'benchmarking', 'bio-ethics', 'biochemistry', 'bioclimatology', 'bioconversion', 'biometrics', 'bioprocess', 'biotechnology', 'branch', 'brick', 'broadcasting', 'brucellosis', 'buckwheat', 'by-catch', 'by-election', 'by-product', 'cadmium', 'calcium', 'calf', 'camping', 'cancer', 'candidate', 'cannery', 'carbon', 'carcase', 'Caricom', 'carpet', 'carrier', 'cartel', 'cartography', 'case-law', 'cassava', 'cast-iron', 'catalogue', 'cataloguing', 'catering', 'Catholicism', 'cattle', 'cease-fire', 'cellulose', 'cement', 'cemetery', 'Cenelec', 'censorship', 'census', 'ceramics', 'cereal-growing', 'cereals', 'cervidae', 'champagne', 'chaptalisation', 'charcoal', 'charge', 'chartering', 'cheese', 'chemistry', 'cheque', 'child', 'chlorine', 'chromium', 'church', 'cider', 'cindynics', 'cinema', 'circular', 'civics', 'civilisation', 'claim', 'classification', 'clergy', 'climate', 'climatology', 'cloning', 'clothing', 'clover', 'co-determination', 'co-financing', 'co-insurance', 'coal', 'cobalt', 'cocoa', 'coding', 'coffee', 'cohabitation', 'coke', 'collectivism', 'colonialism', 'Comecon', 'comitology', 'commemoration', 'commuting', 'competition', 'competitiveness', 'composition', 'computer', 'concessionaire', 'concrete', 'condiment', 'confectionery', 'confidentiality', 'conglomerate', 'conifer', 'Connacht', 'conservatism', 'consortium', 'constitution', 'consulate', 'consultancy', 'consumer', 'consumption', 'container', 'contraception', 'contract', 'cooperative', 'copper', 'copyright', 'cork', 'corporatism', 'correspondence', 'corrosion', 'corruption', 'Corsica', 'cosmology', 'costing', 'cotton', 'countertrade', 'county', 'cow', 'craftsman', 'cream', 'credit', 'crew', 'crime', 'criminology', 'crustacean', 'cryptography', 'culture', 'Curacao', 'custody', 'customers', 'customs', 'cybernetics', 'cyclone', 'cytology', 'Czechoslovakia', 'decentralisation', 'decision', 'decision-making', 'decolonisation', 'decontamination', 'decree', 'delinquency', 'democracy', 'democratisation', 'depoliticisation', 'descendant', 'desertification', 'dictatorship', 'dictionary', 'directive', 'directory', 'discounting', 'dissidence', 'divorce', 'doctor', 'document', 'documentation', 'Dominica', 'eco-label', 'Ecofin', 'ecologism', 'ecology', 'econometrics', 'economics', 'economy', 'Ecosoc', 'ecosystem', 'Ecu', 'Ecuador', 'education', 'election', 'electorate', 'electrochemistry', 'electrometallurgy', 'electronics', 'electrotechnology', 'emancipation', 'encyclopaedia', 'epidemic', 'ergonomics', 'ethics', 'eugenics', 'Eurocommunism', 'Eurocontrol', 'Eurocorps', 'Eurocredit', 'Eurocurrency', 'Eurydice', 'eutrophication', 'evidence', 'ex-serviceman', 'executive', 'facsimile', 'Fascism', 'financing', 'floriculture', 'France', 'Franche-Comte', 'franchising', 'furnace', 'Galicia', 'genetics', 'geochemistry', 'geophysics', 'glucose', 'governance', 'Greece', 'gynaecology', 'handicrafts', 'helicopter', 'herbicide', 'homicide', 'horticulture', 'hydrocarbon', 'hydroponics', 'ice', 'icebreaker', 'Iceland', 'Ile-de-France', 'illiteracy', 'income', 'incompatibility', 'incorporation', 'incoterms', 'incunabula', 'indemnification', 'infancy', 'inheritance', 'injunction', 'insect', 'insecticide', 'insurance', 'intellectual', 'interactivity', 'interference', 'invoicing', 'isoglucose', 'Jamaica', 'jurisdiction', 'lactose', 'Languedoc-Roussillon', 'legitimacy', 'Liberec', 'Liechtenstein', 'linguistics', 'livestock', 'lockout', 'logistics', 'lubricants', 'lucerne', 'Macao', 'Macedonia', 'machinery', 'macroeconomics', 'Madagascar', 'maintenance', 'manuscript', 'Marches', 'mathematics', 'mechanisation', 'medicament', 'medicine', 'mendicity', 'mercenary', 'merchandising', 'merchant', 'Mercosur', 'mercury', 'Mexico', 'micro-computer', 'micro-enterprise', 'microeconomics', 'microelectronics', 'microfinance', 'microform', 'microloan', 'Micronesia', 'microorganism', 'mnemonics', 'mollusc', 'Moluccas', 'Monaco', 'monocracy', 'Morocco', 'municipality', 'music', 'nanotechnology', 'narcotic', 'Nicaragua', 'nickel', 'nomenclature', 'non-violence', 'nuisance', 'ocean', 'Oceania', 'oceanography', 'offence', 'oligarchy', 'Olomouc', 'optics', 'orchard', 'ordinance', 'outplacement', 'outsourcing', 'over-production', 'pacifism', 'packaging', 'paediatrics', 'Pardubice', 'peace', 'peacekeeping', 'pesticide', 'petrochemicals', 'pharmacist', 'pharmacology', 'pharmacy', 'photochemistry', 'Picardy', 'piracy', 'plasticiser', 'plastics', 'plebiscite', 'police', 'politician', 'politics', 'post-communism', 'pre-packaging', 'prefabrication', 'prices', 'production', 'productivity', 'protectionism', 'protocol', 'province', 'psychiatry', 'psychoanalysis', 'psychology', 'publication', 'purchase', 'racism', 'radioactivity', 'receivership', 'recommendation', 'record', 'recording', 'recruitment', 'rediscounting', 'redundancy', 'reinsurance', 'reparcelling', 'repentance', 'republic', 'research', 'residence', 'rice', 'robotics', 'scanner', 'Schleswig-Holstein', 'schooling', 'schoolwork', 'Scotland', 'screen', 'sculpture', 'search', 'section', 'secularity', 'securities', 'self-defence', 'self-financing', 'sericulture', 'service', 'Seychelles', 'Sicily', 'silviculture', 'socialism', 'sociology', 'spectrometry', 'speech', 'starch', 'statistics', 'stock', 'Stockholm', 'subcontracting', 'sucrose', 'suicide', 'surveillance', 'Taric', 'teacher', 'teaching', 'technology', 'telecommunications', 'telematics', 'telemedicine', 'therapeutics', 'thermodynamics', 'ticket', 'tobacco', 'toxicology', 'traceability', 'tractor', 'triticale', 'Tuscany', 'underproduction', 'Unesco', 'Unicef', 'usufruct', 'Utrecht', 'vaccination', 'vaccine', 'vehicle', 'victim', 'vinification', 'violence', 'viticulture', 'volcanology', 'Vysocina', 'watercourse', 'workplace', 'zinc', 'zootechnics'], \n",
      "\n",
      "Triple\n",
      "['European Convention', 'of paragraph is', 'c']\n",
      "Found concepts\n",
      "[], [], ['account', 'accountant', 'accounting', 'acculturation', 'acid', 'acidification', 'acoustics', 'advance', 'aerodynamics', 'Africa', 'aircraft', 'alcohol', 'alcoholism', 'Alsace', 'America', 'anarchism', 'Anglicanism', 'Antarctica', 'antibiotic', 'apiculture', 'apprentice', 'apprenticeship', 'aquaculture', 'arboriculture', 'archaeology', 'archipelago', 'architecture', 'archives', 'Arctic', 'association', 'astronautics', 'Attica', 'bankruptcy', 'Basilicata', 'benchmarking', 'bio-ethics', 'biochemistry', 'bioclimatology', 'bioconversion', 'biometrics', 'bioprocess', 'biotechnology', 'branch', 'brick', 'broadcasting', 'brucellosis', 'buckwheat', 'by-catch', 'by-election', 'by-product', 'cadmium', 'calcium', 'calf', 'camping', 'cancer', 'candidate', 'cannery', 'carbon', 'carcase', 'Caricom', 'carpet', 'carrier', 'cartel', 'cartography', 'case-law', 'cassava', 'cast-iron', 'catalogue', 'cataloguing', 'catering', 'Catholicism', 'cattle', 'cease-fire', 'cellulose', 'cement', 'cemetery', 'Cenelec', 'censorship', 'census', 'ceramics', 'cereal-growing', 'cereals', 'cervidae', 'champagne', 'chaptalisation', 'charcoal', 'charge', 'chartering', 'cheese', 'chemistry', 'cheque', 'child', 'chlorine', 'chromium', 'church', 'cider', 'cindynics', 'cinema', 'circular', 'civics', 'civilisation', 'claim', 'classification', 'clergy', 'climate', 'climatology', 'cloning', 'clothing', 'clover', 'co-determination', 'co-financing', 'co-insurance', 'coal', 'cobalt', 'cocoa', 'coding', 'coffee', 'cohabitation', 'coke', 'collectivism', 'colonialism', 'Comecon', 'comitology', 'commemoration', 'commuting', 'competition', 'competitiveness', 'composition', 'computer', 'concessionaire', 'concrete', 'condiment', 'confectionery', 'confidentiality', 'conglomerate', 'conifer', 'Connacht', 'conservatism', 'consortium', 'constitution', 'consulate', 'consultancy', 'consumer', 'consumption', 'container', 'contraception', 'contract', 'cooperative', 'copper', 'copyright', 'cork', 'corporatism', 'correspondence', 'corrosion', 'corruption', 'Corsica', 'cosmology', 'costing', 'cotton', 'countertrade', 'county', 'cow', 'craftsman', 'cream', 'credit', 'crew', 'crime', 'criminology', 'crustacean', 'cryptography', 'culture', 'Curacao', 'custody', 'customers', 'customs', 'cybernetics', 'cyclone', 'cytology', 'Czechoslovakia', 'decentralisation', 'decision', 'decision-making', 'decolonisation', 'decontamination', 'decree', 'delinquency', 'democracy', 'democratisation', 'depoliticisation', 'descendant', 'desertification', 'dictatorship', 'dictionary', 'directive', 'directory', 'discounting', 'dissidence', 'divorce', 'doctor', 'document', 'documentation', 'Dominica', 'eco-label', 'Ecofin', 'ecologism', 'ecology', 'econometrics', 'economics', 'economy', 'Ecosoc', 'ecosystem', 'Ecu', 'Ecuador', 'education', 'election', 'electorate', 'electrochemistry', 'electrometallurgy', 'electronics', 'electrotechnology', 'emancipation', 'encyclopaedia', 'epidemic', 'ergonomics', 'ethics', 'eugenics', 'Eurocommunism', 'Eurocontrol', 'Eurocorps', 'Eurocredit', 'Eurocurrency', 'Eurydice', 'eutrophication', 'evidence', 'ex-serviceman', 'executive', 'facsimile', 'Fascism', 'financing', 'floriculture', 'France', 'Franche-Comte', 'franchising', 'furnace', 'Galicia', 'genetics', 'geochemistry', 'geophysics', 'glucose', 'governance', 'Greece', 'gynaecology', 'handicrafts', 'helicopter', 'herbicide', 'homicide', 'horticulture', 'hydrocarbon', 'hydroponics', 'ice', 'icebreaker', 'Iceland', 'Ile-de-France', 'illiteracy', 'income', 'incompatibility', 'incorporation', 'incoterms', 'incunabula', 'indemnification', 'infancy', 'inheritance', 'injunction', 'insect', 'insecticide', 'insurance', 'intellectual', 'interactivity', 'interference', 'invoicing', 'isoglucose', 'Jamaica', 'jurisdiction', 'lactose', 'Languedoc-Roussillon', 'legitimacy', 'Liberec', 'Liechtenstein', 'linguistics', 'livestock', 'lockout', 'logistics', 'lubricants', 'lucerne', 'Macao', 'Macedonia', 'machinery', 'macroeconomics', 'Madagascar', 'maintenance', 'manuscript', 'Marches', 'mathematics', 'mechanisation', 'medicament', 'medicine', 'mendicity', 'mercenary', 'merchandising', 'merchant', 'Mercosur', 'mercury', 'Mexico', 'micro-computer', 'micro-enterprise', 'microeconomics', 'microelectronics', 'microfinance', 'microform', 'microloan', 'Micronesia', 'microorganism', 'mnemonics', 'mollusc', 'Moluccas', 'Monaco', 'monocracy', 'Morocco', 'municipality', 'music', 'nanotechnology', 'narcotic', 'Nicaragua', 'nickel', 'nomenclature', 'non-violence', 'nuisance', 'ocean', 'Oceania', 'oceanography', 'offence', 'oligarchy', 'Olomouc', 'optics', 'orchard', 'ordinance', 'outplacement', 'outsourcing', 'over-production', 'pacifism', 'packaging', 'paediatrics', 'Pardubice', 'peace', 'peacekeeping', 'pesticide', 'petrochemicals', 'pharmacist', 'pharmacology', 'pharmacy', 'photochemistry', 'Picardy', 'piracy', 'plasticiser', 'plastics', 'plebiscite', 'police', 'politician', 'politics', 'post-communism', 'pre-packaging', 'prefabrication', 'prices', 'production', 'productivity', 'protectionism', 'protocol', 'province', 'psychiatry', 'psychoanalysis', 'psychology', 'publication', 'purchase', 'racism', 'radioactivity', 'receivership', 'recommendation', 'record', 'recording', 'recruitment', 'rediscounting', 'redundancy', 'reinsurance', 'reparcelling', 'repentance', 'republic', 'research', 'residence', 'rice', 'robotics', 'scanner', 'Schleswig-Holstein', 'schooling', 'schoolwork', 'Scotland', 'screen', 'sculpture', 'search', 'section', 'secularity', 'securities', 'self-defence', 'self-financing', 'sericulture', 'service', 'Seychelles', 'Sicily', 'silviculture', 'socialism', 'sociology', 'spectrometry', 'speech', 'starch', 'statistics', 'stock', 'Stockholm', 'subcontracting', 'sucrose', 'suicide', 'surveillance', 'Taric', 'teacher', 'teaching', 'technology', 'telecommunications', 'telematics', 'telemedicine', 'therapeutics', 'thermodynamics', 'ticket', 'tobacco', 'toxicology', 'traceability', 'tractor', 'triticale', 'Tuscany', 'underproduction', 'Unesco', 'Unicef', 'usufruct', 'Utrecht', 'vaccination', 'vaccine', 'vehicle', 'victim', 'vinification', 'violence', 'viticulture', 'volcanology', 'Vysocina', 'watercourse', 'workplace', 'zinc', 'zootechnics'], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'European Convention']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], [], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'European Convention for Protection']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['protectionism'], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'Convention for Protection of Human Rights']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['protectionism', 'human rights'], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'Convention']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], [], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'Article 47 of Charter of Fundamental Rights of European Union']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering', 'Reunion'], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'Article 47']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], [], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'Convention for Protection']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['protectionism'], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'Article 47 Charter of Fundamental Rights of European Union']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering', 'Reunion'], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'Convention for Protection']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['protectionism'], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'European Convention for Protection']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['protectionism'], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'Article 47 Charter']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering'], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'Article 47 of Charter of Fundamental Rights of European Union']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering', 'Reunion'], \n",
      "\n",
      "Triple\n",
      "['legal aid', 'is in', 'criminal proceedings']\n",
      "Found concepts\n",
      "['legality', 'legal aid'], [], ['criminal proceedings'], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'European Convention']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], [], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'Article 47']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], [], \n",
      "\n",
      "Triple\n",
      "['Article 47', 'of paragraph is', 'Charter']\n",
      "Found concepts\n",
      "[], [], ['chartering'], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'Article 47 Charter of Fundamental Rights']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering'], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'Article 47 Charter of Fundamental Rights of European Union']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering', 'Reunion'], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'Article 47 of Charter']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering'], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'Article 47 Charter of Fundamental Rights']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering'], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'Article 47 of Charter']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering'], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'Convention']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], [], \n",
      "\n",
      "Triple\n",
      "['International Covenant', 'of Article is', 'd']\n",
      "Found concepts\n",
      "[], [], [], \n",
      "\n",
      "Triple\n",
      "['European Convention', 'of paragraph is', 'Charter']\n",
      "Found concepts\n",
      "[], [], ['chartering'], \n",
      "\n",
      "Triple\n",
      "['right', 'third paragraph of', 'Article 47 of Charter of Fundamental Rights']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering'], \n",
      "\n",
      "Triple\n",
      "['European Convention', 'of paragraph is', 'Article 6 3']\n",
      "Found concepts\n",
      "[], [], [], \n",
      "\n",
      "Triple\n",
      "['Article 47', 'of paragraph is', 'Article 6 3']\n",
      "Found concepts\n",
      "[], [], [], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'Article 47 of Charter of Fundamental Rights']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering'], \n",
      "\n",
      "Triple\n",
      "['right', 'paragraph of', 'Article 47 Charter']\n",
      "Found concepts\n",
      "['copyright', 'Euroright'], [], ['chartering'], \n",
      "\n",
      "Triple\n",
      "['Charter', 'has', 'same legal value as Treaties']\n",
      "Found concepts\n",
      "['chartering'], [], ['legality'], \n",
      "\n",
      "Triple\n",
      "['Charter', 'has', 'legal value as Treaties']\n",
      "Found concepts\n",
      "['chartering'], [], ['legality'], \n",
      "\n",
      "Triple\n",
      "['Charter', 'has', 'same value']\n",
      "Found concepts\n",
      "['chartering'], [], [], \n",
      "\n",
      "Triple\n",
      "['Member States', 'are', 'parties']\n",
      "Found concepts\n",
      "[], [], [], \n",
      "\n",
      "Triple\n",
      "['Charter', 'has', 'value as Treaties']\n",
      "Found concepts\n",
      "['chartering'], [], [], \n",
      "\n",
      "Triple\n",
      "['Charter', 'has', 'same value as Treaties']\n",
      "Found concepts\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[212], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound concepts\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m elem \u001B[38;5;129;01min\u001B[39;00m triple:\n\u001B[1;32m----> 8\u001B[0m     a \u001B[38;5;241m=\u001B[39m find_corresponding_eurovoc_concept(elem, concept_list, \u001B[43mstopwords\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43menglish\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(a, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001B[0m, in \u001B[0;36mWordListCorpusReader.words\u001B[1;34m(self, fileids, ignore_lines_startswith)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwords\u001B[39m(\u001B[38;5;28mself\u001B[39m, fileids\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, ignore_lines_startswith\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m     20\u001B[0m         line\n\u001B[1;32m---> 21\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m line_tokenize(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfileids\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     22\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m line\u001B[38;5;241m.\u001B[39mstartswith(ignore_lines_startswith)\n\u001B[0;32m     23\u001B[0m     ]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001B[0m, in \u001B[0;36mCorpusReader.raw\u001B[1;34m(self, fileids)\u001B[0m\n\u001B[0;32m    216\u001B[0m contents \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m fileids:\n\u001B[1;32m--> 218\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[0;32m    219\u001B[0m         contents\u001B[38;5;241m.\u001B[39mappend(fp\u001B[38;5;241m.\u001B[39mread())\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concat(contents)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001B[0m, in \u001B[0;36mCorpusReader.open\u001B[1;34m(self, file)\u001B[0m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;124;03mReturn an open stream that can be used to read the given file.\u001B[39;00m\n\u001B[0;32m    225\u001B[0m \u001B[38;5;124;03mIf the file's encoding is not None, then the stream will\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;124;03m:param file: The file identifier of the file to read.\u001B[39;00m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    230\u001B[0m encoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding(file)\n\u001B[1;32m--> 231\u001B[0m stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_root\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mopen(encoding)\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stream\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:334\u001B[0m, in \u001B[0;36mFileSystemPathPointer.join\u001B[1;34m(self, fileid)\u001B[0m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mjoin\u001B[39m(\u001B[38;5;28mself\u001B[39m, fileid):\n\u001B[0;32m    333\u001B[0m     _path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_path, fileid)\n\u001B[1;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mFileSystemPathPointer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py:41\u001B[0m, in \u001B[0;36mpy3_data.<locals>._decorator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_decorator\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     40\u001B[0m     args \u001B[38;5;241m=\u001B[39m (args[\u001B[38;5;241m0\u001B[39m], add_py3_data(args[\u001B[38;5;241m1\u001B[39m])) \u001B[38;5;241m+\u001B[39m args[\u001B[38;5;241m2\u001B[39m:]\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minit_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:311\u001B[0m, in \u001B[0;36mFileSystemPathPointer.__init__\u001B[1;34m(self, _path)\u001B[0m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;124;03mCreate a new path pointer for the given absolute path.\u001B[39;00m\n\u001B[0;32m    306\u001B[0m \n\u001B[0;32m    307\u001B[0m \u001B[38;5;124;03m:raise IOError: If the given path does not exist.\u001B[39;00m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    310\u001B[0m _path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(_path)\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexists\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_path\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo such file or directory: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m _path)\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_path \u001B[38;5;241m=\u001B[39m _path\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\genericpath.py:19\u001B[0m, in \u001B[0;36mexists\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 19\u001B[0m     \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "save = []\n",
    "\n",
    "for triple in list_of_triples:\n",
    "    print(\"Triple\")\n",
    "    print(triple)\n",
    "    print(\"Found concepts\")\n",
    "    for elem in triple:\n",
    "        a = find_corresponding_eurovoc_concept(elem, concept_list, stopwords.words('english'))\n",
    "        print(a, end = \", \")\n",
    "\n",
    "    print()\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T10:56:13.567863100Z",
     "start_time": "2023-06-06T10:55:57.331343Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Directive_(EU)_2016_1919_en.txt ...\n",
      "importing Directive_(EU)_2016_343_en.txt ...\n",
      "importing Directive_(EU)_2016_800_en.txt ...\n",
      "importing Directive_2010_64_EU_en.txt ...\n",
      "importing Directive_2012_13_EU_en.txt ...\n",
      "importing Directive_2013_48_EU_en.txt ...\n"
     ]
    }
   ],
   "source": [
    "# Extracting all existing txt documents in the path\n",
    "data_path = my_path + \"\\\\data\\\\\" + lan + \"\\\\directives_txt\"\n",
    "document_name_list = find_folder_with_type(data_path, '.txt') # detection of txt files in the folder\n",
    "document_dic = folder_list_to_dic(data_path, document_name_list) # storing document content in a dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T09:21:17.634894700Z",
     "start_time": "2023-06-06T09:21:17.565427300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Entity Extraction (NER)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through  Directive_(EU)_2016_1919_en.txt ...\n",
      "Going through  Directive_(EU)_2016_343_en.txt ...\n",
      "Going through  Directive_(EU)_2016_800_en.txt ...\n",
      "Going through  Directive_2010_64_EU_en.txt ...\n",
      "Going through  Directive_2012_13_EU_en.txt ...\n",
      "Going through  Directive_2013_48_EU_en.txt ...\n"
     ]
    }
   ],
   "source": [
    "# tagging document\n",
    "data_path = my_path + \"\\\\data\\\\\" + lan + \"\\\\directives_txt_tagged\"\n",
    "updated_concept_list = update_concept_list(document_name_list, document_dic, concept_list, eurovoc_reverse_dic)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T09:22:38.805628500Z",
     "start_time": "2023-06-06T09:21:44.041504200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# have to run this only once (if file \"updated_eurovoc_en.tsv\" exists no need to run it)\n",
    "create_tsv_of_any_given_concept(updated_concept_list, lan)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T07:43:33.945826300Z",
     "start_time": "2023-05-26T07:43:33.855642800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Relation Extraction (RE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# !pip install stanford_openie"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-25T08:36:07.643141200Z",
     "start_time": "2023-05-25T08:36:07.621088200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "# https://stanfordnlp.github.io/CoreNLP/openie.html#api\n",
    "# Default value of openie.affinity_probability_cap was 1/3.\n",
    "properties = {\n",
    "    'openie.affinity_probability_cap': 2 / 3,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T09:23:19.131103500Z",
     "start_time": "2023-06-06T09:23:19.087210300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx8G -cp C:\\Users\\dnaen\\.stanfordnlp_resources\\stanford-corenlp-4.5.3/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-8a856ee80ea9479f.props -preload openie\n",
      "Document 'Directive_(EU)_2016_1919_en.txt' finished\n",
      "Document 'Directive_(EU)_2016_343_en.txt' finished\n",
      "Document 'Directive_(EU)_2016_800_en.txt' finished\n",
      "Document 'Directive_2010_64_EU_en.txt' finished\n",
      "Document 'Directive_2012_13_EU_en.txt' finished\n",
      "Document 'Directive_2013_48_EU_en.txt' finished\n"
     ]
    }
   ],
   "source": [
    "document_dic_tokenized = split_text_into_sentence(document_name_list, document_dic)\n",
    "list_of_triples = []\n",
    "triple_id = []\n",
    "testing = False\n",
    "\n",
    "with StanfordOpenIE(properties=properties) as client: # opening the server\n",
    "    if testing: print()\n",
    "    for doc_name in document_name_list:\n",
    "        for sentence in document_dic_tokenized[doc_name]:\n",
    "            if testing: print(\"Sentence\")\n",
    "            if testing: print(sentence)\n",
    "            current_triple_list_of_sentence = get_triples_stanford_openie(client, sentence) # returns [subject, relation, object]\n",
    "            if current_triple_list_of_sentence: # only run the loop if list not empty\n",
    "                index = 0\n",
    "                for current_triple in current_triple_list_of_sentence: # a sentence can have multiple triples\n",
    "                    if testing: print(\"Triple \" + str(index) + \": \" + str(current_triple))\n",
    "                    list_of_triples.append(current_triple)\n",
    "                    index += 1\n",
    "\n",
    "                triple_id.extend([doc_name] * len(current_triple_list_of_sentence))\n",
    "            if testing: print()\n",
    "        print(\"Document '\" + doc_name + \"' finished\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T09:25:42.647968800Z",
     "start_time": "2023-06-06T09:23:19.905519400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [i[0] for i in list_of_triples]\n",
    "\n",
    "# extract relation\n",
    "relation = [i[1] for i in list_of_triples]\n",
    "\n",
    "# extract object\n",
    "target = [i[2] for i in list_of_triples]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T13:40:27.704486200Z",
     "start_time": "2023-06-06T13:40:27.676841100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Build Knowledge Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "data_path = my_path + \"\\\\src\\\\main\"\n",
    "os.chdir(data_path)\n",
    "\n",
    "create_kg_csv(source, relation, target, \"predefined_dictionary\", \"stanford_OpenIE\", lan, triple_id)\n",
    "\n",
    "# create_combined_eurovoc([\"en\", \"fr\", \"de\"]) # has to be ran only once"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T13:40:30.625541100Z",
     "start_time": "2023-06-06T13:40:30.521816Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. Translate\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# saving the already created concepts\n",
    "# if it's not, remember you can use \"create_tsv_of_language(given_language)\" or \"create_tsv_of_any_given_concept(given_concept_dict, given_language)\" to create them\n",
    "data_of_en = pd.read_csv('updated_eurovoc_en.tsv',sep='\\t')\n",
    "data_of_de = pd.read_csv('eurovoc_de.tsv',sep='\\t')\n",
    "data_of_fr = pd.read_csv('eurovoc_fr.tsv',sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T07:46:41.378248100Z",
     "start_time": "2023-05-26T07:46:41.335143600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from row: 5538\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[241], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# creating german KG\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43msimple_translator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpredefined_dictionary\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstanford_OpenIE\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[240], line 82\u001B[0m, in \u001B[0;36msimple_translator\u001B[1;34m(target_language, re_type, model_used, extra_info)\u001B[0m\n\u001B[0;32m     80\u001B[0m source_en, relation_en, target_en \u001B[38;5;241m=\u001B[39m lines[\u001B[38;5;241m0\u001B[39m], lines[\u001B[38;5;241m1\u001B[39m], lines[\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m     81\u001B[0m \u001B[38;5;66;03m# src(source) = english, dest(destination) = language to translate to\u001B[39;00m\n\u001B[1;32m---> 82\u001B[0m translated_source, translated_relation, translated_target \u001B[38;5;241m=\u001B[39m translator\u001B[38;5;241m.\u001B[39mtranslate(source_en, src \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m\"\u001B[39m, dest \u001B[38;5;241m=\u001B[39m target_language), translator\u001B[38;5;241m.\u001B[39mtranslate(relation_en, src \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m\"\u001B[39m, dest \u001B[38;5;241m=\u001B[39m target_language), \u001B[43mtranslator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranslate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_en\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43men\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtarget_language\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     84\u001B[0m temp_row \u001B[38;5;241m=\u001B[39m [unidecode(translated_source\u001B[38;5;241m.\u001B[39mtext), unidecode(translated_relation\u001B[38;5;241m.\u001B[39mtext), unidecode(translated_target\u001B[38;5;241m.\u001B[39mtext), unidecode(lines[\u001B[38;5;241m3\u001B[39m])]\n\u001B[0;32m     85\u001B[0m \u001B[38;5;66;03m# index = 0\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# for row in temp_row:\u001B[39;00m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;66;03m#     temp_row[index] = row.replace(u'\\u200b', '') # this \"space\" character gives error, but it just adds extra space so just removing it\u001B[39;00m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m#     index += 1\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\client.py:210\u001B[0m, in \u001B[0;36mTranslator.translate\u001B[1;34m(self, text, dest, src, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m    209\u001B[0m origin \u001B[38;5;241m=\u001B[39m text\n\u001B[1;32m--> 210\u001B[0m data, response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_translate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;66;03m# this code will be updated when the format is changed.\u001B[39;00m\n\u001B[0;32m    213\u001B[0m translated \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([d[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m d[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m data[\u001B[38;5;241m0\u001B[39m]])\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\client.py:108\u001B[0m, in \u001B[0;36mTranslator._translate\u001B[1;34m(self, text, dest, src, override)\u001B[0m\n\u001B[0;32m    104\u001B[0m params \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mbuild_params(client\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient_type, query\u001B[38;5;241m=\u001B[39mtext, src\u001B[38;5;241m=\u001B[39msrc, dest\u001B[38;5;241m=\u001B[39mdest,\n\u001B[0;32m    105\u001B[0m                             token\u001B[38;5;241m=\u001B[39mtoken, override\u001B[38;5;241m=\u001B[39moverride)\n\u001B[0;32m    107\u001B[0m url \u001B[38;5;241m=\u001B[39m urls\u001B[38;5;241m.\u001B[39mTRANSLATE\u001B[38;5;241m.\u001B[39mformat(host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pick_service_url())\n\u001B[1;32m--> 108\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m r\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[0;32m    111\u001B[0m     data \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mformat_json(r\u001B[38;5;241m.\u001B[39mtext)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_client.py:755\u001B[0m, in \u001B[0;36mClient.get\u001B[1;34m(self, url, params, headers, cookies, auth, allow_redirects, timeout)\u001B[0m\n\u001B[0;32m    744\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\n\u001B[0;32m    745\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    746\u001B[0m     url: URLTypes,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    753\u001B[0m     timeout: typing\u001B[38;5;241m.\u001B[39mUnion[TimeoutTypes, UnsetType] \u001B[38;5;241m=\u001B[39m UNSET,\n\u001B[0;32m    754\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Response:\n\u001B[1;32m--> 755\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    756\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    757\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    758\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    759\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    760\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    761\u001B[0m \u001B[43m        \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    762\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    763\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    764\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_client.py:600\u001B[0m, in \u001B[0;36mClient.request\u001B[1;34m(self, method, url, data, files, json, params, headers, cookies, auth, allow_redirects, timeout)\u001B[0m\n\u001B[0;32m    575\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    576\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    577\u001B[0m     method: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    588\u001B[0m     timeout: typing\u001B[38;5;241m.\u001B[39mUnion[TimeoutTypes, UnsetType] \u001B[38;5;241m=\u001B[39m UNSET,\n\u001B[0;32m    589\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Response:\n\u001B[0;32m    590\u001B[0m     request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuild_request(\n\u001B[0;32m    591\u001B[0m         method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[0;32m    592\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    598\u001B[0m         cookies\u001B[38;5;241m=\u001B[39mcookies,\n\u001B[0;32m    599\u001B[0m     )\n\u001B[1;32m--> 600\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_redirects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    602\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_client.py:620\u001B[0m, in \u001B[0;36mClient.send\u001B[1;34m(self, request, stream, auth, allow_redirects, timeout)\u001B[0m\n\u001B[0;32m    616\u001B[0m timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(timeout, UnsetType) \u001B[38;5;28;01melse\u001B[39;00m Timeout(timeout)\n\u001B[0;32m    618\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuild_auth(request, auth)\n\u001B[1;32m--> 620\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    621\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    624\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n\u001B[0;32m    625\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_client.py:647\u001B[0m, in \u001B[0;36mClient.send_handling_redirects\u001B[1;34m(self, request, auth, timeout, allow_redirects, history)\u001B[0m\n\u001B[0;32m    644\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(history) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_redirects:\n\u001B[0;32m    645\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m TooManyRedirects()\n\u001B[1;32m--> 647\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    648\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    650\u001B[0m response\u001B[38;5;241m.\u001B[39mhistory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(history)\n\u001B[0;32m    652\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m response\u001B[38;5;241m.\u001B[39mis_redirect:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_client.py:684\u001B[0m, in \u001B[0;36mClient.send_handling_auth\u001B[1;34m(self, request, history, auth, timeout)\u001B[0m\n\u001B[0;32m    682\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[0;32m    683\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 684\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    685\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m auth\u001B[38;5;241m.\u001B[39mrequires_response_body:\n\u001B[0;32m    686\u001B[0m         response\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_client.py:714\u001B[0m, in \u001B[0;36mClient.send_single_request\u001B[1;34m(self, request, timeout)\u001B[0m\n\u001B[0;32m    705\u001B[0m transport \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransport_for_url(request\u001B[38;5;241m.\u001B[39murl)\n\u001B[0;32m    707\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    708\u001B[0m     (\n\u001B[0;32m    709\u001B[0m         http_version,\n\u001B[0;32m    710\u001B[0m         status_code,\n\u001B[0;32m    711\u001B[0m         reason_phrase,\n\u001B[0;32m    712\u001B[0m         headers,\n\u001B[0;32m    713\u001B[0m         stream,\n\u001B[1;32m--> 714\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[43mtransport\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    715\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    716\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    717\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    718\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    719\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    720\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    721\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    722\u001B[0m     \u001B[38;5;66;03m# Add the original request to any HTTPError unless\u001B[39;00m\n\u001B[0;32m    723\u001B[0m     \u001B[38;5;66;03m# there'a already a request attached in the case of\u001B[39;00m\n\u001B[0;32m    724\u001B[0m     \u001B[38;5;66;03m# a ProxyError.\u001B[39;00m\n\u001B[0;32m    725\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m exc\u001B[38;5;241m.\u001B[39m_request \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:152\u001B[0m, in \u001B[0;36mSyncConnectionPool.request\u001B[1;34m(self, method, url, headers, stream, timeout)\u001B[0m\n\u001B[0;32m    149\u001B[0m         logger\u001B[38;5;241m.\u001B[39mtrace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreuse connection=\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, connection)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 152\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NewConnectionRequired:\n\u001B[0;32m    156\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001B[0m, in \u001B[0;36mSyncHTTPConnection.request\u001B[1;34m(self, method, url, headers, stream, timeout)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     75\u001B[0m logger\u001B[38;5;241m.\u001B[39mtrace(\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconnection.request method=\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m url=\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m headers=\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, method, url, headers\n\u001B[0;32m     77\u001B[0m )\n\u001B[1;32m---> 78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py:118\u001B[0m, in \u001B[0;36mSyncHTTP2Connection.request\u001B[1;34m(self, method, url, headers, stream, timeout)\u001B[0m\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreams[stream_id] \u001B[38;5;241m=\u001B[39m h2_stream\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevents[stream_id] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m--> 118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mh2_stream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_streams_semaphore\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py:292\u001B[0m, in \u001B[0;36mSyncHTTP2Stream.request\u001B[1;34m(self, method, url, headers, stream, timeout)\u001B[0m\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend_body(stream, timeout)\n\u001B[0;32m    291\u001B[0m \u001B[38;5;66;03m# Receive the response.\u001B[39;00m\n\u001B[1;32m--> 292\u001B[0m status_code, headers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreceive_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    293\u001B[0m reason_phrase \u001B[38;5;241m=\u001B[39m get_reason_phrase(status_code)\n\u001B[0;32m    294\u001B[0m stream \u001B[38;5;241m=\u001B[39m SyncByteStream(\n\u001B[0;32m    295\u001B[0m     iterator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbody_iter(timeout), close_func\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_closed\n\u001B[0;32m    296\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py:344\u001B[0m, in \u001B[0;36mSyncHTTP2Stream.receive_response\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    340\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    341\u001B[0m \u001B[38;5;124;03mRead the response status and headers from the network.\u001B[39;00m\n\u001B[0;32m    342\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 344\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait_for_event\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    345\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h2\u001B[38;5;241m.\u001B[39mevents\u001B[38;5;241m.\u001B[39mResponseReceived):\n\u001B[0;32m    346\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py:197\u001B[0m, in \u001B[0;36mSyncHTTP2Connection.wait_for_event\u001B[1;34m(self, stream_id, timeout)\u001B[0m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_lock:\n\u001B[0;32m    196\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevents[stream_id]:\n\u001B[1;32m--> 197\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreceive_events\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevents[stream_id]\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py:204\u001B[0m, in \u001B[0;36mSyncHTTP2Connection.receive_events\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreceive_events\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout: TimeoutDict) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;124;03m    Read some data from the network, and update the H2 state.\u001B[39;00m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 204\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     events \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh2_state\u001B[38;5;241m.\u001B[39mreceive_data(data)\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m events:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_backends\\sync.py:62\u001B[0m, in \u001B[0;36mSyncSocketStream.read\u001B[1;34m(self, n, timeout)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock\u001B[38;5;241m.\u001B[39msettimeout(read_timeout)\n\u001B[1;32m---> 62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:1226\u001B[0m, in \u001B[0;36mSSLSocket.recv\u001B[1;34m(self, buflen, flags)\u001B[0m\n\u001B[0;32m   1222\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1223\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1224\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1225\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1226\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuflen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1227\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1228\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv(buflen, flags)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:1101\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1099\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[0;32m   1100\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1101\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1102\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SSLError \u001B[38;5;28;01mas\u001B[39;00m x:\n\u001B[0;32m   1103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m SSL_ERROR_EOF \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuppress_ragged_eofs:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# creating german KG\n",
    "# update_triple_csv_file_with_concepts(\"predefined_dictionary\", \"stanford_OpenIE\", lan, list(data_of_fr[\"FR\"]))\n",
    "simple_translator(\"fr\", \"predefined_dictionary\", \"stanford_OpenIE\", \"\") # CAUTION: since this uses API it can \"timeout\", so if happens just run it again (as the translator just starts from the place it has left it's fine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T14:33:52.579296800Z",
     "start_time": "2023-06-06T14:33:34.963533600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from row: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[235], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# creating french KG\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43msimple_translator_with_concept\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpredefined_dictionary\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstanford_OpenIE\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_of_en\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_of_fr\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[233], line 131\u001B[0m, in \u001B[0;36msimple_translator_with_concept\u001B[1;34m(target_language, re_type, model_used, given_concept_en, given_concept_na, extra_info)\u001B[0m\n\u001B[0;32m    128\u001B[0m source_en, relation_en, target_en \u001B[38;5;241m=\u001B[39m lines[\u001B[38;5;241m0\u001B[39m], lines[\u001B[38;5;241m1\u001B[39m], lines[\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m    130\u001B[0m \u001B[38;5;66;03m# src(source) = english, dest(destination) = language to translate to\u001B[39;00m\n\u001B[1;32m--> 131\u001B[0m translated_source, translated_relation, translated_target \u001B[38;5;241m=\u001B[39m translator\u001B[38;5;241m.\u001B[39mtranslate(source_en, src \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m\"\u001B[39m, dest \u001B[38;5;241m=\u001B[39m target_language), \u001B[43mtranslator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranslate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrelation_en\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43men\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtarget_language\u001B[49m\u001B[43m)\u001B[49m, translator\u001B[38;5;241m.\u001B[39mtranslate(target_en, src \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m\"\u001B[39m, dest \u001B[38;5;241m=\u001B[39m target_language)\n\u001B[0;32m    133\u001B[0m translated_source \u001B[38;5;241m=\u001B[39m translated_source\u001B[38;5;241m.\u001B[39mtext\n\u001B[0;32m    134\u001B[0m translated_relation \u001B[38;5;241m=\u001B[39m translated_relation\u001B[38;5;241m.\u001B[39mtext\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\client.py:210\u001B[0m, in \u001B[0;36mTranslator.translate\u001B[1;34m(self, text, dest, src, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m    209\u001B[0m origin \u001B[38;5;241m=\u001B[39m text\n\u001B[1;32m--> 210\u001B[0m data, response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_translate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;66;03m# this code will be updated when the format is changed.\u001B[39;00m\n\u001B[0;32m    213\u001B[0m translated \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([d[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m d[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m data[\u001B[38;5;241m0\u001B[39m]])\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\client.py:108\u001B[0m, in \u001B[0;36mTranslator._translate\u001B[1;34m(self, text, dest, src, override)\u001B[0m\n\u001B[0;32m    104\u001B[0m params \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mbuild_params(client\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient_type, query\u001B[38;5;241m=\u001B[39mtext, src\u001B[38;5;241m=\u001B[39msrc, dest\u001B[38;5;241m=\u001B[39mdest,\n\u001B[0;32m    105\u001B[0m                             token\u001B[38;5;241m=\u001B[39mtoken, override\u001B[38;5;241m=\u001B[39moverride)\n\u001B[0;32m    107\u001B[0m url \u001B[38;5;241m=\u001B[39m urls\u001B[38;5;241m.\u001B[39mTRANSLATE\u001B[38;5;241m.\u001B[39mformat(host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pick_service_url())\n\u001B[1;32m--> 108\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m r\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[0;32m    111\u001B[0m     data \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mformat_json(r\u001B[38;5;241m.\u001B[39mtext)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_client.py:755\u001B[0m, in \u001B[0;36mClient.get\u001B[1;34m(self, url, params, headers, cookies, auth, allow_redirects, timeout)\u001B[0m\n\u001B[0;32m    744\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\n\u001B[0;32m    745\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    746\u001B[0m     url: URLTypes,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    753\u001B[0m     timeout: typing\u001B[38;5;241m.\u001B[39mUnion[TimeoutTypes, UnsetType] \u001B[38;5;241m=\u001B[39m UNSET,\n\u001B[0;32m    754\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Response:\n\u001B[1;32m--> 755\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    756\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    757\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    758\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    759\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    760\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    761\u001B[0m \u001B[43m        \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    762\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    763\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    764\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_client.py:600\u001B[0m, in \u001B[0;36mClient.request\u001B[1;34m(self, method, url, data, files, json, params, headers, cookies, auth, allow_redirects, timeout)\u001B[0m\n\u001B[0;32m    575\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    576\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    577\u001B[0m     method: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    588\u001B[0m     timeout: typing\u001B[38;5;241m.\u001B[39mUnion[TimeoutTypes, UnsetType] \u001B[38;5;241m=\u001B[39m UNSET,\n\u001B[0;32m    589\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Response:\n\u001B[0;32m    590\u001B[0m     request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuild_request(\n\u001B[0;32m    591\u001B[0m         method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[0;32m    592\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    598\u001B[0m         cookies\u001B[38;5;241m=\u001B[39mcookies,\n\u001B[0;32m    599\u001B[0m     )\n\u001B[1;32m--> 600\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_redirects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    602\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_client.py:626\u001B[0m, in \u001B[0;36mClient.send\u001B[1;34m(self, request, stream, auth, allow_redirects, timeout)\u001B[0m\n\u001B[0;32m    624\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n\u001B[0;32m    625\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 626\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m         response\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_models.py:901\u001B[0m, in \u001B[0;36mResponse.read\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    897\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    898\u001B[0m \u001B[38;5;124;03mRead and return the response content.\u001B[39;00m\n\u001B[0;32m    899\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_content\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 901\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([part \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_bytes()])\n\u001B[0;32m    902\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_models.py:901\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    897\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    898\u001B[0m \u001B[38;5;124;03mRead and return the response content.\u001B[39;00m\n\u001B[0;32m    899\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_content\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 901\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([part \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_bytes()])\n\u001B[0;32m    902\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_models.py:912\u001B[0m, in \u001B[0;36mResponse.iter_bytes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    910\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content\n\u001B[0;32m    911\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 912\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_raw():\n\u001B[0;32m    913\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39mdecode(chunk)\n\u001B[0;32m    914\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39mflush()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpx\\_models.py:945\u001B[0m, in \u001B[0;36mResponse.iter_raw\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    942\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResponseClosed()\n\u001B[0;32m    944\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_stream_consumed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 945\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raw_stream:\n\u001B[0;32m    946\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m part\n\u001B[0;32m    947\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:49\u001B[0m, in \u001B[0;36mResponseByteStream.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[1;32m---> 49\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream:\n\u001B[0;32m     50\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m chunk\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\base.py:57\u001B[0m, in \u001B[0;36mSyncByteStream.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;124;03m    Yield bytes representing the request or response body.\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 57\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator:\n\u001B[0;32m     58\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m chunk\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py:360\u001B[0m, in \u001B[0;36mSyncHTTP2Stream.body_iter\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    358\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbody_iter\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout: TimeoutDict) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[0;32m    359\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 360\u001B[0m         event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait_for_event\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    361\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h2\u001B[38;5;241m.\u001B[39mevents\u001B[38;5;241m.\u001B[39mDataReceived):\n\u001B[0;32m    362\u001B[0m             amount \u001B[38;5;241m=\u001B[39m event\u001B[38;5;241m.\u001B[39mflow_controlled_length\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py:197\u001B[0m, in \u001B[0;36mSyncHTTP2Connection.wait_for_event\u001B[1;34m(self, stream_id, timeout)\u001B[0m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_lock:\n\u001B[0;32m    196\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevents[stream_id]:\n\u001B[1;32m--> 197\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreceive_events\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevents[stream_id]\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py:204\u001B[0m, in \u001B[0;36mSyncHTTP2Connection.receive_events\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreceive_events\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout: TimeoutDict) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;124;03m    Read some data from the network, and update the H2 state.\u001B[39;00m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 204\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     events \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh2_state\u001B[38;5;241m.\u001B[39mreceive_data(data)\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m events:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\httpcore\\_backends\\sync.py:62\u001B[0m, in \u001B[0;36mSyncSocketStream.read\u001B[1;34m(self, n, timeout)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock\u001B[38;5;241m.\u001B[39msettimeout(read_timeout)\n\u001B[1;32m---> 62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:1226\u001B[0m, in \u001B[0;36mSSLSocket.recv\u001B[1;34m(self, buflen, flags)\u001B[0m\n\u001B[0;32m   1222\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1223\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1224\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1225\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1226\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuflen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1227\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1228\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv(buflen, flags)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\ssl.py:1101\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1099\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[0;32m   1100\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1101\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1102\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SSLError \u001B[38;5;28;01mas\u001B[39;00m x:\n\u001B[0;32m   1103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m SSL_ERROR_EOF \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuppress_ragged_eofs:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# creating french KG\n",
    "\n",
    "simple_translator_with_concept(\"fr\", \"predefined_dictionary\", \"stanford_OpenIE\", data_of_en, data_of_fr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T13:43:37.997441800Z",
     "start_time": "2023-06-06T13:43:29.664988800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trash"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "# nltk.download('punkt') # unsupervised trainable model, which means it can be trained on unlabeled data (Data that has not been tagged with information identifying its characteristics, properties, or categories is referred to as unlabeled data.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# To check what new concepts were added\n",
    "\n",
    "# temp_data_of_en_old = pd.read_csv('eurovoc_en.tsv',sep='\\t')\n",
    "# temp = list(updated_concept_list.values())\n",
    "# my_dict = {i: temp.count(i) for i in temp}\n",
    "# multiple_elements = []\n",
    "# for key, value in my_dict.items():\n",
    "#     if value > 1:\n",
    "#         multiple_elements.append(key)\n",
    "#\n",
    "# for elem in multiple_elements:\n",
    "#     value = {i for i in updated_concept_list if updated_concept_list[i] == elem}\n",
    "#\n",
    "#     for val in value:\n",
    "#         if val in list(temp_data_of_en_old[\"EN\"]): print(\"Old one is: \" + val)\n",
    "#\n",
    "#     print(str(value) + \" in id: \" + elem)\n",
    "#     print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T08:29:09.126516200Z",
     "start_time": "2023-06-06T08:29:09.105708400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To try out triple extraction\n",
    "\n",
    "# text = \"Can the prosecution guarantee the protection of suspect's procedural rights regarding the specific mechanisms and common minimum standards that are in place, such as the Charter of Fundamental Rights and the European Convention on Human Rights?\"\n",
    "#\n",
    "# # \"Which conclusions adopted a programme of measures to implement the principle of mutual recognition of decision in criminal matters?\"\n",
    "#\n",
    "#\n",
    "# properties = {\n",
    "#     'openie.affinity_probability_cap': 2 / 3,\n",
    "# }\n",
    "#\n",
    "# list_of_triples = []\n",
    "#\n",
    "# with StanfordOpenIE(properties=properties) as client: # opening the server\n",
    "#     current_triple_list_of_sentence = get_triples_stanford_openie(client, text) # returns [subject, relation, object]\n",
    "#     if current_triple_list_of_sentence: # only run the loop if list not empty\n",
    "#         index = 0\n",
    "#         for current_triple in current_triple_list_of_sentence: # a sentence can have multiple triples\n",
    "#             list_of_triples.append(current_triple)\n",
    "#             index += 1\n",
    "#\n",
    "# a, b, c = get_unique_words_from_triples(list_of_triples)\n",
    "# print(\"Subjects to look into:\")\n",
    "# print(a)\n",
    "# print()\n",
    "# print(\"Relations to look into:\")\n",
    "# print(b)\n",
    "# print()\n",
    "# print(\"Objects to look into:\")\n",
    "# print(c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To check whether grouping function works\n",
    "\n",
    "# temp_text = \"protection of suspect 's procedural rights regarding specific mechanisms\"\n",
    "# temp_text = temp_text.split()\n",
    "# temp_group = \"\"\n",
    "# group_list = []\n",
    "# # groups of 2, groups of 3, groups of 4, ...\n",
    "#\n",
    "# for i in range(len(temp_text)):\n",
    "#     print()\n",
    "#     print(\"Groups of \", i+1)\n",
    "#     for j in range(len(temp_text) - i):\n",
    "#         for k in range(j, j+i+1):\n",
    "#             if k == j+i: temp_group = temp_group + temp_text[k]\n",
    "#             else: temp_group = temp_group + temp_text[k] + \" \"\n",
    "#             print(temp_text[k], end=\" \")\n",
    "#         group_list.append(temp_group)\n",
    "#         temp_group = \"\"\n",
    "#         print(\"\", end=\" | \")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for word in possible_words_list:\n",
    "#         word = word.lower()\n",
    "#         if word not in given_stopwords_list:\n",
    "#             for concept in given_concept_list:\n",
    "#                 concept_uni = unidecode(concept) # because the triples of other languages will be in unidecode\n",
    "#                 if word in concept_uni and len(word.split()) == len(concept_uni.split()):\n",
    "#                     print(\"Word: \", word)\n",
    "#                     print(\"Found concept \", concept_uni)\n",
    "#                     print()\n",
    "#                     elems_concept.append(concept_uni)\n",
    "#\n",
    "#     return elems_concept"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To check whether \"find_corresponding_eurovoc_concept\" works\n",
    "\n",
    "# for triple in list_of_triples:\n",
    "#     print(\"Triple\")\n",
    "#     print(triple)\n",
    "#     print(\"Found concepts\")\n",
    "#     for elem in triple:\n",
    "#         a = find_corresponding_eurovoc_concept(elem, concept_list, stopwords.words('english'))\n",
    "#         print(a, end = \", \")\n",
    "#\n",
    "#     print()\n",
    "#     print()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
