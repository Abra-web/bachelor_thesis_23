{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-14T17:06:07.134771Z",
     "end_time": "2023-05-14T17:06:07.153776Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import xml.etree.ElementTree as Xet # for parsing and creating XML data\n",
    "import pandas as pd\n",
    "import os, csv, re, nltk\n",
    "from flair.data import Corpus # in order to use the functions tha flair has\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings # these embeddings helps NER to perform better\n",
    "from itertools import islice\n",
    "from nltk.stem import WordNetLemmatizer # previously need to download \"nltk.download('wordnet')\" and \"nltk.download('omw-1.4')\". But beware if new version comes out"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NER (Named-Entity Recognition)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# functions\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# English: en, German: de, French: fr, ... -> creates the tsv of given descriptor of any language\n",
    "def create_tsv_of_language(given_language):\n",
    "    \"\"\"\n",
    "    Before running this function below, the \"desc_\"\".xml\" file (that is downloaded from EuroVoc website) needs to be downloaded and added to package \"data/\"\"/descriptors/...\"\n",
    "    \"\"\"\n",
    "    cols = ['ID', given_language.upper()] # will be saving in a tsv with ids and their corresponding terms\n",
    "    rows = []\n",
    "\n",
    "    # parsing the xml file -> with the given EuroVoc descriptors\n",
    "    temp_path = os.getcwd()\n",
    "    temp_path = temp_path.replace(\"src\\\\main\", \"data\\\\\" + given_language + \"\\\\descriptors\\\\desc_\" + given_language + \".xml\")\n",
    "    xml_parse = Xet.parse(temp_path)\n",
    "    root = xml_parse.getroot()\n",
    "\n",
    "    # iterate through the elements of xml file\n",
    "    for element in root:\n",
    "        rows.append({\"ID\": element.find(\"DESCRIPTEUR_ID\").text, given_language.upper(): element.find(\"LIBELLE\").text})\n",
    "\n",
    "    # creating the tsv file\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    df.to_csv(\"eurovoc_\" + given_language + \".tsv\", sep='\\t', index=False) # using sep='\\t' gives us a tsv file instead of csv\n",
    "\n",
    "# this function assumes we get the text annotated as [entity_value](entity_name), and assigns prefixes B, I, and 0 to each token\n",
    "def get_tokens_with_entities(raw_text: str):\n",
    "    # split the text by spaces (but not splitting the space inside the square brackets (so not splitting the \"multi-word\" entity value yet))\n",
    "    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n",
    "\n",
    "    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n",
    "    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n",
    "\n",
    "    # flags: re.IGNORECASE and re.MULTILINE\n",
    "    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M) # using it to compile a regular expression pattern provided as a string into a regex pattern object\n",
    "\n",
    "    tokens_with_entities = []\n",
    "\n",
    "    for raw_token in raw_tokens:\n",
    "        match = entity_value_pattern_compiled.match(raw_token) # if no match then returns None\n",
    "\n",
    "        if match:\n",
    "            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n",
    "\n",
    "            # we prefix the name of entity differently\n",
    "            # B- indicates beginning of an entity\n",
    "            # I- indicates the token is not a new entity itself but rather a part of existing one\n",
    "            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n",
    "                entity_prefix = \"B\" if i == 0 else \"I\"\n",
    "                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n",
    "                tokens_with_entities.append((raw_entity_token, entity_name))\n",
    "        else:\n",
    "            tokens_with_entities.append((raw_token, \"O\")) # no match\n",
    "\n",
    "    return tokens_with_entities\n",
    "\n",
    "# NLTK VERSION\n",
    "def regex_from_term_nltk(term, lemmatizer): # TODO add some kind of automatic noun-verb-... identifier for lemmatization (so parts-of-speech required to add)\n",
    "    regex = r\"\\b(\" # Regex Opening\n",
    "    tokensList = nltk.word_tokenize(term)\n",
    "\n",
    "    # Adding terms to regex\n",
    "    if len(tokensList) == 1: # in case of one-word term\n",
    "        for token in tokensList:\n",
    "            regex += token_cleaning(token, lemmatizer)\n",
    "\n",
    "    else: # if it is a multi-word term\n",
    "        decount = len(tokensList)\n",
    "        for token in tokensList:\n",
    "            decount = decount-1\n",
    "            # add between-words\n",
    "            if decount != len(tokensList)-1:\n",
    "                regex+= r'\\w*\\W\\w*\\W*'\n",
    "            # add token\n",
    "            regex += token_cleaning(token, lemmatizer)\n",
    "\n",
    "    regex += '''\\w{0,5})(\\W)''' # Regex Closure\n",
    "    return regex\n",
    "\n",
    "def token_cleaning(token, lemmatizer):\n",
    "    token = token.lower()\n",
    "    token = lemmatizer.lemmatize(token)\n",
    "    return token\n",
    "\n",
    "# Functions for document processing were taken from @https://github.com/shashankmc/eurovoc_entity_link/blob/master/EurovocTagger.py and were modified\n",
    "def tsv_dic_processing(path):\n",
    "    \"\"\"\n",
    "    :param path: the name of the eurovoc.tsv file\n",
    "    :return: Dic: Dictionary in style of {ID: Word}\n",
    "    :return: RevDic: Dictionary in style of {Word: ID}\n",
    "    :return: list1: list of IDs\n",
    "    :return: list2: list of words (concepts)\n",
    "    \"\"\"\n",
    "    # Dic, RevDic, list1, list2\n",
    "    # Only works with a 2-columns ([ID], [EN]) TSV file\n",
    "    Dic = {}\n",
    "    RevDic = {}\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    with open(path, 'rt', encoding='utf8') as csvfile:\n",
    "        myreader = csv.reader(csvfile, delimiter='\\t')\n",
    "        rcount = 0\n",
    "        for row in myreader:\n",
    "            rcount += 1\n",
    "            ccount = 0\n",
    "            if rcount > 1:\n",
    "                for cells in row:\n",
    "                    ccount += 1\n",
    "                    if ccount ==1:\n",
    "                        list1.append(cells)\n",
    "                        key = cells\n",
    "                    else:\n",
    "                        list2.append(cells)\n",
    "                        value = cells\n",
    "                Dic[key] = value\n",
    "                RevDic[value] = key\n",
    "    return Dic, RevDic, list1, list2\n",
    "\n",
    "def find_folder_with_type(given_path, doc_type): # returns all documents found in path\n",
    "    doc_list = []\n",
    "    for doc in os.listdir(given_path):\n",
    "        if re.search (r'.*\\%s$' % doc_type, doc) is not None: # even though this shows as error in IDE it's fine\n",
    "            doc_list.append(doc)\n",
    "    return doc_list\n",
    "\n",
    "\n",
    "def folder_list_to_dic(given_path, given_list):\n",
    "    dic = {}\n",
    "    old_path = os.getcwd() # saving the previous working dir so we can switch back to that dir later\n",
    "    os.chdir(given_path)\n",
    "\n",
    "    # the input should be a list of file contained in a folder\n",
    "    for file_name in given_list:\n",
    "        print('importing', file_name, '...')\n",
    "        with open(\"%s\" % file_name, \"r\", encoding='utf8') as my_file:\n",
    "            text = my_file.read()\n",
    "        dic[file_name]= text\n",
    "\n",
    "    os.chdir(old_path)\n",
    "    return dic\n",
    "\n",
    "# tagging by researching concept-regexed as a substring of the text (by using NLTK)\n",
    "def tagging_document(path_of_tagged, given_doc_list, given_doc_dic, given_concept_list, given_eurovoc_reverse_dic):\n",
    "    \"\"\"\n",
    "    This function takes the information of the descriptor (e.g., {id:concept}, id list, concept list, ...) and then with the given document information it creates the new tagged document in tagged folder. Additionally, it returns the new updated concept list which contains additional \"concepts\" found in the document text that seems to be related to one of the original concepts. Thus, expanding the vocabulary we have.\n",
    "\n",
    "    :param path_of_tagged: the location (dir) of the tagged folder\n",
    "    :param given_doc_list: a list of names of the documents\n",
    "    :param given_doc_dic: a dic that contains the contents of the document i.e. {doc_name: doc_text}\n",
    "    :param given_concept_list: the original concept list downloaded from Eurovoc\n",
    "    :param given_eurovoc_reverse_dic: opposite of \"given_concept_list\" so {concept: id}\n",
    "    :return: new_concept: this is the new expanded concept list\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    old_path = os.getcwd() # saving the previous working dir so we can switch back to that dir later\n",
    "    os.chdir(path_of_tagged)\n",
    "    new_concept = given_concept_list.copy()\n",
    "\n",
    "    for doc_name in given_doc_list:\n",
    "        tags_list=[]\n",
    "        tagged_text = \"\"\n",
    "        print('tagging', doc_name,'...')\n",
    "        text = given_doc_dic[doc_name]\n",
    "        text = text.lower()\n",
    "        tagged_text = text # document's initial text\n",
    "\n",
    "        # a concept tag will be done with a star (*), and the identifier with a +\n",
    "        for concept in given_concept_list:\n",
    "\n",
    "            if concept != \"\": # if concept empty, will tag everything (so need to make sure that it's not empty)\n",
    "                # REGEX CREATION: creating regex of the concept such that it can be used to search in doc later\n",
    "                regex = regex_from_term_nltk(concept, lemmatizer)\n",
    "\n",
    "                # concept = concept.strip()\n",
    "                # TAGGING #\n",
    "                # semantically neutral symbols are chosen to prevent eurovoc concepts from matching tags\n",
    "                if re.search(regex, text) is not None:\n",
    "                    # these prints can be used to check performance\n",
    "                    # print(\"Match made!\")\n",
    "                    # print(\"Found: \" + re.search(regex, text).group() + \", for concept: \" + concept)\n",
    "                    match_in_text = re.search(regex, text).group()\n",
    "                    if match_in_text not in given_concept_list:\n",
    "                        new_concept.append(match_in_text)\n",
    "\n",
    "                    tags_list.append(concept)\n",
    "                    sub_regex = r\"[\" + concept + r\"]\"\n",
    "                    sub_regex += r\"(\" + given_eurovoc_reverse_dic[concept] + r\") \" # insert the identifier\n",
    "                    tagged_text = re.sub(regex, sub_regex, tagged_text)\n",
    "\n",
    "    # create a new file with the tagged file\n",
    "        file = open(\"%s_TAGGED.txt\" % doc_name, \"w\", encoding='utf8')\n",
    "        file.write(tagged_text)\n",
    "        file.close()\n",
    "\n",
    "    os.chdir(old_path) # change back to previous path\n",
    "\n",
    "    return new_concept"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T17:06:08.451739Z",
     "end_time": "2023-05-14T17:06:08.475684Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Relations Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# functions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.Importing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "# this has to be ran only once, because it creates the eurovoc_en.tsv file (which should already be there)\n",
    "# create_tsv_of_language(\"en\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T17:12:10.839807Z",
     "end_time": "2023-05-09T17:12:10.852387Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnaen\\PycharmProjects\\bachelor_thesis_23\\src\\main\n"
     ]
    }
   ],
   "source": [
    "# to make sure that we are in the original working directory\n",
    "current_path = os.getcwd()\n",
    "data_path = current_path.replace(\"data\\\\en\\\\directives_txt_tagged\", \"src\\\\main\")\n",
    "os.chdir(data_path)\n",
    "print(os.getcwd()) # this should return something like \"...\\src\\main\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T17:06:24.958584Z",
     "end_time": "2023-05-14T17:06:24.969662Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eurovoc imported\n"
     ]
    }
   ],
   "source": [
    "tsv_file = \"eurovoc_en.tsv\"\n",
    "\n",
    "# getting info of ids and concepts from the tsv file\n",
    "eurovoc_dic, eurovoc_reverse_dic, id_list, concept_list = tsv_dic_processing(tsv_file)\n",
    "print('Eurovoc imported')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T17:06:30.640844Z",
     "end_time": "2023-05-14T17:06:30.673975Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Directive_(EU)_2016_1919_en.txt ...\n",
      "importing Directive_(EU)_2016_343_en.txt ...\n",
      "importing Directive_(EU)_2016_800_en.txt ...\n",
      "importing Directive_2010_64_EU_en.txt ...\n",
      "importing Directive_2012_13_EU_en.txt ...\n",
      "importing Directive_2013_48_EU_en.txt ...\n"
     ]
    }
   ],
   "source": [
    "# Extracting all existing txt documents in the path\n",
    "current_path = os.getcwd()\n",
    "data_path = current_path.replace(\"src\\\\main\", \"data\\\\en\\\\directives_txt\")\n",
    "document_name_list = find_folder_with_type(data_path, '.txt') # detection of txt files in the folder\n",
    "document_dic = folder_list_to_dic(data_path, document_name_list) # storing document content in a dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T17:06:32.515031Z",
     "end_time": "2023-05-14T17:06:32.598659Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Entity Extraction (NER)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagging Directive_(EU)_2016_1919_en.txt ...\n",
      "tagging Directive_(EU)_2016_343_en.txt ...\n",
      "tagging Directive_(EU)_2016_800_en.txt ...\n",
      "tagging Directive_2010_64_EU_en.txt ...\n",
      "tagging Directive_2012_13_EU_en.txt ...\n",
      "tagging Directive_2013_48_EU_en.txt ...\n"
     ]
    }
   ],
   "source": [
    "# tagging document\n",
    "data_path = current_path.replace(\"src\\\\main\", \"data\\\\en\\\\directives_txt_tagged\")\n",
    "updated_concept_list = tagging_document(data_path, document_name_list, document_dic, concept_list, eurovoc_reverse_dic)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T17:08:11.815047Z",
     "end_time": "2023-05-14T17:09:20.882469Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6797\n",
      "7467\n"
     ]
    }
   ],
   "source": [
    "# from here it can be seen that we have found new entities that are related to the original concept words\n",
    "print(len(concept_list))\n",
    "print(len(updated_concept_list)) # TODO don't forget to add the it's corresponding id too"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T17:11:07.429823Z",
     "end_time": "2023-05-14T17:11:07.460997Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Relation Extraction (RE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T17:13:23.595783Z",
     "end_time": "2023-05-09T17:13:23.635582Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T17:13:23.612279Z",
     "end_time": "2023-05-09T17:13:23.649193Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T17:13:23.624040Z",
     "end_time": "2023-05-09T17:13:23.649193Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trash"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "# nltk.download('punkt') # unsupervised trainable model, which means it can be trained on unlabeled data (Data that has not been tagged with information identifying its characteristics, properties, or categories is referred to as unlabeled data.)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T17:13:23.645204Z",
     "end_time": "2023-05-09T17:13:23.657289Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
